{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pickle import dump\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from itertools import product\n",
    "import json\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sent = pd.read_csv('data/data_sentiment.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44112"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([True if 'tsla' in i.lower() or '@tesla' in i.lower() or '$tesla' in i.lower() else False for i in data_sent['Tweet']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tsla = data_sent[data_sent[\"Stock Name\"] == 'TSLA']['Tweet']\n",
    "# print(len(tsla))\n",
    "# iter = [True if 'tsla' in i.lower() or '@tesla' in i.lower() or '$tesla' in i.lower() else False for i in data_sent['Tweet']]\n",
    "# for n,i in enumerate(iter):\n",
    "#     if not i:\n",
    "#         print(tsla[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tweet, stock in zip(data_sent['Tweet'],data_sent['Stock Name']):\n",
    "#     if 'tesla' in tweet.lower() and stock != 'TSLA':\n",
    "#         print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions:\n",
    "- Stocks are being picked based on name of the company or stock ticker\n",
    "- If a twitter post discusses more than one stock then column \"Stock name\" is empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_hist = pd.read_csv('data/stock_yfinance_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Stock Name</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>positive</th>\n",
       "      <th>neutral</th>\n",
       "      <th>negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-09-29 23:41:16+00:00</td>\n",
       "      <td>Mainstream media has done an amazing job at br...</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>Tesla, Inc.</td>\n",
       "      <td>0.0573</td>\n",
       "      <td>0.2339</td>\n",
       "      <td>0.7088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-09-29 23:24:43+00:00</td>\n",
       "      <td>Tesla delivery estimates are at around 364k fr...</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>Tesla, Inc.</td>\n",
       "      <td>0.3148</td>\n",
       "      <td>0.6753</td>\n",
       "      <td>0.0099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-09-29 23:18:08+00:00</td>\n",
       "      <td>3/ Even if I include 63.0M unvested RSUs as of...</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>Tesla, Inc.</td>\n",
       "      <td>0.0785</td>\n",
       "      <td>0.8904</td>\n",
       "      <td>0.0311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-09-29 22:40:07+00:00</td>\n",
       "      <td>@RealDanODowd @WholeMarsBlog @Tesla Hahaha why...</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>Tesla, Inc.</td>\n",
       "      <td>0.0120</td>\n",
       "      <td>0.0724</td>\n",
       "      <td>0.9156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-09-29 22:27:05+00:00</td>\n",
       "      <td>@RealDanODowd @Tesla Stop trying to kill kids,...</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>Tesla, Inc.</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0452</td>\n",
       "      <td>0.9497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80788</th>\n",
       "      <td>2021-10-07 17:11:57+00:00</td>\n",
       "      <td>Some of the fastest growing tech stocks on the...</td>\n",
       "      <td>XPEV</td>\n",
       "      <td>XPeng Inc.</td>\n",
       "      <td>0.9481</td>\n",
       "      <td>0.0486</td>\n",
       "      <td>0.0033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80789</th>\n",
       "      <td>2021-10-04 17:05:59+00:00</td>\n",
       "      <td>With earnings on the horizon, here is a quick ...</td>\n",
       "      <td>XPEV</td>\n",
       "      <td>XPeng Inc.</td>\n",
       "      <td>0.4226</td>\n",
       "      <td>0.5692</td>\n",
       "      <td>0.0082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80790</th>\n",
       "      <td>2021-10-01 04:43:41+00:00</td>\n",
       "      <td>Our record delivery results are a testimony of...</td>\n",
       "      <td>XPEV</td>\n",
       "      <td>XPeng Inc.</td>\n",
       "      <td>0.9137</td>\n",
       "      <td>0.0832</td>\n",
       "      <td>0.0031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80791</th>\n",
       "      <td>2021-10-01 00:03:32+00:00</td>\n",
       "      <td>We delivered 10,412 Smart EVs in Sep 2021, rea...</td>\n",
       "      <td>XPEV</td>\n",
       "      <td>XPeng Inc.</td>\n",
       "      <td>0.9433</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80792</th>\n",
       "      <td>2021-09-30 10:22:52+00:00</td>\n",
       "      <td>Why can XPeng P5 deliver outstanding performan...</td>\n",
       "      <td>XPEV</td>\n",
       "      <td>XPeng Inc.</td>\n",
       "      <td>0.8704</td>\n",
       "      <td>0.1251</td>\n",
       "      <td>0.0046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80793 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Date  \\\n",
       "0      2022-09-29 23:41:16+00:00   \n",
       "1      2022-09-29 23:24:43+00:00   \n",
       "2      2022-09-29 23:18:08+00:00   \n",
       "3      2022-09-29 22:40:07+00:00   \n",
       "4      2022-09-29 22:27:05+00:00   \n",
       "...                          ...   \n",
       "80788  2021-10-07 17:11:57+00:00   \n",
       "80789  2021-10-04 17:05:59+00:00   \n",
       "80790  2021-10-01 04:43:41+00:00   \n",
       "80791  2021-10-01 00:03:32+00:00   \n",
       "80792  2021-09-30 10:22:52+00:00   \n",
       "\n",
       "                                                   Tweet Stock Name  \\\n",
       "0      Mainstream media has done an amazing job at br...       TSLA   \n",
       "1      Tesla delivery estimates are at around 364k fr...       TSLA   \n",
       "2      3/ Even if I include 63.0M unvested RSUs as of...       TSLA   \n",
       "3      @RealDanODowd @WholeMarsBlog @Tesla Hahaha why...       TSLA   \n",
       "4      @RealDanODowd @Tesla Stop trying to kill kids,...       TSLA   \n",
       "...                                                  ...        ...   \n",
       "80788  Some of the fastest growing tech stocks on the...       XPEV   \n",
       "80789  With earnings on the horizon, here is a quick ...       XPEV   \n",
       "80790  Our record delivery results are a testimony of...       XPEV   \n",
       "80791  We delivered 10,412 Smart EVs in Sep 2021, rea...       XPEV   \n",
       "80792  Why can XPeng P5 deliver outstanding performan...       XPEV   \n",
       "\n",
       "      Company Name  positive  neutral  negative  \n",
       "0      Tesla, Inc.    0.0573   0.2339    0.7088  \n",
       "1      Tesla, Inc.    0.3148   0.6753    0.0099  \n",
       "2      Tesla, Inc.    0.0785   0.8904    0.0311  \n",
       "3      Tesla, Inc.    0.0120   0.0724    0.9156  \n",
       "4      Tesla, Inc.    0.0052   0.0452    0.9497  \n",
       "...            ...       ...      ...       ...  \n",
       "80788   XPeng Inc.    0.9481   0.0486    0.0033  \n",
       "80789   XPeng Inc.    0.4226   0.5692    0.0082  \n",
       "80790   XPeng Inc.    0.9137   0.0832    0.0031  \n",
       "80791   XPeng Inc.    0.9433   0.0543    0.0024  \n",
       "80792   XPeng Inc.    0.8704   0.1251    0.0046  \n",
       "\n",
       "[80793 rows x 7 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Stock Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-09-30</td>\n",
       "      <td>260.333344</td>\n",
       "      <td>263.043335</td>\n",
       "      <td>258.333344</td>\n",
       "      <td>258.493347</td>\n",
       "      <td>258.493347</td>\n",
       "      <td>53868000</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-10-01</td>\n",
       "      <td>259.466675</td>\n",
       "      <td>260.260010</td>\n",
       "      <td>254.529999</td>\n",
       "      <td>258.406677</td>\n",
       "      <td>258.406677</td>\n",
       "      <td>51094200</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-10-04</td>\n",
       "      <td>265.500000</td>\n",
       "      <td>268.989990</td>\n",
       "      <td>258.706665</td>\n",
       "      <td>260.510010</td>\n",
       "      <td>260.510010</td>\n",
       "      <td>91449900</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-10-05</td>\n",
       "      <td>261.600006</td>\n",
       "      <td>265.769989</td>\n",
       "      <td>258.066681</td>\n",
       "      <td>260.196655</td>\n",
       "      <td>260.196655</td>\n",
       "      <td>55297800</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-10-06</td>\n",
       "      <td>258.733337</td>\n",
       "      <td>262.220001</td>\n",
       "      <td>257.739990</td>\n",
       "      <td>260.916656</td>\n",
       "      <td>260.916656</td>\n",
       "      <td>43898400</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6295</th>\n",
       "      <td>2022-09-23</td>\n",
       "      <td>13.090000</td>\n",
       "      <td>13.892000</td>\n",
       "      <td>12.860000</td>\n",
       "      <td>13.710000</td>\n",
       "      <td>13.710000</td>\n",
       "      <td>28279600</td>\n",
       "      <td>XPEV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6296</th>\n",
       "      <td>2022-09-26</td>\n",
       "      <td>14.280000</td>\n",
       "      <td>14.830000</td>\n",
       "      <td>14.070000</td>\n",
       "      <td>14.370000</td>\n",
       "      <td>14.370000</td>\n",
       "      <td>27891300</td>\n",
       "      <td>XPEV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6297</th>\n",
       "      <td>2022-09-27</td>\n",
       "      <td>14.580000</td>\n",
       "      <td>14.800000</td>\n",
       "      <td>13.580000</td>\n",
       "      <td>13.710000</td>\n",
       "      <td>13.710000</td>\n",
       "      <td>21160800</td>\n",
       "      <td>XPEV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6298</th>\n",
       "      <td>2022-09-28</td>\n",
       "      <td>13.050000</td>\n",
       "      <td>13.421000</td>\n",
       "      <td>12.690000</td>\n",
       "      <td>13.330000</td>\n",
       "      <td>13.330000</td>\n",
       "      <td>31799400</td>\n",
       "      <td>XPEV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6299</th>\n",
       "      <td>2022-09-29</td>\n",
       "      <td>12.550000</td>\n",
       "      <td>12.850000</td>\n",
       "      <td>11.850000</td>\n",
       "      <td>12.110000</td>\n",
       "      <td>12.110000</td>\n",
       "      <td>33044800</td>\n",
       "      <td>XPEV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6300 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date        Open        High         Low       Close   Adj Close  \\\n",
       "0     2021-09-30  260.333344  263.043335  258.333344  258.493347  258.493347   \n",
       "1     2021-10-01  259.466675  260.260010  254.529999  258.406677  258.406677   \n",
       "2     2021-10-04  265.500000  268.989990  258.706665  260.510010  260.510010   \n",
       "3     2021-10-05  261.600006  265.769989  258.066681  260.196655  260.196655   \n",
       "4     2021-10-06  258.733337  262.220001  257.739990  260.916656  260.916656   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "6295  2022-09-23   13.090000   13.892000   12.860000   13.710000   13.710000   \n",
       "6296  2022-09-26   14.280000   14.830000   14.070000   14.370000   14.370000   \n",
       "6297  2022-09-27   14.580000   14.800000   13.580000   13.710000   13.710000   \n",
       "6298  2022-09-28   13.050000   13.421000   12.690000   13.330000   13.330000   \n",
       "6299  2022-09-29   12.550000   12.850000   11.850000   12.110000   12.110000   \n",
       "\n",
       "        Volume Stock Name  \n",
       "0     53868000       TSLA  \n",
       "1     51094200       TSLA  \n",
       "2     91449900       TSLA  \n",
       "3     55297800       TSLA  \n",
       "4     43898400       TSLA  \n",
       "...        ...        ...  \n",
       "6295  28279600       XPEV  \n",
       "6296  27891300       XPEV  \n",
       "6297  21160800       XPEV  \n",
       "6298  31799400       XPEV  \n",
       "6299  33044800       XPEV  \n",
       "\n",
       "[6300 rows x 8 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['TSLA', 'MSFT', 'PG', 'META', 'AMZN', 'GOOG', 'AMD', 'AAPL',\n",
       "       'NFLX', 'TSM', 'KO', 'F', 'COST', 'DIS', 'VZ', 'CRM', 'INTC', 'BA',\n",
       "       'BX', 'NOC', 'PYPL', 'ENPH', 'NIO', 'ZS', 'XPEV'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices_hist['Stock Name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sent['Date'] = pd.to_datetime(data_sent['Date'])  # Convert 'Date' column to datetime\n",
    "\n",
    "data_sent['Day'] = data_sent['Date'].dt.date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sent.drop(columns=['Date','Tweet', 'Company Name'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_for_inp(data, prices, ticker : str):\n",
    "\n",
    "    data = data[data['Stock Name'] == ticker]\n",
    "    data = data.drop(columns=['Stock Name'], inplace = False)\n",
    "    twitter_df = data.groupby('Day').mean()\n",
    "    twitter_df['Date'] = twitter_df.index\n",
    "    twitter_df = twitter_df.reset_index(drop=True)\n",
    "\n",
    "    prices['Date'] = pd.to_datetime(prices['Date'])\n",
    "    prices = prices[prices['Stock Name'] == ticker]\n",
    "\n",
    "    twitter_df['Date'] = pd.to_datetime(twitter_df['Date'])\n",
    "\n",
    "    df = pd.merge(twitter_df, prices, on='Date', how='inner')\n",
    "\n",
    "    df = df[df['Stock Name'] == ticker]\n",
    "\n",
    "    df = df.sort_values(by='Date')\n",
    "    df = df.drop(columns='Date')\n",
    "    df = df.drop(columns='Stock Name')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>positive</th>\n",
       "      <th>neutral</th>\n",
       "      <th>negative</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.352172</td>\n",
       "      <td>0.433046</td>\n",
       "      <td>0.214780</td>\n",
       "      <td>260.333344</td>\n",
       "      <td>263.043335</td>\n",
       "      <td>258.333344</td>\n",
       "      <td>258.493347</td>\n",
       "      <td>258.493347</td>\n",
       "      <td>53868000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.412740</td>\n",
       "      <td>0.414943</td>\n",
       "      <td>0.172319</td>\n",
       "      <td>259.466675</td>\n",
       "      <td>260.260010</td>\n",
       "      <td>254.529999</td>\n",
       "      <td>258.406677</td>\n",
       "      <td>258.406677</td>\n",
       "      <td>51094200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.364087</td>\n",
       "      <td>0.436857</td>\n",
       "      <td>0.199058</td>\n",
       "      <td>265.500000</td>\n",
       "      <td>268.989990</td>\n",
       "      <td>258.706665</td>\n",
       "      <td>260.510010</td>\n",
       "      <td>260.510010</td>\n",
       "      <td>91449900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.387511</td>\n",
       "      <td>0.394376</td>\n",
       "      <td>0.218107</td>\n",
       "      <td>261.600006</td>\n",
       "      <td>265.769989</td>\n",
       "      <td>258.066681</td>\n",
       "      <td>260.196655</td>\n",
       "      <td>260.196655</td>\n",
       "      <td>55297800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.397635</td>\n",
       "      <td>0.418137</td>\n",
       "      <td>0.184232</td>\n",
       "      <td>258.733337</td>\n",
       "      <td>262.220001</td>\n",
       "      <td>257.739990</td>\n",
       "      <td>260.916656</td>\n",
       "      <td>260.916656</td>\n",
       "      <td>43898400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>0.438738</td>\n",
       "      <td>0.374231</td>\n",
       "      <td>0.187038</td>\n",
       "      <td>283.089996</td>\n",
       "      <td>284.500000</td>\n",
       "      <td>272.820007</td>\n",
       "      <td>275.329987</td>\n",
       "      <td>275.329987</td>\n",
       "      <td>63748400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>0.417667</td>\n",
       "      <td>0.375392</td>\n",
       "      <td>0.206940</td>\n",
       "      <td>271.829987</td>\n",
       "      <td>284.089996</td>\n",
       "      <td>270.309998</td>\n",
       "      <td>276.010010</td>\n",
       "      <td>276.010010</td>\n",
       "      <td>58076900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>0.467805</td>\n",
       "      <td>0.357291</td>\n",
       "      <td>0.174904</td>\n",
       "      <td>283.839996</td>\n",
       "      <td>288.670013</td>\n",
       "      <td>277.510010</td>\n",
       "      <td>282.940002</td>\n",
       "      <td>282.940002</td>\n",
       "      <td>61925200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>0.471717</td>\n",
       "      <td>0.347468</td>\n",
       "      <td>0.180812</td>\n",
       "      <td>283.079987</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>277.570007</td>\n",
       "      <td>287.809998</td>\n",
       "      <td>287.809998</td>\n",
       "      <td>54664800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>0.344916</td>\n",
       "      <td>0.416191</td>\n",
       "      <td>0.238888</td>\n",
       "      <td>282.760010</td>\n",
       "      <td>283.649994</td>\n",
       "      <td>265.779999</td>\n",
       "      <td>268.209991</td>\n",
       "      <td>268.209991</td>\n",
       "      <td>77620600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>252 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     positive   neutral  negative        Open        High         Low  \\\n",
       "0    0.352172  0.433046  0.214780  260.333344  263.043335  258.333344   \n",
       "1    0.412740  0.414943  0.172319  259.466675  260.260010  254.529999   \n",
       "2    0.364087  0.436857  0.199058  265.500000  268.989990  258.706665   \n",
       "3    0.387511  0.394376  0.218107  261.600006  265.769989  258.066681   \n",
       "4    0.397635  0.418137  0.184232  258.733337  262.220001  257.739990   \n",
       "..        ...       ...       ...         ...         ...         ...   \n",
       "247  0.438738  0.374231  0.187038  283.089996  284.500000  272.820007   \n",
       "248  0.417667  0.375392  0.206940  271.829987  284.089996  270.309998   \n",
       "249  0.467805  0.357291  0.174904  283.839996  288.670013  277.510010   \n",
       "250  0.471717  0.347468  0.180812  283.079987  289.000000  277.570007   \n",
       "251  0.344916  0.416191  0.238888  282.760010  283.649994  265.779999   \n",
       "\n",
       "          Close   Adj Close    Volume  \n",
       "0    258.493347  258.493347  53868000  \n",
       "1    258.406677  258.406677  51094200  \n",
       "2    260.510010  260.510010  91449900  \n",
       "3    260.196655  260.196655  55297800  \n",
       "4    260.916656  260.916656  43898400  \n",
       "..          ...         ...       ...  \n",
       "247  275.329987  275.329987  63748400  \n",
       "248  276.010010  276.010010  58076900  \n",
       "249  282.940002  282.940002  61925200  \n",
       "250  287.809998  287.809998  54664800  \n",
       "251  268.209991  268.209991  77620600  \n",
       "\n",
       "[252 rows x 9 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_for_inp(data_sent, prices_hist, 'TSLA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(data, test_size : int):\n",
    "    train_size = len(data) - test_size\n",
    "    data_train = data[0:train_size]\n",
    "    data_test = data[train_size:]\n",
    "    return data_train, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_X_y(df, target_column, train : bool, sent : bool):\n",
    "    \n",
    "    target_df_series = pd.DataFrame(df[target_column])\n",
    "    \n",
    "    if train:\n",
    "        data = pd.DataFrame(df.iloc[:, :])\n",
    "    else:\n",
    "        data = df[['positive', 'negative', 'neutral']]\n",
    "\n",
    "    if sent is False:\n",
    "        data = df.drop(columns=['positive', 'negative', 'neutral'])\n",
    "\n",
    "    return data, target_df_series\n",
    "\n",
    "def normalize_data(X, y, range):\n",
    "\n",
    "    '''\n",
    "    df: dataframe object\n",
    "    range: type tuple -> (lower_bound, upper_bound)\n",
    "        lower_bound: int\n",
    "        upper_bound: int\n",
    "    target_column: type str -> should reflect closing price of stock\n",
    "    '''\n",
    "    X_scaler = MinMaxScaler(feature_range=range)\n",
    "    y_scaler = MinMaxScaler(feature_range=range)\n",
    "    X_scaler.fit(X)\n",
    "    y_scaler.fit(y)\n",
    "\n",
    "    X_scale_dataset = X_scaler.fit_transform(X)\n",
    "    y_scale_dataset = y_scaler.fit_transform(y)\n",
    "    \n",
    "    dump(X_scaler, open('X_scaler.pkl', 'wb'))\n",
    "    dump(y_scaler, open('y_scaler.pkl', 'wb'))\n",
    "\n",
    "    return (X_scale_dataset,y_scale_dataset,y_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_out(predictions, test_y):\n",
    "    # determine rmse\n",
    "\n",
    "    # For regression problems\n",
    "    mse = mean_squared_error(predictions, test_y)\n",
    "    mae = mean_absolute_error(predictions, test_y)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return mse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(predictions, test_y, ticker):\n",
    "    \n",
    "    #test_y = [i[4] for i in test_y]\n",
    "    # Plotting the two series\n",
    "    plt.plot(predictions, label='Predicted')\n",
    "    plt.plot(test_y, label='Test')\n",
    "\n",
    "    # Adding labels and title\n",
    "    plt.xlabel('Day')\n",
    "    plt.ylabel('Price')\n",
    "    plt.title(f'Prediction of {ticker}')\n",
    "\n",
    "    # Adding legend\n",
    "    plt.legend()\n",
    "\n",
    "    # Displaying the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflect_along_x_axis(array, first_value):\n",
    "    # Get the y-coordinate of the first value of the prediction array\n",
    "    y_intercept = first_value\n",
    "    \n",
    "    # Reflect the array along the x-axis at this y-coordinate\n",
    "    reflected_array = 2 * y_intercept - array\n",
    "    \n",
    "    return reflected_array\n",
    "\n",
    "\n",
    "def plot_predictions(test_y_actual, predictions, random_walk, ticker):\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    \n",
    "    # Flatten the arrays for easier plotting\n",
    "    #test_y_flat = reflect_along_x_axis(test_y_actual.reset_index(drop=True), list(test_y_actual.reset_index(drop=True)['Close'].to_numpy())[0])\n",
    "    test_y_flat = test_y_actual\n",
    "    predictions_flat = predictions\n",
    "\n",
    "    plt.plot(random_walk, color='green', label='Random Walk')\n",
    "    \n",
    "    # Plotting the actual values\n",
    "    plt.plot(test_y_flat, color='blue', label='Actual Prices')\n",
    "    \n",
    "    # Plotting the predicted values\n",
    "    plt.plot(predictions_flat, color='red', linestyle='--', label='Predicted Prices')\n",
    "    \n",
    "    # Adding title and labels\n",
    "    plt.title(f'{ticker} Stock Price Prediction')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Stock Price')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def random_walk_forecast(history, n_steps, variability_factor=0.05):\n",
    "    \"\"\"\n",
    "    Generate a random walk forecast with variability.\n",
    "\n",
    "    Parameters:\n",
    "        history (array-like): Array containing the historical values of the time series.\n",
    "        n_steps (int): Number of steps to forecast into the future.\n",
    "        variability_factor (float): Factor controlling the amount of variability in the forecast.\n",
    "\n",
    "    Returns:\n",
    "        array: Array containing the forecasted values.\n",
    "    \"\"\"\n",
    "    forecast = []\n",
    "    for _ in range(n_steps):\n",
    "        # Generate a random noise term\n",
    "        noise = np.random.normal(loc=0, scale=variability_factor * np.std(history))\n",
    "        # Add noise to the last observed value\n",
    "        forecast.append(history[-1] + noise)\n",
    "    return np.array(forecast)\n",
    "\n",
    "# Example usage\n",
    "# Assuming 'prices_hist' is your time series data\n",
    "# Let's say you want to forecast 30 days into the future\n",
    "n_steps = 21\n",
    "history = prices_hist[prices_hist['Stock Name'] == 'TSLA']['Close'].values.astype(float)  # Assuming 'Close' is the column containing the time series data\n",
    "\n",
    "# Make the forecast with some variability (adjust variability_factor as needed)\n",
    "variability_factor = 0.05  # Adjust this parameter to control the amount of variability\n",
    "forecast = random_walk_forecast(history, n_steps, variability_factor)\n",
    "\n",
    "# Generate integers for the x-axis\n",
    "x_values = np.arange(len(history), len(history) + n_steps)\n",
    "\n",
    "# Plotting\n",
    "plt.plot(x_values, forecast, label='Random Walk Forecast with Variability', linestyle='--')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Random Walk Forecast with Variability')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, ticker : str):\n",
    "\n",
    "    tsl_train, tsl_test = split_train_test(data, 20)\n",
    "\n",
    "    tsl_train_x, tsl_train_y = split_X_y(tsl_train, 'Close', True, True)\n",
    "    tsl_test_x, tsl_test_y = split_X_y(tsl_test, 'Close', True, True)\n",
    "\n",
    "    tsl_train_x, tsl_train_y, y_scaler_train =  normalize_data(tsl_train_x, tsl_train_y, (-1,1))\n",
    "    tsl_test_x, tsl_test_y, y_scaler_test =  normalize_data(tsl_test_x, tsl_test_y, (-1,1))\n",
    "\n",
    "    print(tsl_train_x.shape)\n",
    "    print(tsl_train_y.shape)\n",
    "\n",
    "    # Define LSTM model\n",
    "    model = Sequential([\n",
    "        LSTM(units=200, return_sequences=True, input_shape=(tsl_train_x.shape[1], 1)),\n",
    "        LSTM(units=200, return_sequences=True, input_shape=(tsl_train_x.shape[1], 1)),\n",
    "        LSTM(units=50, return_sequences=True, input_shape=(tsl_train_x.shape[1], 1)),\n",
    "        LSTM(units=50, return_sequences=False),\n",
    "        Dense(units=25),\n",
    "        Dense(units=1)\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(tsl_train_x, tsl_train_y, epochs=8, batch_size=32)\n",
    "\n",
    "    # Perform one-step ahead forecasting for 20 days\n",
    "    y_pred = one_step_ahead_forecasting(model, tsl_test_x)\n",
    "\n",
    "    # Inverse scale the predictions\n",
    "    predictions = y_scaler_test.inverse_transform(np.array(y_pred).reshape(-1, 1))\n",
    "\n",
    "    test_y = y_scaler_test.inverse_transform(tsl_test_y)\n",
    "\n",
    "    return predictions, test_y\n",
    "\n",
    "# Adjusted function for one-step ahead forecasting for 20 days\n",
    "def one_step_ahead_forecasting(model, X_test):\n",
    "    \"\"\"\n",
    "    Perform one-step ahead forecasting for 20 days.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained LSTM model.\n",
    "    - X_test: Test data in the shape (samples, time steps, features).\n",
    "\n",
    "    Returns:\n",
    "    - y_pred: Predicted stock prices for the next 20 days.\n",
    "    \"\"\"\n",
    "\n",
    "    y_pred = []\n",
    "    X_current = X_test[0]  # Start with the first data point in the test set\n",
    "\n",
    "    for i in range(20):\n",
    "        # Select only the relevant features for prediction\n",
    "        X_current_pred = X_current\n",
    "        # Predict the next value\n",
    "        pred = model.predict(np.array([X_current_pred]))\n",
    "        y_pred.append(pred[0])\n",
    "\n",
    "        print(X_current)\n",
    "        print(X_current[1:])\n",
    "        print(pred)\n",
    "        # Update X_current by appending the predicted value and removing the first element\n",
    "        X_current = pred[0]\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mses = []\n",
    "for i in range(10):\n",
    "    ticker = 'TSLA'\n",
    "    predictions, test_y = train(df_for_inp(data_sent, prices_hist, 'TSLA'), ticker)\n",
    "    mse = rmse_out(predictions, test_y)\n",
    "    print(f'rmse: {mse}')\n",
    "    mses.append(mse)\n",
    "    # plot(predictions, test_y, ticker)\n",
    "    predictions = predictions[:len(test_y)]  # Align lengths if necessary\n",
    "    plot_predictions(test_y, predictions, test_y, ticker)\n",
    "\n",
    "print(sum([i[1] for i in mses])/10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding Window (Rolling Window) Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(x, y, window_size, prediction_length):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(x) - window_size - prediction_length + 1):\n",
    "        X.append(x[i:(i + window_size)])\n",
    "        Y.append(y[(i + window_size):(i + window_size + prediction_length)])\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(train_X, train_y, test_X, test_y):\n",
    "    num_train_samples, num_time_steps, num_features = train_X.shape\n",
    "    num_test_samples = test_X.shape[0]\n",
    "    num_prediction_steps = train_y.shape[1]\n",
    "\n",
    "    train_X_2d = train_X.reshape(num_train_samples * num_time_steps, num_features)\n",
    "    test_X_2d = test_X.reshape(num_test_samples * num_time_steps, num_features)\n",
    "\n",
    "    scaler_X = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    train_X_scaled = scaler_X.fit_transform(train_X_2d).reshape(num_train_samples, num_time_steps, num_features)\n",
    "    test_X_scaled = scaler_X.transform(test_X_2d).reshape(num_test_samples, num_time_steps, num_features)\n",
    "    \n",
    "    train_y_2d = train_y.reshape(num_train_samples, num_prediction_steps)\n",
    "    test_y_2d = test_y.reshape(num_test_samples, num_prediction_steps)\n",
    "    \n",
    "    train_y_scaled = scaler_y.fit_transform(train_y_2d)\n",
    "    test_y_scaled = scaler_y.transform(test_y_2d)\n",
    "\n",
    "    return train_X_scaled, train_y_scaled, test_X_scaled, test_y_scaled, scaler_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_importance(model, X, y, metric=mean_squared_error):\n",
    "    baseline_score = metric(y, model.predict(X))\n",
    "    importances = []\n",
    "    for i in range(X.shape[2]):\n",
    "        X_permuted = X.copy()\n",
    "        np.random.shuffle(X_permuted[:, :, i])\n",
    "        permuted_score = metric(y, model.predict(X_permuted))\n",
    "        importances.append(permuted_score - baseline_score)\n",
    "    return np.array(importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, prices, ticker, window_size=10, prediction_length=1, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_days=5, sent=True):\n",
    "    df = df_for_inp(data, prices, ticker)\n",
    "    \n",
    "    tsl_train, tsl_test = split_train_test(df, predict_days)\n",
    "    train_X, train_y = split_X_y(tsl_train, 'Close', True, sent)\n",
    "    test_X, test_y = split_X_y(tsl_test, 'Close', True, sent)\n",
    "    \n",
    "    tsl_train_x, tsl_train_y = create_dataset(train_X, train_y, window_size, prediction_length)\n",
    "    tsl_test_x, tsl_test_y = create_dataset(test_X, test_y, window_size, prediction_length)\n",
    "    \n",
    "    tsl_train_x, tsl_train_y, tsl_test_x, tsl_test_y, y_scaler = normalize_data(\n",
    "        tsl_train_x, tsl_train_y, tsl_test_x, tsl_test_y)\n",
    "\n",
    "    model = Sequential([\n",
    "        LSTM(units=lstm_units, return_sequences=True, input_shape=(window_size, tsl_train_x.shape[2])),\n",
    "        Dropout(rate=dropout_rate),\n",
    "        LSTM(units=lstm_units, return_sequences=True),\n",
    "        Dropout(rate=dropout_rate),\n",
    "        LSTM(units=lstm_units, return_sequences=False),\n",
    "        Dropout(rate=dropout_rate),\n",
    "        Dense(units=25),\n",
    "        Dense(units=prediction_length)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "    model.fit(tsl_train_x, tsl_train_y, epochs=100, batch_size=batch_size, validation_split=0.1, callbacks=[early_stopping])\n",
    "\n",
    "    # Assuming X_test and y_test are your test data\n",
    "    importances = permutation_importance(model, tsl_test_x, tsl_test_y)\n",
    "\n",
    "        \n",
    "    predictions = model.predict(tsl_test_x)\n",
    "    predictions = y_scaler.inverse_transform(predictions)\n",
    "    tsl_test_y = y_scaler.inverse_transform(tsl_test_y)\n",
    "\n",
    "    return predictions, test_y, test_X, importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to append results to JSON file\n",
    "def append_to_json(file_path, data):\n",
    "    with open(file_path, 'a') as f:\n",
    "        f.write(json.dumps(data) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 288ms/step - loss: 0.4135 - val_loss: 0.0086\n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.2676 - val_loss: 0.0168\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0930 - val_loss: 0.0558\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0639 - val_loss: 0.0283\n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0442 - val_loss: 0.0126\n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0412 - val_loss: 0.0090\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0272 - val_loss: 0.0053\n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0293 - val_loss: 0.0039\n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0289 - val_loss: 0.0037\n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0289 - val_loss: 0.0039\n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0250 - val_loss: 0.0053\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0210 - val_loss: 0.0054\n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0243 - val_loss: 0.0063\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0216 - val_loss: 0.0074\n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0238 - val_loss: 0.0071\n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0237 - val_loss: 0.0075\n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0239 - val_loss: 0.0066\n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0232 - val_loss: 0.0076\n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0229 - val_loss: 0.0072\n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0219 - val_loss: 0.0077\n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0204 - val_loss: 0.0080\n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0243 - val_loss: 0.0087\n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0213 - val_loss: 0.0084\n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0191 - val_loss: 0.0096\n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0220 - val_loss: 0.0096\n",
      "Epoch 26/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0197 - val_loss: 0.0093\n",
      "Epoch 27/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0205 - val_loss: 0.0105\n",
      "Epoch 28/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0191 - val_loss: 0.0101\n",
      "Epoch 29/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0195 - val_loss: 0.0100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 506ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "RMSE: (96.25071835212876, 7.8909912109375)\n",
      "RMSE Random Walk: (232.08973279020228, 12.337782727884715)\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n",
      "[183.08999634 183.16999817 169.27000427 159.1499939  169.58000183\n",
      " 160.72000122 159.92999268 160.19000244 168.80000305 167.11000061\n",
      " 171.23712158 168.94671631 166.47363281 168.44624329 172.76239014\n",
      " 165.72520447 166.75434875 166.23789978 154.99864197 160.74819946\n",
      " 158.67376709 162.24629211 155.33151245 159.29547119 160.79370117\n",
      " 158.7076416  156.99822998 157.27059937 158.00390625 158.97210693]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 152ms/step - loss: 0.4124 - val_loss: 0.0104\n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.2849 - val_loss: 0.0138\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1573 - val_loss: 0.0211\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0973 - val_loss: 0.0120\n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0486 - val_loss: 0.0145\n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0362 - val_loss: 0.0118\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0337 - val_loss: 0.0063\n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0320 - val_loss: 0.0049\n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0238 - val_loss: 0.0044\n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0262 - val_loss: 0.0044\n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0264 - val_loss: 0.0048\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0269 - val_loss: 0.0055\n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0245 - val_loss: 0.0070\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0260 - val_loss: 0.0072\n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0202 - val_loss: 0.0079\n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0212 - val_loss: 0.0082\n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0206 - val_loss: 0.0085\n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0209 - val_loss: 0.0085\n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0208 - val_loss: 0.0084\n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0248 - val_loss: 0.0094\n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0249 - val_loss: 0.0080\n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0247 - val_loss: 0.0106\n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0226 - val_loss: 0.0091\n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0216 - val_loss: 0.0105\n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0208 - val_loss: 0.0100\n",
      "Epoch 26/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0201 - val_loss: 0.0105\n",
      "Epoch 27/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0187 - val_loss: 0.0094\n",
      "Epoch 28/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0205 - val_loss: 0.0111\n",
      "Epoch 29/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0223 - val_loss: 0.0106\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 409ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "RMSE: (95.70095299340319, 7.842893981933594)\n",
      "RMSE Random Walk: (225.15988428885166, 12.156392589834809)\n",
      "[183.08999634 183.16999817 169.27000427 159.1499939  169.58000183\n",
      " 160.72000122 159.92999268 160.19000244 168.80000305 167.11000061\n",
      " 171.23712158 168.94671631 166.47363281 168.44624329 172.76239014\n",
      " 165.72520447 166.75434875 166.23789978 154.99864197 160.74819946\n",
      " 158.67376709 162.24629211 155.33151245 159.29547119 160.79370117\n",
      " 158.7076416  156.99822998 157.27059937 158.00390625 158.97210693]\n",
      "[183.08999634 183.16999817 169.27000427 159.1499939  169.58000183\n",
      " 160.72000122 159.92999268 160.19000244 168.80000305 167.11000061\n",
      " 174.67495728 181.98713684 175.34515381 173.21031189 169.02999878\n",
      " 167.85945129 161.13435364 167.936203   157.86103821 165.28660583\n",
      " 171.54447937 160.20014954 168.08807373 167.58325195 156.19041443\n",
      " 155.15325928 154.60594177 155.23199463 156.27629089 157.40325928]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 121ms/step - loss: 0.3627 - val_loss: 0.0569\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1939 - val_loss: 0.0401\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1097 - val_loss: 0.0296\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0679 - val_loss: 0.0227\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0455 - val_loss: 0.0163\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0375 - val_loss: 0.0294\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0340 - val_loss: 0.0350\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0329 - val_loss: 0.0481\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0294 - val_loss: 0.0479\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0270 - val_loss: 0.0490\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0306 - val_loss: 0.0404\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0292 - val_loss: 0.0406\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0265 - val_loss: 0.0366\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0265 - val_loss: 0.0377\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0269 - val_loss: 0.0380\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0279 - val_loss: 0.0401\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0288 - val_loss: 0.0393\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0241 - val_loss: 0.0413\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0251 - val_loss: 0.0381\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0248 - val_loss: 0.0410\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0248 - val_loss: 0.0382\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0253 - val_loss: 0.0404\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0236 - val_loss: 0.0394\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0258 - val_loss: 0.0447\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0266 - val_loss: 0.0351\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 395ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "RMSE: (308.9593032050005, 17.072711181640624)\n",
      "RMSE Random Walk: (50.253401117015656, 5.593443989748506)\n",
      "[366.17999268 366.33999634 338.54000854 318.29998779 339.16000366\n",
      " 321.44000244 319.85998535 320.38000488 337.6000061  334.22000122\n",
      " 345.91207886 350.93385315 341.81878662 341.65655518 341.79238892\n",
      " 333.58465576 327.88870239 334.17410278 312.85968018 326.0348053\n",
      " 330.21824646 322.44644165 323.41958618 326.87872314 316.9841156\n",
      " 313.86090088 311.60417175 312.50259399 314.28019714 316.37536621]\n",
      "[142.30000305 138.22999573 133.22000122 133.61999512 133.80000305\n",
      " 137.27999878 130.75       129.78999329 128.72999573 126.76999664\n",
      " 140.95774841 143.82951355 143.63235474 144.55227661 143.21763611\n",
      " 140.7519989  144.04293823 141.73757935 145.37547302 145.5163269\n",
      " 143.647995   145.07626343 137.97731018 139.60244751 137.91786194\n",
      " 136.78323364 136.03466797 135.60340881 135.08729553 134.59991455]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 138ms/step - loss: 0.3809 - val_loss: 0.0732\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.2507 - val_loss: 0.0526\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1573 - val_loss: 0.0326\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0942 - val_loss: 0.0204\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0550 - val_loss: 0.0192\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0396 - val_loss: 0.0242\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0318 - val_loss: 0.0324\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0325 - val_loss: 0.0338\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0338 - val_loss: 0.0394\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0315 - val_loss: 0.0361\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0282 - val_loss: 0.0373\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0288 - val_loss: 0.0359\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0277 - val_loss: 0.0356\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0289 - val_loss: 0.0361\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0273 - val_loss: 0.0347\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0267 - val_loss: 0.0371\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0289 - val_loss: 0.0371\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0268 - val_loss: 0.0366\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0271 - val_loss: 0.0379\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0235 - val_loss: 0.0393\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0288 - val_loss: 0.0347\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0298 - val_loss: 0.0398\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0268 - val_loss: 0.0362\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0262 - val_loss: 0.0389\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0272 - val_loss: 0.0423\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 382ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "RMSE: (279.94809848443253, 15.334182739257812)\n",
      "RMSE Random Walk: (49.617418111451556, 5.568007879952416)\n",
      "[508.47999573 504.56999207 471.76000977 451.91998291 472.96000671\n",
      " 458.72000122 450.60998535 450.16999817 466.33000183 460.98999786\n",
      " 486.86982727 494.7633667  485.45114136 486.20883179 485.01002502\n",
      " 474.33665466 471.93164062 475.91168213 458.2351532  471.5511322\n",
      " 473.86624146 467.52270508 461.39689636 466.48117065 454.90197754\n",
      " 450.64413452 447.63883972 448.10600281 449.36749268 450.97528076]\n",
      "[142.30000305 138.22999573 133.22000122 133.61999512 133.80000305\n",
      " 137.27999878 130.75       129.78999329 128.72999573 126.76999664\n",
      " 130.14250183 139.44862366 139.5994873  142.34519958 137.84738159\n",
      " 141.12736511 143.69087219 142.59184265 138.14659119 144.12513733\n",
      " 140.08850098 140.38600159 150.44059753 126.85450745 139.60479736\n",
      " 138.40861511 137.42851257 136.92555237 136.3230896  135.64849854]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 113ms/step - loss: 0.3692 - val_loss: 0.1639\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1882 - val_loss: 0.0584\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0944 - val_loss: 0.0468\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0571 - val_loss: 0.0274\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0457 - val_loss: 0.0209\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0375 - val_loss: 0.0297\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0360 - val_loss: 0.0300\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0360 - val_loss: 0.0467\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0364 - val_loss: 0.0446\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0353 - val_loss: 0.0490\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0337 - val_loss: 0.0433\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0353 - val_loss: 0.0495\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0325 - val_loss: 0.0458\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0317 - val_loss: 0.0483\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0297 - val_loss: 0.0533\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0293 - val_loss: 0.0500\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0325 - val_loss: 0.0587\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0270 - val_loss: 0.0558\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0296 - val_loss: 0.0602\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0281 - val_loss: 0.0681\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0280 - val_loss: 0.0532\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0284 - val_loss: 0.0728\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0260 - val_loss: 0.0676\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0271 - val_loss: 0.0635\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0262 - val_loss: 0.0688\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 441ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "RMSE: (120.79281931965379, 10.539051055908203)\n",
      "RMSE Random Walk: (29.974602727654144, 4.445116048077895)\n",
      "[650.77999878 642.79998779 604.98001099 585.53997803 606.76000977\n",
      " 596.         581.35998535 579.95999146 595.05999756 587.75999451\n",
      " 617.0123291  634.21199036 625.05062866 628.55403137 622.85740662\n",
      " 615.46401978 615.62251282 618.50352478 596.38174438 615.67626953\n",
      " 613.95474243 607.90870667 611.8374939  593.3356781  594.5067749\n",
      " 589.05274963 585.06735229 585.03155518 585.69058228 586.6237793 ]\n",
      "[174.1499939  171.52000427 167.57000732 167.22999573 167.52999878\n",
      " 170.02999878 163.61999512 161.38000488 158.91000366 157.22000122\n",
      " 165.41876221 164.37776184 168.13400269 162.80259705 165.65008545\n",
      " 166.77729797 168.91668701 162.99627686 161.003479   163.83039856\n",
      " 165.29162598 164.13172913 172.56796265 165.85296631 162.22833252\n",
      " 161.82594299 161.44174194 161.1366272  160.8170929  160.43962097]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 117ms/step - loss: 0.3595 - val_loss: 0.2034\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2481 - val_loss: 0.0883\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1158 - val_loss: 0.0366\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0663 - val_loss: 0.0394\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0476 - val_loss: 0.0228\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0422 - val_loss: 0.0317\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0395 - val_loss: 0.0402\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0375 - val_loss: 0.0360\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0364 - val_loss: 0.0442\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0347 - val_loss: 0.0369\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0366 - val_loss: 0.0468\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0352 - val_loss: 0.0394\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0364 - val_loss: 0.0486\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0353 - val_loss: 0.0409\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0328 - val_loss: 0.0448\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0362 - val_loss: 0.0406\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0323 - val_loss: 0.0438\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0351 - val_loss: 0.0425\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0343 - val_loss: 0.0401\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0325 - val_loss: 0.0421\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0356 - val_loss: 0.0406\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0337 - val_loss: 0.0406\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0325 - val_loss: 0.0391\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0319 - val_loss: 0.0393\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0338 - val_loss: 0.0403\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 378ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "RMSE: (185.13608003660337, 12.672310638427735)\n",
      "RMSE Random Walk: (29.8629449742568, 4.451635928794181)\n",
      "[824.92999268 814.31999207 772.55001831 752.76997375 774.29000854\n",
      " 766.02999878 744.97998047 741.33999634 753.97000122 744.97999573\n",
      " 782.43109131 798.5897522  793.18463135 791.35662842 788.50749207\n",
      " 782.24131775 784.53919983 781.49980164 757.38522339 779.50666809\n",
      " 779.24636841 772.04043579 784.40545654 759.18864441 756.73510742\n",
      " 750.87869263 746.50909424 746.16818237 746.50767517 747.06340027]\n",
      "[174.1499939  171.52000427 167.57000732 167.22999573 167.52999878\n",
      " 170.02999878 163.61999512 161.38000488 158.91000366 157.22000122\n",
      " 164.56573486 170.86425781 163.41793823 167.66427612 160.55769348\n",
      " 166.03245544 169.11865234 163.38790894 165.82400513 174.52760315\n",
      " 161.87992859 169.21820068 167.28643799 161.18464661 168.4903717\n",
      " 167.92454529 167.31829834 166.85621643 166.39321899 165.79379272]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 115ms/step - loss: 0.3758 - val_loss: 0.0273\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2629 - val_loss: 0.0515\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1606 - val_loss: 0.0360\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0865 - val_loss: 0.0283\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0533 - val_loss: 0.0241\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0401 - val_loss: 0.0125\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0320 - val_loss: 0.0125\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0320 - val_loss: 0.0171\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0301 - val_loss: 0.0132\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0277 - val_loss: 0.0122\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0263 - val_loss: 0.0101\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0271 - val_loss: 0.0098\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0279 - val_loss: 0.0100\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0263 - val_loss: 0.0099\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0267 - val_loss: 0.0101\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0253 - val_loss: 0.0113\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0274 - val_loss: 0.0102\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0254 - val_loss: 0.0100\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0253 - val_loss: 0.0096\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0272 - val_loss: 0.0095\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0281 - val_loss: 0.0092\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0271 - val_loss: 0.0099\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0277 - val_loss: 0.0093\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0253 - val_loss: 0.0096\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0267 - val_loss: 0.0093\n",
      "Epoch 26/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0258 - val_loss: 0.0103\n",
      "Epoch 27/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0255 - val_loss: 0.0093\n",
      "Epoch 28/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0261 - val_loss: 0.0097\n",
      "Epoch 29/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0261 - val_loss: 0.0091\n",
      "Epoch 30/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0275 - val_loss: 0.0094\n",
      "Epoch 31/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0241 - val_loss: 0.0094\n",
      "Epoch 32/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0239 - val_loss: 0.0097\n",
      "Epoch 33/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0252 - val_loss: 0.0093\n",
      "Epoch 34/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0234 - val_loss: 0.0091\n",
      "Epoch 35/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0257 - val_loss: 0.0092\n",
      "Epoch 36/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0227 - val_loss: 0.0091\n",
      "Epoch 37/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0229 - val_loss: 0.0102\n",
      "Epoch 38/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0324 - val_loss: 0.0092\n",
      "Epoch 39/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0240 - val_loss: 0.0092\n",
      "Epoch 40/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0244 - val_loss: 0.0090\n",
      "Epoch 41/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0225 - val_loss: 0.0095\n",
      "Epoch 42/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0240 - val_loss: 0.0093\n",
      "Epoch 43/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0290 - val_loss: 0.0098\n",
      "Epoch 44/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0231 - val_loss: 0.0091\n",
      "Epoch 45/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0250 - val_loss: 0.0092\n",
      "Epoch 46/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0249 - val_loss: 0.0093\n",
      "Epoch 47/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0237 - val_loss: 0.0092\n",
      "Epoch 48/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0242 - val_loss: 0.0096\n",
      "Epoch 49/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0263 - val_loss: 0.0094\n",
      "Epoch 50/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0269 - val_loss: 0.0098\n",
      "Epoch 51/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0237 - val_loss: 0.0093\n",
      "Epoch 52/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0270 - val_loss: 0.0096\n",
      "Epoch 53/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0250 - val_loss: 0.0096\n",
      "Epoch 54/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0226 - val_loss: 0.0103\n",
      "Epoch 55/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0267 - val_loss: 0.0095\n",
      "Epoch 56/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0240 - val_loss: 0.0100\n",
      "Epoch 57/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0229 - val_loss: 0.0095\n",
      "Epoch 58/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0203 - val_loss: 0.0098\n",
      "Epoch 59/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0235 - val_loss: 0.0096\n",
      "Epoch 60/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0215 - val_loss: 0.0100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 360ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "RMSE: (121.04849491953208, 10.487434768676755)\n",
      "RMSE Random Walk: (47.22471086782877, 5.486020436507074)\n",
      "[999.07998657 985.83999634 940.12002563 919.99996948 941.82000732\n",
      " 936.05999756 908.59997559 902.72000122 912.88000488 902.19999695\n",
      " 946.99682617 969.45401001 956.60256958 959.02090454 949.06518555\n",
      " 948.27377319 953.65785217 944.88771057 923.20922852 954.03427124\n",
      " 941.126297   941.25863647 951.69189453 920.37329102 925.22547913\n",
      " 918.80323792 913.82739258 913.0243988  912.90089417 912.85719299]\n",
      "[122.51000214 120.31999969 120.86000061 118.12000275 115.06999969\n",
      " 114.76999664 114.69999695 117.69999695 111.30000305 110.33999634\n",
      " 117.11555481 116.40652466 116.83857727 116.49591827 117.72878265\n",
      " 116.93598938 116.86266327 117.72132111 116.90466309 117.2520752\n",
      " 116.89620209 117.56663513 116.71125793 116.84579468 115.68292999\n",
      " 114.8523407  114.09333038 113.30571747 113.01338196 112.44904327]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 116ms/step - loss: 0.3939 - val_loss: 0.0227\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2402 - val_loss: 0.0246\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0999 - val_loss: 0.0189\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0500 - val_loss: 0.0183\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0395 - val_loss: 0.0137\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0347 - val_loss: 0.0092\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0329 - val_loss: 0.0090\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0301 - val_loss: 0.0104\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0299 - val_loss: 0.0106\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0273 - val_loss: 0.0106\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0259 - val_loss: 0.0107\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0286 - val_loss: 0.0106\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0299 - val_loss: 0.0103\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0285 - val_loss: 0.0098\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0279 - val_loss: 0.0103\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0280 - val_loss: 0.0091\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0300 - val_loss: 0.0093\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0270 - val_loss: 0.0091\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0281 - val_loss: 0.0090\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0283 - val_loss: 0.0091\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0255 - val_loss: 0.0095\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0270 - val_loss: 0.0091\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0267 - val_loss: 0.0100\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0301 - val_loss: 0.0094\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0297 - val_loss: 0.0095\n",
      "Epoch 26/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0260 - val_loss: 0.0101\n",
      "Epoch 27/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0282 - val_loss: 0.0094\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 369ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "RMSE: (168.08443008458121, 12.411872863769528)\n",
      "RMSE Random Walk: (42.42409447630017, 5.2361421704931645)\n",
      "[1121.58998871 1106.15999603 1060.98002625 1038.11997223 1056.89000702\n",
      " 1050.8299942  1023.29997253 1020.41999817 1024.18000793 1012.53999329\n",
      " 1064.11238098 1085.86053467 1073.44114685 1075.51682281 1066.7939682\n",
      " 1065.20976257 1070.52051544 1062.60903168 1040.1138916  1071.28634644\n",
      " 1058.02249908 1058.82527161 1068.40315247 1037.21908569 1040.90840912\n",
      " 1033.65557861 1027.92072296 1026.33011627 1025.91427612 1025.30623627]\n",
      "[122.51000214 120.31999969 120.86000061 118.12000275 115.06999969\n",
      " 114.76999664 114.69999695 117.69999695 111.30000305 110.33999634\n",
      " 118.71471405 118.54238129 120.69555664 118.81071472 118.59772491\n",
      " 118.81988525 116.89136505 119.11799622 117.42119598 119.6561203\n",
      " 119.54586029 118.63741302 116.43701935 117.93876648 119.21529388\n",
      " 118.12568665 117.1210556  116.08470154 115.21322632 114.58078766]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 136ms/step - loss: 0.2607 - val_loss: 0.0048\n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1515 - val_loss: 0.0213\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0751 - val_loss: 0.0132\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0530 - val_loss: 0.0068\n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0330 - val_loss: 0.0038\n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0194 - val_loss: 0.0059\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0154 - val_loss: 0.0086\n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0151 - val_loss: 0.0105\n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0138 - val_loss: 0.0085\n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0145 - val_loss: 0.0065\n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0125 - val_loss: 0.0047\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0118 - val_loss: 0.0041\n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0112 - val_loss: 0.0039\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0109 - val_loss: 0.0041\n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0100 - val_loss: 0.0045\n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0103 - val_loss: 0.0049\n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0104 - val_loss: 0.0045\n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0113 - val_loss: 0.0046\n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0098 - val_loss: 0.0051\n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0102 - val_loss: 0.0051\n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0093 - val_loss: 0.0057\n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0095 - val_loss: 0.0053\n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0089 - val_loss: 0.0058\n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0099 - val_loss: 0.0056\n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0093 - val_loss: 0.0059\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 358ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "RMSE: (864.0719568580389, 24.509075927734376)\n",
      "RMSE Random Walk: (110.29141960820898, 7.994592256476127)\n",
      "[1244.09999084 1226.47999573 1181.84002686 1156.23997498 1171.96000671\n",
      " 1165.59999084 1137.99996948 1138.11999512 1135.48001099 1122.87998962\n",
      " 1182.82709503 1204.40291595 1194.13670349 1194.32753754 1185.39169312\n",
      " 1184.02964783 1187.41188049 1181.72702789 1157.53508759 1190.94246674\n",
      " 1177.56835938 1177.46268463 1184.84017181 1155.15785217 1160.123703\n",
      " 1151.78126526 1145.04177856 1142.41481781 1141.12750244 1139.88702393]\n",
      "[244.11000061 242.69999695 249.30000305 249.11000061 245.69000244\n",
      " 241.1499939  245.16999817 241.16000366 226.53999329 224.55000305\n",
      " 282.59835815 258.77044678 213.11260986 288.20870972 264.82531738\n",
      " 272.03063965 237.21672058 244.68540955 237.3024292  236.74395752\n",
      " 206.68630981 205.80172729 215.93635559 248.54762268 258.84475708\n",
      " 258.45751953 256.91052246 254.5165863  251.23608398 251.3634491 ]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 144ms/step - loss: 0.3099 - val_loss: 0.0025\n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1704 - val_loss: 0.0062\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0943 - val_loss: 0.0092\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0472 - val_loss: 0.0042\n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0349 - val_loss: 0.0028\n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0249 - val_loss: 0.0021\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0204 - val_loss: 0.0021\n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0154 - val_loss: 0.0029\n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0150 - val_loss: 0.0040\n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0130 - val_loss: 0.0050\n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0131 - val_loss: 0.0053\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0122 - val_loss: 0.0049\n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0123 - val_loss: 0.0043\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0125 - val_loss: 0.0038\n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0126 - val_loss: 0.0033\n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0110 - val_loss: 0.0031\n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0119 - val_loss: 0.0028\n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0124 - val_loss: 0.0030\n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0124 - val_loss: 0.0029\n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0120 - val_loss: 0.0030\n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0105 - val_loss: 0.0029\n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0116 - val_loss: 0.0032\n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0113 - val_loss: 0.0029\n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0131 - val_loss: 0.0030\n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0124 - val_loss: 0.0029\n",
      "Epoch 26/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0125 - val_loss: 0.0027\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 478ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "RMSE: (511.28382051141244, 19.014466094970704)\n",
      "RMSE Random Walk: (129.73549821446642, 8.673649317923449)\n",
      "[1488.20999146 1469.17999268 1431.14002991 1405.34997559 1417.65000916\n",
      " 1406.74998474 1383.16996765 1379.27999878 1362.02000427 1347.42999268\n",
      " 1465.42545319 1463.17336273 1407.24931335 1482.53624725 1450.2170105\n",
      " 1456.06028748 1424.62860107 1426.41243744 1394.83751678 1427.68642426\n",
      " 1384.25466919 1383.26441193 1400.7765274  1403.70547485 1418.96846008\n",
      " 1410.23878479 1401.95230103 1396.93140411 1392.36358643 1391.25047302]\n",
      "[244.11000061 242.69999695 249.30000305 249.11000061 245.69000244\n",
      " 241.1499939  245.16999817 241.16000366 226.53999329 224.55000305\n",
      " 267.58120728 262.03665161 249.61781311 255.42601013 255.58587646\n",
      " 251.39779663 274.2996521  239.28053284 235.29165649 251.82798767\n",
      " 264.93997192 228.93710327 257.37173462 250.4848175  246.34501648\n",
      " 245.91351318 244.9250946  243.39744568 241.48857117 239.71224976]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 139ms/step - loss: 0.4136 - val_loss: 0.0085\n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2397 - val_loss: 0.0274\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1179 - val_loss: 0.0455\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0753 - val_loss: 0.0209\n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0460 - val_loss: 0.0144\n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0349 - val_loss: 0.0062\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0297 - val_loss: 0.0050\n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0280 - val_loss: 0.0045\n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0293 - val_loss: 0.0042\n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0270 - val_loss: 0.0046\n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0280 - val_loss: 0.0054\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0250 - val_loss: 0.0066\n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0242 - val_loss: 0.0079\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0222 - val_loss: 0.0080\n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0236 - val_loss: 0.0082\n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0239 - val_loss: 0.0078\n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0213 - val_loss: 0.0074\n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0204 - val_loss: 0.0071\n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0213 - val_loss: 0.0081\n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0187 - val_loss: 0.0098\n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0227 - val_loss: 0.0089\n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0226 - val_loss: 0.0117\n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0222 - val_loss: 0.0089\n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0228 - val_loss: 0.0118\n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0211 - val_loss: 0.0104\n",
      "Epoch 26/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0195 - val_loss: 0.0120\n",
      "Epoch 27/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0185 - val_loss: 0.0109\n",
      "Epoch 28/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0219 - val_loss: 0.0118\n",
      "Epoch 29/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0220 - val_loss: 0.0123\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 372ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "RMSE: (100.42685669718776, 8.465130615234376)\n",
      "RMSE Random Walk: (217.16451283514547, 11.948245652079963)\n",
      "[1732.31999207 1711.87998962 1680.44003296 1654.4599762  1663.3400116\n",
      " 1647.89997864 1628.33996582 1620.44000244 1588.55999756 1571.97999573\n",
      " 1733.00666046 1725.21001434 1656.86712646 1737.96225739 1705.80288696\n",
      " 1707.45808411 1698.92825317 1665.69297028 1630.12917328 1679.51441193\n",
      " 1649.19464111 1612.2015152  1658.14826202 1654.19029236 1665.31347656\n",
      " 1656.15229797 1646.87739563 1640.32884979 1633.85215759 1630.96272278]\n",
      "[183.08999634 183.16999817 169.27000427 159.1499939  169.58000183\n",
      " 160.72000122 159.92999268 160.19000244 168.80000305 167.11000061\n",
      " 183.41966248 168.76516724 168.92684937 169.53309631 159.8815918\n",
      " 159.40713501 159.13989258 160.02632141 166.92012024 162.2931366\n",
      " 159.95495605 163.54081726 157.13217163 168.26000977 157.34031677\n",
      " 156.29130554 155.1065979  154.85691833 155.55464172 156.37191772]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 138ms/step - loss: 0.3959 - val_loss: 0.0099\n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.2868 - val_loss: 0.0084\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.1640 - val_loss: 0.0272\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1022 - val_loss: 0.0202\n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0642 - val_loss: 0.0150\n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0492 - val_loss: 0.0144\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0363 - val_loss: 0.0092\n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0261 - val_loss: 0.0064\n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0274 - val_loss: 0.0054\n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0254 - val_loss: 0.0051\n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0237 - val_loss: 0.0048\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0279 - val_loss: 0.0050\n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0250 - val_loss: 0.0054\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0244 - val_loss: 0.0057\n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0224 - val_loss: 0.0064\n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0223 - val_loss: 0.0066\n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0200 - val_loss: 0.0067\n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0202 - val_loss: 0.0074\n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0242 - val_loss: 0.0075\n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0209 - val_loss: 0.0076\n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0209 - val_loss: 0.0081\n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0204 - val_loss: 0.0082\n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0201 - val_loss: 0.0085\n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0195 - val_loss: 0.0084\n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0194 - val_loss: 0.0091\n",
      "Epoch 26/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0205 - val_loss: 0.0092\n",
      "Epoch 27/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0186 - val_loss: 0.0092\n",
      "Epoch 28/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0190 - val_loss: 0.0096\n",
      "Epoch 29/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0186 - val_loss: 0.0102\n",
      "Epoch 30/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0187 - val_loss: 0.0103\n",
      "Epoch 31/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0177 - val_loss: 0.0097\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 429ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "RMSE: (194.43483181582997, 11.630782318115234)\n",
      "RMSE Random Walk: (226.580977322886, 12.570358025569375)\n",
      "[1915.4099884  1895.04998779 1849.71003723 1813.60997009 1832.92001343\n",
      " 1808.61997986 1788.2699585  1780.63000488 1757.36000061 1739.08999634\n",
      " 1916.42632294 1893.97518158 1825.79397583 1907.4953537  1865.68447876\n",
      " 1866.86521912 1858.06814575 1825.71929169 1797.04929352 1841.80754852\n",
      " 1809.14959717 1775.74233246 1815.28043365 1822.45030212 1822.65379333\n",
      " 1812.44360352 1801.98399353 1795.18576813 1789.40679932 1787.3346405 ]\n",
      "[183.08999634 183.16999817 169.27000427 159.1499939  169.58000183\n",
      " 160.72000122 159.92999268 160.19000244 168.80000305 167.11000061\n",
      " 176.262146   179.62088013 174.24571228 174.58093262 165.34516907\n",
      " 172.81898499 173.33308411 162.06248474 164.60836792 165.36100769\n",
      " 169.58148193 173.78082275 164.67799377 166.25408936 166.5393219\n",
      " 165.07292175 163.815979   163.83790588 164.60081482 165.33413696]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 101ms/step - loss: 0.3631 - val_loss: 0.0626\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2193 - val_loss: 0.0505\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.1544 - val_loss: 0.0262\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0805 - val_loss: 0.0161\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0463 - val_loss: 0.0226\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0325 - val_loss: 0.0425\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0307 - val_loss: 0.0612\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0301 - val_loss: 0.0551\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0308 - val_loss: 0.0522\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0293 - val_loss: 0.0385\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0267 - val_loss: 0.0387\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0271 - val_loss: 0.0344\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0287 - val_loss: 0.0359\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0269 - val_loss: 0.0392\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0287 - val_loss: 0.0375\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0297 - val_loss: 0.0410\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0250 - val_loss: 0.0402\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0281 - val_loss: 0.0399\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0242 - val_loss: 0.0373\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0241 - val_loss: 0.0397\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0241 - val_loss: 0.0409\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0247 - val_loss: 0.0403\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0238 - val_loss: 0.0418\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0266 - val_loss: 0.0400\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 375ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "RMSE: (491.47791744872814, 19.680788421630858)\n",
      "RMSE Random Walk: (50.07958578310131, 5.567517488692352)\n",
      "[2098.49998474 2078.21998596 2018.9800415  1972.75996399 2002.50001526\n",
      " 1969.33998108 1948.19995117 1940.82000732 1926.16000366 1906.19999695\n",
      " 2092.68846893 2073.59606171 2000.03968811 2082.07628632 2031.02964783\n",
      " 2039.6842041  2031.40122986 1987.78177643 1961.65766144 2007.16855621\n",
      " 1978.7310791  1949.52315521 1979.95842743 1988.70439148 1989.19311523\n",
      " 1977.51652527 1965.79997253 1959.02367401 1954.00761414 1952.66877747]\n",
      "[142.30000305 138.22999573 133.22000122 133.61999512 133.80000305\n",
      " 137.27999878 130.75       129.78999329 128.72999573 126.76999664\n",
      " 146.43203735 133.44448853 144.04582214 148.29658508 141.51396179\n",
      " 145.35055542 140.22315979 148.97662354 137.86979675 129.07510376\n",
      " 139.76863098 143.64425659 137.20588684 141.28848267 150.99615479\n",
      " 149.30456543 148.35736084 147.97151184 147.41123962 146.92956543]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 130ms/step - loss: 0.3567 - val_loss: 0.0671\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2158 - val_loss: 0.0405\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.1145 - val_loss: 0.0306\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0597 - val_loss: 0.0200\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0394 - val_loss: 0.0249\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0355 - val_loss: 0.0293\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0314 - val_loss: 0.0348\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0312 - val_loss: 0.0388\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0270 - val_loss: 0.0411\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0291 - val_loss: 0.0379\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0269 - val_loss: 0.0386\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0285 - val_loss: 0.0369\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0264 - val_loss: 0.0381\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0285 - val_loss: 0.0346\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0294 - val_loss: 0.0353\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0273 - val_loss: 0.0384\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0273 - val_loss: 0.0358\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0273 - val_loss: 0.0362\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0266 - val_loss: 0.0384\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0240 - val_loss: 0.0376\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0245 - val_loss: 0.0374\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0261 - val_loss: 0.0399\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0289 - val_loss: 0.0351\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0269 - val_loss: 0.0398\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 331ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "RMSE: (320.2266829783155, 17.572241973876952)\n",
      "RMSE Random Walk: (49.72997428307201, 5.694370074202126)\n",
      "[2240.79998779 2216.44998169 2152.20004272 2106.37995911 2136.30001831\n",
      " 2106.61997986 2078.94995117 2070.61000061 2054.88999939 2032.96999359\n",
      " 2239.12050629 2207.04055023 2144.08551025 2230.3728714  2172.54360962\n",
      " 2185.03475952 2171.62438965 2136.75839996 2099.52745819 2136.24365997\n",
      " 2118.49971008 2093.1674118  2117.16431427 2129.99287415 2140.18927002\n",
      " 2126.8210907  2114.15733337 2106.99518585 2101.41885376 2099.5983429 ]\n",
      "[142.30000305 138.22999573 133.22000122 133.61999512 133.80000305\n",
      " 137.27999878 130.75       129.78999329 128.72999573 126.76999664\n",
      " 142.20339966 149.79890442 146.18548584 149.4319458  146.38458252\n",
      " 148.15913391 145.11450195 144.74806213 141.80352783 148.11759949\n",
      " 139.35150146 148.04071045 139.82740784 137.30915833 135.24247742\n",
      " 134.23162842 133.39163208 132.83872986 132.1940155  131.56045532]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 126ms/step - loss: 0.3651 - val_loss: 0.1989\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2249 - val_loss: 0.0959\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1083 - val_loss: 0.0365\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0562 - val_loss: 0.0224\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0513 - val_loss: 0.0204\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0409 - val_loss: 0.0331\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0378 - val_loss: 0.0438\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0338 - val_loss: 0.0469\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0342 - val_loss: 0.0508\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0335 - val_loss: 0.0508\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0329 - val_loss: 0.0558\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0331 - val_loss: 0.0455\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0308 - val_loss: 0.0540\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0305 - val_loss: 0.0514\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0312 - val_loss: 0.0612\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0305 - val_loss: 0.0535\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0306 - val_loss: 0.0582\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0324 - val_loss: 0.0759\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0333 - val_loss: 0.0467\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0291 - val_loss: 0.0749\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0268 - val_loss: 0.0510\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0320 - val_loss: 0.0699\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0294 - val_loss: 0.0675\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0277 - val_loss: 0.0675\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0272 - val_loss: 0.0723\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 372ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "RMSE: (132.37338364378084, 10.971290588378906)\n",
      "RMSE Random Walk: (29.49648896675057, 4.400086876460743)\n",
      "[2383.09999084 2354.67997742 2285.42004395 2239.99995422 2270.10002136\n",
      " 2243.89997864 2209.69995117 2200.3999939  2183.61999512 2159.73999023\n",
      " 2381.32390594 2356.83945465 2290.27099609 2379.8048172  2318.92819214\n",
      " 2333.19389343 2316.7388916  2281.5064621  2241.33098602 2284.36125946\n",
      " 2257.85121155 2241.20812225 2256.99172211 2267.30203247 2275.43174744\n",
      " 2261.05271912 2247.54896545 2239.83391571 2233.61286926 2231.15879822]\n",
      "[174.1499939  171.52000427 167.57000732 167.22999573 167.52999878\n",
      " 170.02999878 163.61999512 161.38000488 158.91000366 157.22000122\n",
      " 165.36834717 162.2441864  168.90332031 163.39758301 160.949646\n",
      " 171.80784607 169.94299316 166.55445862 161.55982971 167.5027771\n",
      " 163.15475464 167.89100647 166.06091309 169.21873474 162.60054016\n",
      " 162.18104553 161.76348877 161.41183472 161.07307434 160.69940186]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 320ms/step - loss: 0.3454 - val_loss: 0.1721\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1647 - val_loss: 0.0583\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0863 - val_loss: 0.0414\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0587 - val_loss: 0.0345\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0450 - val_loss: 0.0258\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0397 - val_loss: 0.0375\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0383 - val_loss: 0.0380\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0357 - val_loss: 0.0384\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0332 - val_loss: 0.0490\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0360 - val_loss: 0.0474\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0352 - val_loss: 0.0476\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0336 - val_loss: 0.0508\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0351 - val_loss: 0.0428\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0339 - val_loss: 0.0475\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0343 - val_loss: 0.0383\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0345 - val_loss: 0.0466\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0351 - val_loss: 0.0388\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0329 - val_loss: 0.0460\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0348 - val_loss: 0.0375\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0362 - val_loss: 0.0430\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0338 - val_loss: 0.0392\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0320 - val_loss: 0.0441\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0342 - val_loss: 0.0406\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0352 - val_loss: 0.0425\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0339 - val_loss: 0.0394\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 354ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "RMSE: (197.7097807908198, 12.708633422851562)\n",
      "RMSE Random Walk: (29.524214350774137, 4.312740463218235)\n",
      "[2557.24998474 2526.19998169 2452.99005127 2407.22994995 2437.63002014\n",
      " 2413.92997742 2373.31994629 2361.77999878 2342.52999878 2316.95999146\n",
      " 2546.69225311 2519.08364105 2459.17431641 2543.20240021 2479.87783813\n",
      " 2505.0017395  2486.68188477 2448.06092072 2402.89081573 2451.86403656\n",
      " 2421.00596619 2409.09912872 2423.05263519 2436.52076721 2438.0322876\n",
      " 2423.23376465 2409.31245422 2401.24575043 2394.6859436  2391.85820007]\n",
      "[174.1499939  171.52000427 167.57000732 167.22999573 167.52999878\n",
      " 170.02999878 163.61999512 161.38000488 158.91000366 157.22000122\n",
      " 169.96839905 163.95324707 157.83154297 169.77270508 168.1893158\n",
      " 169.02320862 163.50468445 163.74412537 164.09008789 164.46173096\n",
      " 159.7729187  166.55764771 163.24723816 166.16467285 171.27444458\n",
      " 170.64715576 169.98219299 169.46868896 168.98912048 168.38951111]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 107ms/step - loss: 0.4024 - val_loss: 0.0249\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2748 - val_loss: 0.0418\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1435 - val_loss: 0.0401\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0679 - val_loss: 0.0346\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0519 - val_loss: 0.0183\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0342 - val_loss: 0.0107\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0323 - val_loss: 0.0121\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0274 - val_loss: 0.0161\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0299 - val_loss: 0.0121\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0287 - val_loss: 0.0107\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0287 - val_loss: 0.0099\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0289 - val_loss: 0.0091\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0279 - val_loss: 0.0094\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0309 - val_loss: 0.0091\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0280 - val_loss: 0.0093\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0282 - val_loss: 0.0090\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0275 - val_loss: 0.0094\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0267 - val_loss: 0.0092\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0289 - val_loss: 0.0103\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0281 - val_loss: 0.0094\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0249 - val_loss: 0.0101\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0271 - val_loss: 0.0093\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0254 - val_loss: 0.0096\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0264 - val_loss: 0.0093\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0252 - val_loss: 0.0095\n",
      "Epoch 26/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0239 - val_loss: 0.0099\n",
      "Epoch 27/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0255 - val_loss: 0.0097\n",
      "Epoch 28/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0242 - val_loss: 0.0092\n",
      "Epoch 29/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0239 - val_loss: 0.0095\n",
      "Epoch 30/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0262 - val_loss: 0.0091\n",
      "Epoch 31/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0261 - val_loss: 0.0092\n",
      "Epoch 32/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0236 - val_loss: 0.0092\n",
      "Epoch 33/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0248 - val_loss: 0.0093\n",
      "Epoch 34/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0249 - val_loss: 0.0092\n",
      "Epoch 35/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0241 - val_loss: 0.0096\n",
      "Epoch 36/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0241 - val_loss: 0.0097\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 345ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "RMSE: (144.1546256863919, 11.552733612060544)\n",
      "RMSE Random Walk: (43.53900340919542, 5.062847478090462)\n",
      "[2731.39997864 2697.71998596 2620.56005859 2574.45994568 2605.16001892\n",
      " 2583.9599762  2536.93994141 2523.16000366 2501.44000244 2474.17999268\n",
      " 2716.66065216 2683.03688812 2617.00585938 2712.97510529 2648.06715393\n",
      " 2674.02494812 2650.18656921 2611.80504608 2566.98090363 2616.32576752\n",
      " 2580.77888489 2575.65677643 2586.29987335 2602.68544006 2609.30673218\n",
      " 2593.88092041 2579.29464722 2570.71443939 2563.67506409 2560.24771118]\n",
      "[122.51000214 120.31999969 120.86000061 118.12000275 115.06999969\n",
      " 114.76999664 114.69999695 117.69999695 111.30000305 110.33999634\n",
      " 118.05217743 118.4542923  117.59150696 119.31267548 118.78527069\n",
      " 117.61426544 118.23640442 117.68724823 118.29005432 117.69080353\n",
      " 118.23791504 118.06829834 117.53762054 118.90265656 116.24098969\n",
      " 115.26490784 114.85036469 114.06231689 114.2401886  113.86472321]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 108ms/step - loss: 0.3939 - val_loss: 0.0245\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2441 - val_loss: 0.0284\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1099 - val_loss: 0.0236\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0718 - val_loss: 0.0149\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0561 - val_loss: 0.0152\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0418 - val_loss: 0.0106\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0318 - val_loss: 0.0094\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0300 - val_loss: 0.0093\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0340 - val_loss: 0.0096\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0293 - val_loss: 0.0095\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0302 - val_loss: 0.0117\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0302 - val_loss: 0.0103\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0280 - val_loss: 0.0107\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0279 - val_loss: 0.0102\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0293 - val_loss: 0.0103\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0265 - val_loss: 0.0103\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0286 - val_loss: 0.0103\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0283 - val_loss: 0.0104\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0268 - val_loss: 0.0110\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0271 - val_loss: 0.0102\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0282 - val_loss: 0.0101\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0268 - val_loss: 0.0101\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0276 - val_loss: 0.0100\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0278 - val_loss: 0.0099\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0275 - val_loss: 0.0097\n",
      "Epoch 26/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0262 - val_loss: 0.0102\n",
      "Epoch 27/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0267 - val_loss: 0.0096\n",
      "Epoch 28/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0295 - val_loss: 0.0099\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 355ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "RMSE: (139.8351280359871, 11.274168014526364)\n",
      "RMSE Random Walk: (42.319926848600836, 5.169663081834667)\n",
      "[2853.90998077 2818.03998566 2741.4200592  2692.57994843 2720.23001862\n",
      " 2698.72997284 2651.63993835 2640.86000061 2612.74000549 2584.51998901\n",
      " 2834.71282959 2801.49118042 2734.59736633 2832.28778076 2766.85242462\n",
      " 2791.63921356 2768.42297363 2729.49229431 2685.27095795 2734.01657104\n",
      " 2699.01679993 2693.72507477 2703.8374939  2721.58809662 2725.54772186\n",
      " 2709.14582825 2694.1450119  2684.77675629 2677.91525269 2674.11243439]\n",
      "[122.51000214 120.31999969 120.86000061 118.12000275 115.06999969\n",
      " 114.76999664 114.69999695 117.69999695 111.30000305 110.33999634\n",
      " 118.32714081 117.80577087 118.94126129 119.03337097 117.40010071\n",
      " 117.57945251 117.08815002 115.82958221 117.29406738 119.18371582\n",
      " 118.48728943 116.74578094 118.13542938 115.44745636 117.20332336\n",
      " 116.2477951  115.38560486 114.43656921 113.6785202  113.16298676]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 149ms/step - loss: 0.2659 - val_loss: 0.0047\n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.1339 - val_loss: 0.0330\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0774 - val_loss: 0.0184\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0476 - val_loss: 0.0069\n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0323 - val_loss: 0.0033\n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0163 - val_loss: 0.0053\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0171 - val_loss: 0.0094\n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0152 - val_loss: 0.0111\n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0154 - val_loss: 0.0091\n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0124 - val_loss: 0.0064\n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0119 - val_loss: 0.0049\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0115 - val_loss: 0.0039\n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0111 - val_loss: 0.0040\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0108 - val_loss: 0.0042\n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0122 - val_loss: 0.0046\n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0100 - val_loss: 0.0052\n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0130 - val_loss: 0.0052\n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0119 - val_loss: 0.0050\n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0138 - val_loss: 0.0046\n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0118 - val_loss: 0.0046\n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0103 - val_loss: 0.0049\n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0105 - val_loss: 0.0053\n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0118 - val_loss: 0.0061\n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0111 - val_loss: 0.0060\n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0100 - val_loss: 0.0061\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 351ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "RMSE: (744.5887321362973, 23.069071960449218)\n",
      "RMSE Random Walk: (111.60961295764821, 8.457127789374615)\n",
      "[2976.41998291 2938.35998535 2862.28005981 2810.69995117 2835.30001831\n",
      " 2813.49996948 2766.3399353  2758.55999756 2724.04000854 2694.85998535\n",
      " 2953.0399704  2919.29695129 2853.53862762 2951.32115173 2884.25252533\n",
      " 2909.21866608 2885.51112366 2845.32187653 2802.56502533 2853.20028687\n",
      " 2817.50408936 2810.47085571 2821.97292328 2837.03555298 2842.75104523\n",
      " 2825.39362335 2809.53061676 2799.2133255  2791.59377289 2787.27542114]\n",
      "[244.11000061 242.69999695 249.30000305 249.11000061 245.69000244\n",
      " 241.1499939  245.16999817 241.16000366 226.53999329 224.55000305\n",
      " 265.36550903 278.71725464 259.58432007 256.9772644  274.09274292\n",
      " 264.20019531 242.83880615 263.41503906 266.70657349 251.79814148\n",
      " 227.90071106 208.72920227 226.10983276 264.50949097 237.45800781\n",
      " 237.6257782  236.32359314 234.32383728 231.92073059 230.61463928]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 128ms/step - loss: 0.2881 - val_loss: 0.0029\n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1861 - val_loss: 0.0054\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1014 - val_loss: 0.0110\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0714 - val_loss: 0.0062\n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0465 - val_loss: 0.0036\n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0339 - val_loss: 0.0025\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0234 - val_loss: 0.0023\n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0164 - val_loss: 0.0031\n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0138 - val_loss: 0.0046\n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0132 - val_loss: 0.0054\n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0125 - val_loss: 0.0056\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0130 - val_loss: 0.0050\n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0127 - val_loss: 0.0044\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0114 - val_loss: 0.0038\n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0128 - val_loss: 0.0033\n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0116 - val_loss: 0.0031\n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0122 - val_loss: 0.0030\n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0117 - val_loss: 0.0030\n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0119 - val_loss: 0.0030\n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0115 - val_loss: 0.0031\n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0111 - val_loss: 0.0032\n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0109 - val_loss: 0.0033\n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0117 - val_loss: 0.0032\n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0111 - val_loss: 0.0032\n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0105 - val_loss: 0.0031\n",
      "Epoch 26/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0111 - val_loss: 0.0030\n",
      "Epoch 27/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0123 - val_loss: 0.0030\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 329ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "RMSE: (285.9536618309094, 13.33879699707031)\n",
      "RMSE Random Walk: (126.86428666081208, 8.980814530537106)\n",
      "[3220.52998352 3181.0599823  3111.58006287 3059.80995178 3080.99002075\n",
      " 3054.64996338 3011.50993347 2999.72000122 2950.58000183 2919.4099884\n",
      " 3218.40547943 3198.01420593 3113.12294769 3208.29841614 3158.34526825\n",
      " 3173.41886139 3128.34992981 3108.73691559 3069.27159882 3104.99842834\n",
      " 3045.40480042 3019.20005798 3048.08275604 3101.54504395 3080.20905304\n",
      " 3063.01940155 3045.8542099  3033.53716278 3023.51450348 3017.89006042]\n",
      "[244.11000061 242.69999695 249.30000305 249.11000061 245.69000244\n",
      " 241.1499939  245.16999817 241.16000366 226.53999329 224.55000305\n",
      " 252.48591614 236.31590271 230.20729065 238.0581665  229.83433533\n",
      " 227.67955017 235.46336365 233.62649536 241.94541931 254.25045776\n",
      " 230.39382935 218.41986084 200.85115051 216.73254395 225.98658752\n",
      " 225.4921875  224.36538696 222.79576111 220.88612366 219.10844421]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 127ms/step - loss: 0.4018 - val_loss: 0.0073\n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2285 - val_loss: 0.0337\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0965 - val_loss: 0.0600\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0633 - val_loss: 0.0219\n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0519 - val_loss: 0.0106\n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0342 - val_loss: 0.0060\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0354 - val_loss: 0.0039\n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0272 - val_loss: 0.0041\n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0285 - val_loss: 0.0039\n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0254 - val_loss: 0.0041\n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0255 - val_loss: 0.0045\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0268 - val_loss: 0.0066\n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0244 - val_loss: 0.0065\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0230 - val_loss: 0.0069\n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0210 - val_loss: 0.0071\n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0242 - val_loss: 0.0069\n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0213 - val_loss: 0.0070\n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0231 - val_loss: 0.0071\n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0204 - val_loss: 0.0073\n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0220 - val_loss: 0.0081\n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0197 - val_loss: 0.0085\n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0228 - val_loss: 0.0092\n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0235 - val_loss: 0.0093\n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0210 - val_loss: 0.0109\n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0205 - val_loss: 0.0108\n",
      "Epoch 26/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0185 - val_loss: 0.0101\n",
      "Epoch 27/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0197 - val_loss: 0.0106\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 337ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "RMSE: (132.68203320987521, 9.366136169433593)\n",
      "RMSE Random Walk: (248.15714383844275, 12.51614369886343)\n",
      "[3464.63998413 3423.75997925 3360.88006592 3308.91995239 3326.68002319\n",
      " 3295.79995728 3256.67993164 3240.88000488 3177.11999512 3143.95999146\n",
      " 3470.89139557 3434.33010864 3343.33023834 3446.35658264 3388.17960358\n",
      " 3401.09841156 3363.81329346 3342.36341095 3311.21701813 3359.24888611\n",
      " 3275.79862976 3237.61991882 3248.93390656 3318.27758789 3306.19564056\n",
      " 3288.51158905 3270.21959686 3256.33292389 3244.40062714 3236.99850464]\n",
      "[183.08999634 183.16999817 169.27000427 159.1499939  169.58000183\n",
      " 160.72000122 159.92999268 160.19000244 168.80000305 167.11000061\n",
      " 170.56436157 180.73068237 162.29830933 165.34877014 161.60449219\n",
      " 160.46624756 158.1965332  151.0872345  160.40077209 155.38148499\n",
      " 168.0464325  153.17193604 151.89720154 153.54388428 161.92268372\n",
      " 160.72180176 158.92105103 158.76229858 159.49555969 160.26254272]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 132ms/step - loss: 0.3927 - val_loss: 0.0099\n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.2933 - val_loss: 0.0102\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1549 - val_loss: 0.0268\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0858 - val_loss: 0.0191\n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0520 - val_loss: 0.0179\n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0406 - val_loss: 0.0113\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0354 - val_loss: 0.0073\n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0277 - val_loss: 0.0052\n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0270 - val_loss: 0.0045\n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0250 - val_loss: 0.0047\n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0265 - val_loss: 0.0047\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0245 - val_loss: 0.0050\n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0248 - val_loss: 0.0055\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0246 - val_loss: 0.0062\n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0217 - val_loss: 0.0066\n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0238 - val_loss: 0.0069\n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0219 - val_loss: 0.0076\n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0214 - val_loss: 0.0078\n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0206 - val_loss: 0.0080\n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0209 - val_loss: 0.0084\n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0198 - val_loss: 0.0080\n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0218 - val_loss: 0.0085\n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0207 - val_loss: 0.0083\n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0190 - val_loss: 0.0081\n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0213 - val_loss: 0.0108\n",
      "Epoch 26/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0248 - val_loss: 0.0084\n",
      "Epoch 27/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0229 - val_loss: 0.0122\n",
      "Epoch 28/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0220 - val_loss: 0.0094\n",
      "Epoch 29/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0246 - val_loss: 0.0128\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 339ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "RMSE: (260.00593280366155, 12.559359741210937)\n",
      "RMSE Random Walk: (202.70064278360638, 11.513442993240854)\n",
      "[3647.72998047 3606.92997742 3530.15007019 3468.06994629 3496.26002502\n",
      " 3456.5199585  3416.60992432 3401.07000732 3345.91999817 3311.06999207\n",
      " 3641.45575714 3615.06079102 3505.62854767 3611.70535278 3549.78409576\n",
      " 3561.56465912 3522.00982666 3493.45064545 3471.61779022 3514.63037109\n",
      " 3443.84506226 3390.79185486 3400.83110809 3471.82147217 3468.11832428\n",
      " 3449.23339081 3429.14064789 3415.09522247 3403.89618683 3397.26104736]\n",
      "[183.08999634 183.16999817 169.27000427 159.1499939  169.58000183\n",
      " 160.72000122 159.92999268 160.19000244 168.80000305 167.11000061\n",
      " 177.33139038 180.48666382 172.63171387 163.45748901 169.12414551\n",
      " 169.17298889 163.74034119 167.12828064 156.44502258 160.03659058\n",
      " 167.59197998 168.41235352 168.54759216 167.25115967 170.49697876\n",
      " 169.56315613 168.86265564 168.99951172 169.57499695 170.10037231]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 109ms/step - loss: 0.3941 - val_loss: 0.0627\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2196 - val_loss: 0.0326\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0980 - val_loss: 0.0216\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0642 - val_loss: 0.0224\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0452 - val_loss: 0.0246\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0335 - val_loss: 0.0372\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0295 - val_loss: 0.0408\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0307 - val_loss: 0.0499\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0296 - val_loss: 0.0451\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0309 - val_loss: 0.0447\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0253 - val_loss: 0.0388\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0303 - val_loss: 0.0382\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0270 - val_loss: 0.0376\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0290 - val_loss: 0.0373\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0263 - val_loss: 0.0389\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0251 - val_loss: 0.0378\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0271 - val_loss: 0.0391\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0302 - val_loss: 0.0385\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0287 - val_loss: 0.0381\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0254 - val_loss: 0.0357\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0253 - val_loss: 0.0369\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0273 - val_loss: 0.0391\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0264 - val_loss: 0.0359\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 383ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "RMSE: (266.2211297424248, 14.48591651916504)\n",
      "RMSE Random Walk: (49.44405201215849, 5.763152097442081)\n",
      "[3830.81997681 3790.09997559 3699.42007446 3627.21994019 3665.84002686\n",
      " 3617.23995972 3576.53991699 3561.26000977 3514.72000122 3478.17999268\n",
      " 3818.78714752 3795.54745483 3678.26026154 3775.1628418  3718.90824127\n",
      " 3730.73764801 3685.75016785 3660.57892609 3628.06281281 3674.66696167\n",
      " 3611.43704224 3559.20420837 3569.37870026 3639.07263184 3638.61530304\n",
      " 3618.79654694 3598.00330353 3584.09473419 3573.47118378 3567.36141968]\n",
      "[142.30000305 138.22999573 133.22000122 133.61999512 133.80000305\n",
      " 137.27999878 130.75       129.78999329 128.72999573 126.76999664\n",
      " 145.96281433 141.40701294 142.88197327 122.48347473 111.27671814\n",
      " 160.47807312 138.05192566 138.96949768 147.95465088 141.78128052\n",
      " 150.94506836 146.54914856 150.31712341 132.76951599 124.47356415\n",
      " 123.97679901 123.50350189 123.35005188 123.10818481 122.88835144]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 106ms/step - loss: 0.3409 - val_loss: 0.0557\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1661 - val_loss: 0.0204\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0881 - val_loss: 0.0235\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0597 - val_loss: 0.0237\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0387 - val_loss: 0.0201\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0390 - val_loss: 0.0309\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0312 - val_loss: 0.0303\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0326 - val_loss: 0.0368\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0279 - val_loss: 0.0379\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0279 - val_loss: 0.0414\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0294 - val_loss: 0.0416\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0275 - val_loss: 0.0433\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0276 - val_loss: 0.0414\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0252 - val_loss: 0.0426\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0264 - val_loss: 0.0394\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0279 - val_loss: 0.0411\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0267 - val_loss: 0.0380\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0266 - val_loss: 0.0423\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0294 - val_loss: 0.0354\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0318 - val_loss: 0.0400\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0253 - val_loss: 0.0374\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0253 - val_loss: 0.0404\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0253 - val_loss: 0.0381\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0262 - val_loss: 0.0415\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0284 - val_loss: 0.0376\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "RMSE: (409.41777304930724, 18.578199005126955)\n",
      "RMSE Random Walk: (53.990014271696296, 5.868187573204613)\n",
      "[3973.11997986 3928.32997131 3832.64007568 3760.8399353  3799.64002991\n",
      " 3754.5199585  3707.28991699 3691.05000305 3643.44999695 3604.94998932\n",
      " 3964.74996185 3936.95446777 3821.1422348  3897.64631653 3830.18495941\n",
      " 3891.21572113 3823.80209351 3799.54842377 3776.01746368 3816.44824219\n",
      " 3762.3821106  3705.75335693 3719.69582367 3771.84214783 3763.08886719\n",
      " 3742.77334595 3721.50680542 3707.44478607 3696.57936859 3690.24977112]\n",
      "[142.30000305 138.22999573 133.22000122 133.61999512 133.80000305\n",
      " 137.27999878 130.75       129.78999329 128.72999573 126.76999664\n",
      " 147.35858154 146.18457031 136.34208679 142.6988678  135.14982605\n",
      " 143.12051392 138.95355225 141.64241028 141.78283691 140.50857544\n",
      " 146.23373413 141.27697754 141.43968201 141.56002808 146.96961975\n",
      " 145.20605469 143.71548462 142.87915039 141.99534607 141.03610229]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 115ms/step - loss: 0.3860 - val_loss: 0.2014\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2268 - val_loss: 0.0986\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1153 - val_loss: 0.0436\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0677 - val_loss: 0.0338\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0523 - val_loss: 0.0266\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0401 - val_loss: 0.0293\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0352 - val_loss: 0.0434\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0363 - val_loss: 0.0486\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0350 - val_loss: 0.0481\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0325 - val_loss: 0.0521\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0352 - val_loss: 0.0465\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0336 - val_loss: 0.0469\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0297 - val_loss: 0.0473\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0330 - val_loss: 0.0448\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0308 - val_loss: 0.0486\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0334 - val_loss: 0.0492\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0305 - val_loss: 0.0534\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0288 - val_loss: 0.0554\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0317 - val_loss: 0.0529\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0311 - val_loss: 0.0526\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0296 - val_loss: 0.0622\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0280 - val_loss: 0.0597\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0283 - val_loss: 0.0662\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0281 - val_loss: 0.0593\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0261 - val_loss: 0.0693\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 363ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "RMSE: (77.9341412040405, 7.985932922363281)\n",
      "RMSE Random Walk: (29.127321746808157, 4.140974657511952)\n",
      "[4115.41998291 4066.55996704 3965.8600769  3894.45993042 3933.44003296\n",
      " 3891.79995728 3838.03991699 3820.83999634 3772.17999268 3731.71998596\n",
      " 4112.1085434  4083.13903809 3957.48432159 4040.34518433 3965.33478546\n",
      " 4034.33623505 3962.75564575 3941.19083405 3917.8003006  3956.95681763\n",
      " 3908.61584473 3847.03033447 3861.13550568 3913.4021759  3910.05848694\n",
      " 3887.97940063 3865.22229004 3850.32393646 3838.57471466 3831.28587341]\n",
      "[174.1499939  171.52000427 167.57000732 167.22999573 167.52999878\n",
      " 170.02999878 163.61999512 161.38000488 158.91000366 157.22000122\n",
      " 164.62205505 159.30221558 161.87928772 166.41166687 166.14479065\n",
      " 165.24853516 165.45455933 159.48358154 160.76930237 163.53205872\n",
      " 163.70993042 168.4944458  170.70277405 164.70036316 156.25854492\n",
      " 156.0093689  155.75582886 155.59573364 155.41368103 155.08990479]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 144ms/step - loss: 0.3584 - val_loss: 0.1995\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.2322 - val_loss: 0.0811\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.1098 - val_loss: 0.0387\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0698 - val_loss: 0.0477\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0551 - val_loss: 0.0251\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0403 - val_loss: 0.0345\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0384 - val_loss: 0.0401\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0379 - val_loss: 0.0316\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0366 - val_loss: 0.0433\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0373 - val_loss: 0.0405\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0336 - val_loss: 0.0431\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0342 - val_loss: 0.0397\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0340 - val_loss: 0.0447\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0360 - val_loss: 0.0368\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0326 - val_loss: 0.0458\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0349 - val_loss: 0.0360\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0336 - val_loss: 0.0394\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0387 - val_loss: 0.0443\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0329 - val_loss: 0.0318\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0372 - val_loss: 0.0451\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0350 - val_loss: 0.0346\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0331 - val_loss: 0.0420\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0340 - val_loss: 0.0375\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0345 - val_loss: 0.0418\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0343 - val_loss: 0.0352\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 360ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "RMSE: (181.57652165450855, 12.664134979248047)\n",
      "RMSE Random Walk: (31.194287330129516, 4.273906642443384)\n",
      "[4289.56997681 4238.07997131 4133.43008423 4061.68992615 4100.97003174\n",
      " 4061.82995605 4001.65991211 3982.22000122 3931.08999634 3888.93998718\n",
      " 4276.73059845 4242.44125366 4119.36360931 4206.7568512  4131.47957611\n",
      " 4199.5847702  4128.21020508 4100.67441559 4078.56960297 4120.48887634\n",
      " 4072.32577515 4015.52478027 4031.83827972 4078.10253906 4066.31703186\n",
      " 4043.98876953 4020.9781189  4005.9196701  3993.98839569 3986.3757782 ]\n",
      "[174.1499939  171.52000427 167.57000732 167.22999573 167.52999878\n",
      " 170.02999878 163.61999512 161.38000488 158.91000366 157.22000122\n",
      " 169.42402649 170.03739929 160.65943909 169.93899536 167.69599915\n",
      " 158.06202698 159.42858887 169.79515076 167.36491394 170.92399597\n",
      " 169.08155823 171.99310303 167.93475342 165.33404541 165.52278137\n",
      " 164.7653656  163.99391174 163.41676331 162.77851868 161.98852539]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 125ms/step - loss: 0.3905 - val_loss: 0.0230\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2370 - val_loss: 0.0332\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1431 - val_loss: 0.0211\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0775 - val_loss: 0.0162\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0460 - val_loss: 0.0104\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0367 - val_loss: 0.0123\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0288 - val_loss: 0.0176\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0284 - val_loss: 0.0202\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0317 - val_loss: 0.0158\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0286 - val_loss: 0.0125\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0254 - val_loss: 0.0114\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0307 - val_loss: 0.0096\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0338 - val_loss: 0.0094\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0267 - val_loss: 0.0099\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0286 - val_loss: 0.0097\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0284 - val_loss: 0.0104\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0268 - val_loss: 0.0095\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0264 - val_loss: 0.0101\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0278 - val_loss: 0.0098\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0262 - val_loss: 0.0104\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0237 - val_loss: 0.0100\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0283 - val_loss: 0.0106\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0265 - val_loss: 0.0106\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0233 - val_loss: 0.0094\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0272 - val_loss: 0.0098\n",
      "Epoch 26/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0248 - val_loss: 0.0091\n",
      "Epoch 27/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0270 - val_loss: 0.0097\n",
      "Epoch 28/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0253 - val_loss: 0.0100\n",
      "Epoch 29/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0303 - val_loss: 0.0097\n",
      "Epoch 30/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0272 - val_loss: 0.0100\n",
      "Epoch 31/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0246 - val_loss: 0.0098\n",
      "Epoch 32/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0264 - val_loss: 0.0096\n",
      "Epoch 33/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0249 - val_loss: 0.0101\n",
      "Epoch 34/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0275 - val_loss: 0.0099\n",
      "Epoch 35/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0257 - val_loss: 0.0101\n",
      "Epoch 36/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0251 - val_loss: 0.0105\n",
      "Epoch 37/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0253 - val_loss: 0.0099\n",
      "Epoch 38/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0252 - val_loss: 0.0099\n",
      "Epoch 39/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0249 - val_loss: 0.0092\n",
      "Epoch 40/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0259 - val_loss: 0.0102\n",
      "Epoch 41/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0257 - val_loss: 0.0095\n",
      "Epoch 42/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0249 - val_loss: 0.0101\n",
      "Epoch 43/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0247 - val_loss: 0.0096\n",
      "Epoch 44/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0223 - val_loss: 0.0092\n",
      "Epoch 45/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0234 - val_loss: 0.0091\n",
      "Epoch 46/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0242 - val_loss: 0.0096\n",
      "Epoch 47/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0221 - val_loss: 0.0110\n",
      "Epoch 48/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0256 - val_loss: 0.0096\n",
      "Epoch 49/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0289 - val_loss: 0.0097\n",
      "Epoch 50/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0248 - val_loss: 0.0094\n",
      "Epoch 51/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0224 - val_loss: 0.0104\n",
      "Epoch 52/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0248 - val_loss: 0.0100\n",
      "Epoch 53/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0243 - val_loss: 0.0112\n",
      "Epoch 54/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0257 - val_loss: 0.0101\n",
      "Epoch 55/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0231 - val_loss: 0.0107\n",
      "Epoch 56/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0232 - val_loss: 0.0099\n",
      "Epoch 57/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0213 - val_loss: 0.0104\n",
      "Epoch 58/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0241 - val_loss: 0.0102\n",
      "Epoch 59/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0237 - val_loss: 0.0097\n",
      "Epoch 60/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0226 - val_loss: 0.0099\n",
      "Epoch 61/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0218 - val_loss: 0.0094\n",
      "Epoch 62/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0216 - val_loss: 0.0098\n",
      "Epoch 63/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0243 - val_loss: 0.0099\n",
      "Epoch 64/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0257 - val_loss: 0.0097\n",
      "Epoch 65/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0242 - val_loss: 0.0103\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 365ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "RMSE: (94.5572934418538, 9.033201217651364)\n",
      "RMSE Random Walk: (42.75902702332229, 5.14830873843655)\n",
      "[4463.7199707  4409.59997559 4301.00009155 4228.91992188 4268.50003052\n",
      " 4231.85995483 4165.27990723 4143.6000061  4090.         4046.1599884\n",
      " 4446.15462494 4412.47865295 4280.0230484  4376.69584656 4299.17557526\n",
      " 4357.64679718 4287.63879395 4270.46956635 4245.93451691 4291.41287231\n",
      " 4241.40733337 4187.5178833  4199.77303314 4243.43658447 4231.83981323\n",
      " 4208.75413513 4184.97203064 4169.33643341 4156.76691437 4148.36430359]\n",
      "[122.51000214 120.31999969 120.86000061 118.12000275 115.06999969\n",
      " 114.76999664 114.69999695 117.69999695 111.30000305 110.33999634\n",
      " 114.33868408 114.65822601 115.27789307 116.08418274 115.74105072\n",
      " 114.96928406 115.42666626 115.39542389 115.66299438 115.9316864\n",
      " 115.38191223 115.78388214 115.5140152  114.94366455 114.78346252\n",
      " 113.82746887 112.99123383 112.12498474 112.13553619 111.6217804 ]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 126ms/step - loss: 0.3757 - val_loss: 0.0206\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.1965 - val_loss: 0.0366\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0944 - val_loss: 0.0262\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0491 - val_loss: 0.0193\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0483 - val_loss: 0.0157\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0380 - val_loss: 0.0098\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0341 - val_loss: 0.0089\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0315 - val_loss: 0.0101\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0287 - val_loss: 0.0108\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0297 - val_loss: 0.0125\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0300 - val_loss: 0.0113\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0288 - val_loss: 0.0114\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0303 - val_loss: 0.0110\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0305 - val_loss: 0.0105\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0278 - val_loss: 0.0106\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0275 - val_loss: 0.0105\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0276 - val_loss: 0.0113\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0278 - val_loss: 0.0103\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0258 - val_loss: 0.0113\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0307 - val_loss: 0.0097\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0332 - val_loss: 0.0108\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0307 - val_loss: 0.0098\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0282 - val_loss: 0.0097\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0320 - val_loss: 0.0102\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0304 - val_loss: 0.0091\n",
      "Epoch 26/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0311 - val_loss: 0.0098\n",
      "Epoch 27/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0272 - val_loss: 0.0092\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 364ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "RMSE: (175.53043305303726, 12.776436233520505)\n",
      "RMSE Random Walk: (44.18634085140515, 5.202504124797967)\n",
      "[4586.22997284 4529.91997528 4421.86009216 4347.03992462 4383.57003021\n",
      " 4346.62995148 4279.97990417 4261.30000305 4201.30000305 4156.49998474\n",
      " 4560.49330902 4527.13687897 4395.30094147 4492.7800293  4414.91662598\n",
      " 4472.61608124 4403.06546021 4385.86499023 4361.59751129 4407.34455872\n",
      " 4356.78924561 4303.30176544 4315.28704834 4358.38024902 4346.62327576\n",
      " 4322.581604   4297.96326447 4281.46141815 4268.90245056 4259.98608398]\n",
      "[122.51000214 120.31999969 120.86000061 118.12000275 115.06999969\n",
      " 114.76999664 114.69999695 117.69999695 111.30000305 110.33999634\n",
      " 117.74738312 117.59225464 120.47229004 120.789711   119.05581665\n",
      " 118.81478119 119.36854553 120.2509613  118.68974304 119.64914703\n",
      " 119.50429535 118.91441345 118.62427521 118.64696503 119.11021423\n",
      " 118.00154877 116.99667358 115.89095306 114.96891785 114.36984253]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 139ms/step - loss: 0.2727 - val_loss: 0.0034\n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1949 - val_loss: 0.0172\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1390 - val_loss: 0.0121\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1013 - val_loss: 0.0049\n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0686 - val_loss: 0.0037\n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0408 - val_loss: 0.0077\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0222 - val_loss: 0.0123\n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0165 - val_loss: 0.0130\n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0138 - val_loss: 0.0096\n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0153 - val_loss: 0.0056\n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0128 - val_loss: 0.0039\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0136 - val_loss: 0.0030\n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0128 - val_loss: 0.0030\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0121 - val_loss: 0.0032\n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0108 - val_loss: 0.0037\n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0113 - val_loss: 0.0042\n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0105 - val_loss: 0.0041\n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0104 - val_loss: 0.0043\n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0103 - val_loss: 0.0041\n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0099 - val_loss: 0.0043\n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0121 - val_loss: 0.0040\n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0124 - val_loss: 0.0045\n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0107 - val_loss: 0.0043\n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0104 - val_loss: 0.0047\n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0107 - val_loss: 0.0046\n",
      "Epoch 26/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0114 - val_loss: 0.0050\n",
      "Epoch 27/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0101 - val_loss: 0.0052\n",
      "Epoch 28/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0097 - val_loss: 0.0055\n",
      "Epoch 29/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0090 - val_loss: 0.0055\n",
      "Epoch 30/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0101 - val_loss: 0.0056\n",
      "Epoch 31/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0094 - val_loss: 0.0054\n",
      "Epoch 32/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0101 - val_loss: 0.0057\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 357ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "RMSE: (611.6850826178442, 21.590258789062496)\n",
      "RMSE Random Walk: (128.41550608313847, 8.971654957171314)\n",
      "[4708.73997498 4650.23997498 4542.72009277 4465.15992737 4498.64002991\n",
      " 4461.39994812 4394.67990112 4379.         4312.6000061  4266.83998108\n",
      " 4678.24069214 4644.72913361 4515.77323151 4613.5697403  4533.97244263\n",
      " 4591.43086243 4522.43400574 4506.11595154 4480.28725433 4526.99370575\n",
      " 4476.29354095 4422.21617889 4433.91132355 4477.02721405 4465.73348999\n",
      " 4440.58315277 4414.95993805 4397.35237122 4383.87136841 4374.35592651]\n",
      "[244.11000061 242.69999695 249.30000305 249.11000061 245.69000244\n",
      " 241.1499939  245.16999817 241.16000366 226.53999329 224.55000305\n",
      " 229.68412781 209.46670532 215.68591309 232.50546265 216.16352844\n",
      " 208.11630249 212.19102478 211.07214355 216.70062256 224.70901489\n",
      " 202.28190613 195.86354065 209.09153748 204.98326111 209.75247192\n",
      " 209.18493652 208.71838379 208.69326782 209.03086853 205.69900513]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 151ms/step - loss: 0.2452 - val_loss: 0.0033\n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.1529 - val_loss: 0.0065\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.1221 - val_loss: 0.0024\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0732 - val_loss: 0.0019\n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0383 - val_loss: 0.0018\n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0242 - val_loss: 0.0034\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0175 - val_loss: 0.0052\n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0141 - val_loss: 0.0071\n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0162 - val_loss: 0.0071\n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0141 - val_loss: 0.0068\n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0136 - val_loss: 0.0057\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0140 - val_loss: 0.0049\n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0108 - val_loss: 0.0042\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0116 - val_loss: 0.0037\n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0113 - val_loss: 0.0035\n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0115 - val_loss: 0.0034\n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0107 - val_loss: 0.0034\n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0119 - val_loss: 0.0031\n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0111 - val_loss: 0.0031\n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0117 - val_loss: 0.0031\n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0123 - val_loss: 0.0029\n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0116 - val_loss: 0.0030\n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0124 - val_loss: 0.0028\n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0119 - val_loss: 0.0029\n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0117 - val_loss: 0.0028\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 351ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "RMSE: (531.5894855099615, 19.963333892822266)\n",
      "RMSE Random Walk: (120.03371292114795, 8.489475041552963)\n",
      "[4952.84997559 4892.93997192 4792.02009583 4714.26992798 4744.33003235\n",
      " 4702.54994202 4639.84989929 4620.16000366 4539.13999939 4491.38998413\n",
      " 4907.92481995 4854.19583893 4731.45914459 4846.07520294 4750.13597107\n",
      " 4799.54716492 4734.62503052 4717.18809509 4696.98787689 4751.70272064\n",
      " 4678.57544708 4618.07971954 4643.00286102 4682.01047516 4675.48596191\n",
      " 4649.76808929 4623.67832184 4606.04563904 4592.90223694 4580.05493164]\n",
      "[244.11000061 242.69999695 249.30000305 249.11000061 245.69000244\n",
      " 241.1499939  245.16999817 241.16000366 226.53999329 224.55000305\n",
      " 263.45489502 248.67004395 251.54312134 259.15628052 251.89212036\n",
      " 251.82662964 254.61843872 264.32733154 240.82270813 239.4846344\n",
      " 250.88987732 247.10049438 245.96786499 244.49813843 262.01574707\n",
      " 260.95614624 259.03161621 256.13903809 252.72341919 249.52816772]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 129ms/step - loss: 0.3947 - val_loss: 0.0087\n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2401 - val_loss: 0.0239\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1214 - val_loss: 0.0642\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0605 - val_loss: 0.0324\n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0523 - val_loss: 0.0163\n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0395 - val_loss: 0.0099\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0273 - val_loss: 0.0039\n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0298 - val_loss: 0.0037\n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0302 - val_loss: 0.0041\n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0289 - val_loss: 0.0046\n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0246 - val_loss: 0.0076\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0265 - val_loss: 0.0076\n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0256 - val_loss: 0.0093\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0220 - val_loss: 0.0083\n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0229 - val_loss: 0.0084\n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0221 - val_loss: 0.0082\n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0218 - val_loss: 0.0084\n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0204 - val_loss: 0.0083\n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0229 - val_loss: 0.0090\n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0233 - val_loss: 0.0095\n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0189 - val_loss: 0.0093\n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0221 - val_loss: 0.0104\n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0221 - val_loss: 0.0100\n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0181 - val_loss: 0.0112\n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0198 - val_loss: 0.0103\n",
      "Epoch 26/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0194 - val_loss: 0.0120\n",
      "Epoch 27/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0195 - val_loss: 0.0111\n",
      "Epoch 28/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0180 - val_loss: 0.0128\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 366ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "RMSE: (84.82489136795047, 7.5212646484375)\n",
      "RMSE Random Walk: (236.16034551058783, 12.426643324456505)\n",
      "[5196.9599762  5135.63996887 5041.32009888 4963.37992859 4990.02003479\n",
      " 4943.69993591 4885.01989746 4861.32000732 4765.67999268 4715.93998718\n",
      " 5171.37971497 5102.86588287 4983.00226593 5105.23148346 5002.02809143\n",
      " 5051.37379456 4989.24346924 4981.51542664 4937.81058502 4991.18735504\n",
      " 4929.4653244  4865.18021393 4888.97072601 4926.50861359 4937.50170898\n",
      " 4910.72423553 4882.70993805 4862.18467712 4845.62565613 4829.58309937]\n",
      "[183.08999634 183.16999817 169.27000427 159.1499939  169.58000183\n",
      " 160.72000122 159.92999268 160.19000244 168.80000305 167.11000061\n",
      " 172.00163269 167.61058044 164.56469727 168.65063477 153.98239136\n",
      " 164.02598572 161.39129639 169.11752319 170.16767883 163.08506775\n",
      " 156.14318848 161.86856079 153.49862671 155.61634827 158.43927002\n",
      " 156.81034851 153.96113586 155.07618713 154.43136597 156.21546936]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 132ms/step - loss: 0.3739 - val_loss: 0.0072\n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2148 - val_loss: 0.0208\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1047 - val_loss: 0.0390\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0600 - val_loss: 0.0174\n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0515 - val_loss: 0.0162\n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0391 - val_loss: 0.0180\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0390 - val_loss: 0.0098\n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0294 - val_loss: 0.0064\n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0296 - val_loss: 0.0059\n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0260 - val_loss: 0.0051\n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0258 - val_loss: 0.0046\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0258 - val_loss: 0.0050\n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0238 - val_loss: 0.0047\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0252 - val_loss: 0.0049\n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0255 - val_loss: 0.0056\n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0248 - val_loss: 0.0053\n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0246 - val_loss: 0.0064\n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0237 - val_loss: 0.0064\n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0199 - val_loss: 0.0062\n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0218 - val_loss: 0.0069\n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0222 - val_loss: 0.0072\n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0204 - val_loss: 0.0072\n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0256 - val_loss: 0.0080\n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0222 - val_loss: 0.0075\n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0210 - val_loss: 0.0083\n",
      "Epoch 26/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0217 - val_loss: 0.0083\n",
      "Epoch 27/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0215 - val_loss: 0.0086\n",
      "Epoch 28/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0185 - val_loss: 0.0085\n",
      "Epoch 29/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0212 - val_loss: 0.0092\n",
      "Epoch 30/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0217 - val_loss: 0.0085\n",
      "Epoch 31/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0190 - val_loss: 0.0089\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 432ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "RMSE: (190.12260740661296, 11.610857391357422)\n",
      "RMSE Random Walk: (240.75857502918137, 12.412228204244695)\n",
      "[5380.04997253 5318.80996704 5210.59010315 5122.52992249 5159.60003662\n",
      " 5104.41993713 5044.94989014 5021.51000977 4934.47999573 4883.04998779\n",
      " 5343.38134766 5270.47646332 5147.5669632  5273.88211823 5156.01048279\n",
      " 5215.39978027 5150.63476562 5150.63294983 5107.97826385 5154.27242279\n",
      " 5085.60851288 5027.04877472 5042.46935272 5082.12496185 5095.940979\n",
      " 5067.53458405 5036.67107391 5017.26086426 5000.05702209 4985.79856873]\n",
      "[183.08999634 183.16999817 169.27000427 159.1499939  169.58000183\n",
      " 160.72000122 159.92999268 160.19000244 168.80000305 167.11000061\n",
      " 177.14338684 175.87792969 165.7538147  167.98573303 159.09541321\n",
      " 173.08236694 167.35986328 164.02992249 163.58425903 166.04212952\n",
      " 168.09544373 160.01774597 167.25801086 163.5962677  165.88616943\n",
      " 164.4644165  163.32966614 163.39743042 164.26173401 165.02655029]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 105ms/step - loss: 0.3918 - val_loss: 0.0685\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2279 - val_loss: 0.0525\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1205 - val_loss: 0.0254\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0684 - val_loss: 0.0153\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0430 - val_loss: 0.0183\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0356 - val_loss: 0.0221\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0321 - val_loss: 0.0332\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0314 - val_loss: 0.0350\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0284 - val_loss: 0.0373\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0312 - val_loss: 0.0309\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0279 - val_loss: 0.0342\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0299 - val_loss: 0.0295\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0289 - val_loss: 0.0332\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0291 - val_loss: 0.0305\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0282 - val_loss: 0.0345\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0284 - val_loss: 0.0370\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0252 - val_loss: 0.0365\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0279 - val_loss: 0.0352\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0274 - val_loss: 0.0378\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0269 - val_loss: 0.0350\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0265 - val_loss: 0.0377\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0253 - val_loss: 0.0373\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0234 - val_loss: 0.0371\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0246 - val_loss: 0.0377\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 342ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "RMSE: (251.78590178354062, 14.980699157714843)\n",
      "RMSE Random Walk: (57.60140721392979, 6.1438158228709785)\n",
      "[5563.13996887 5501.97996521 5379.86010742 5281.67991638 5329.18003845\n",
      " 5265.13993835 5204.87988281 5181.70001221 5103.27999878 5050.1599884\n",
      " 5520.5247345  5446.35439301 5313.32077789 5441.86785126 5315.105896\n",
      " 5388.48214722 5317.99462891 5314.66287231 5271.56252289 5320.31455231\n",
      " 5253.7039566  5187.06652069 5209.72736359 5245.72122955 5261.82714844\n",
      " 5231.99900055 5200.00074005 5180.65829468 5164.3187561  5150.82511902]\n",
      "[142.30000305 138.22999573 133.22000122 133.61999512 133.80000305\n",
      " 137.27999878 130.75       129.78999329 128.72999573 126.76999664\n",
      " 134.61230469 142.46240234 136.67375183 145.47520447 144.55895996\n",
      " 148.62667847 143.08605957 142.51237488 138.17848206 141.7510376\n",
      " 137.43545532 149.36450195 152.17845154 137.99131775 131.02886963\n",
      " 130.52903748 129.93034363 129.58073425 129.25965881 128.86837769]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 107ms/step - loss: 0.3602 - val_loss: 0.0663\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2411 - val_loss: 0.0328\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0972 - val_loss: 0.0176\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0469 - val_loss: 0.0170\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0446 - val_loss: 0.0238\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0331 - val_loss: 0.0302\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0323 - val_loss: 0.0326\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0336 - val_loss: 0.0400\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0312 - val_loss: 0.0373\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0305 - val_loss: 0.0433\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0275 - val_loss: 0.0393\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0276 - val_loss: 0.0401\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0276 - val_loss: 0.0391\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0289 - val_loss: 0.0363\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0265 - val_loss: 0.0395\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0310 - val_loss: 0.0356\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0284 - val_loss: 0.0392\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0309 - val_loss: 0.0357\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0281 - val_loss: 0.0373\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0247 - val_loss: 0.0381\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0250 - val_loss: 0.0374\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0254 - val_loss: 0.0387\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0268 - val_loss: 0.0394\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0272 - val_loss: 0.0385\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 341ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "RMSE: (365.2459190470574, 17.656361389160157)\n",
      "RMSE Random Walk: (56.86541141672527, 5.9775508591161906)\n",
      "[5705.43997192 5640.20996094 5513.08010864 5415.2999115  5462.9800415\n",
      " 5402.41993713 5335.62988281 5311.49000549 5232.00999451 5176.92998505\n",
      " 5655.13703918 5588.81679535 5449.99452972 5587.34305573 5459.66485596\n",
      " 5537.10882568 5461.08068848 5457.17524719 5409.74100494 5462.0655899\n",
      " 5391.13941193 5336.43102264 5361.90581512 5383.7125473  5392.85601807\n",
      " 5362.52803802 5329.93108368 5310.23902893 5293.57841492 5279.6934967 ]\n",
      "[142.30000305 138.22999573 133.22000122 133.61999512 133.80000305\n",
      " 137.27999878 130.75       129.78999329 128.72999573 126.76999664\n",
      " 145.97624207 137.45454407 135.08825684 148.49259949 137.84738159\n",
      " 145.33966064 135.52638245 144.22026062 143.1905365  134.83145142\n",
      " 152.11386108 152.30223083 147.26681519 138.48927307 140.40637207\n",
      " 139.13366699 138.01937866 137.35284424 136.70727539 136.01098633]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 106ms/step - loss: 0.3495 - val_loss: 0.1532\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1645 - val_loss: 0.0400\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0845 - val_loss: 0.0344\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0507 - val_loss: 0.0381\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0419 - val_loss: 0.0278\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0352 - val_loss: 0.0421\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0374 - val_loss: 0.0542\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0350 - val_loss: 0.0428\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0319 - val_loss: 0.0607\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0335 - val_loss: 0.0555\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0320 - val_loss: 0.0561\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0341 - val_loss: 0.0583\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0318 - val_loss: 0.0557\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0278 - val_loss: 0.0579\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0317 - val_loss: 0.0563\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0323 - val_loss: 0.0610\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0292 - val_loss: 0.0629\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0294 - val_loss: 0.0649\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0284 - val_loss: 0.0639\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0294 - val_loss: 0.0718\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0268 - val_loss: 0.0652\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0300 - val_loss: 0.0810\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0290 - val_loss: 0.0625\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0304 - val_loss: 0.0774\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0266 - val_loss: 0.0752\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 366ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "RMSE: (181.61428097990574, 12.74657974243164)\n",
      "RMSE Random Walk: (31.529269921156573, 4.528943932406378)\n",
      "[5847.73997498 5778.43995667 5646.30010986 5548.91990662 5596.78004456\n",
      " 5539.69993591 5466.37988281 5441.27999878 5360.73999023 5303.69998169\n",
      " 5801.11328125 5726.27133942 5585.08278656 5735.83565521 5597.51223755\n",
      " 5682.44848633 5596.60707092 5601.39550781 5552.93154144 5596.89704132\n",
      " 5543.25327301 5488.73325348 5509.17263031 5522.20182037 5533.26239014\n",
      " 5501.66170502 5467.95046234 5447.59187317 5430.28569031 5415.70448303]\n",
      "[174.1499939  171.52000427 167.57000732 167.22999573 167.52999878\n",
      " 170.02999878 163.61999512 161.38000488 158.91000366 157.22000122\n",
      " 172.66963196 170.90458679 168.53260803 166.87036133 163.22036743\n",
      " 163.20936584 166.41574097 161.05638123 167.60665894 165.8311615\n",
      " 169.19155884 163.36904907 166.64022827 170.10342407 167.11398315\n",
      " 166.47383118 165.88388062 165.4186554  164.9702301  164.30986023]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 154ms/step - loss: 0.3588 - val_loss: 0.1772\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1671 - val_loss: 0.0705\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0840 - val_loss: 0.0452\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0501 - val_loss: 0.0386\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0429 - val_loss: 0.0294\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0377 - val_loss: 0.0397\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0361 - val_loss: 0.0354\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0369 - val_loss: 0.0349\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0360 - val_loss: 0.0405\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0347 - val_loss: 0.0361\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0350 - val_loss: 0.0428\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0319 - val_loss: 0.0388\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0315 - val_loss: 0.0403\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0330 - val_loss: 0.0393\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0324 - val_loss: 0.0436\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0331 - val_loss: 0.0394\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0321 - val_loss: 0.0384\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0330 - val_loss: 0.0407\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0323 - val_loss: 0.0364\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0325 - val_loss: 0.0400\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0328 - val_loss: 0.0374\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0317 - val_loss: 0.0386\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0329 - val_loss: 0.0375\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0309 - val_loss: 0.0390\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0294 - val_loss: 0.0394\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 339ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "RMSE: (183.75488105221885, 12.785501861572266)\n",
      "RMSE Random Walk: (27.90124610547473, 4.159487787136753)\n",
      "[6021.88996887 5949.95996094 5813.87011719 5716.14990234 5764.31004333\n",
      " 5709.72993469 5629.99987793 5602.66000366 5519.6499939  5460.91998291\n",
      " 5973.78291321 5897.17592621 5753.61539459 5902.70601654 5760.73260498\n",
      " 5845.65785217 5763.02281189 5762.45188904 5720.53820038 5762.72820282\n",
      " 5712.44483185 5652.10230255 5675.81285858 5692.30524445 5700.37637329\n",
      " 5668.13553619 5633.83434296 5613.01052856 5595.25592041 5580.01434326]\n",
      "[174.1499939  171.52000427 167.57000732 167.22999573 167.52999878\n",
      " 170.02999878 163.61999512 161.38000488 158.91000366 157.22000122\n",
      " 169.43887329 169.97344971 167.69300842 163.20736694 172.98202515\n",
      " 166.70257568 164.44474792 171.28382874 168.77931213 164.79785156\n",
      " 166.96206665 165.71324158 163.76733398 162.11003113 166.93138123\n",
      " 166.33392334 165.69320679 165.14642334 164.61524963 163.99411011]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 105ms/step - loss: 0.3653 - val_loss: 0.0201\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2067 - val_loss: 0.0603\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1368 - val_loss: 0.0231\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0885 - val_loss: 0.0195\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0588 - val_loss: 0.0142\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0443 - val_loss: 0.0092\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0339 - val_loss: 0.0120\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0306 - val_loss: 0.0163\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0297 - val_loss: 0.0125\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0308 - val_loss: 0.0142\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0325 - val_loss: 0.0109\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0254 - val_loss: 0.0105\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0308 - val_loss: 0.0092\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0294 - val_loss: 0.0097\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0280 - val_loss: 0.0106\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0263 - val_loss: 0.0103\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0244 - val_loss: 0.0111\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0295 - val_loss: 0.0097\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0263 - val_loss: 0.0107\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0264 - val_loss: 0.0103\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0268 - val_loss: 0.0103\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0262 - val_loss: 0.0104\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0251 - val_loss: 0.0095\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0271 - val_loss: 0.0091\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0265 - val_loss: 0.0097\n",
      "Epoch 26/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0256 - val_loss: 0.0091\n",
      "Epoch 27/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0297 - val_loss: 0.0099\n",
      "Epoch 28/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0250 - val_loss: 0.0091\n",
      "Epoch 29/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0257 - val_loss: 0.0102\n",
      "Epoch 30/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0244 - val_loss: 0.0097\n",
      "Epoch 31/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0233 - val_loss: 0.0095\n",
      "Epoch 32/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0251 - val_loss: 0.0102\n",
      "Epoch 33/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0263 - val_loss: 0.0097\n",
      "Epoch 34/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0225 - val_loss: 0.0099\n",
      "Epoch 35/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0243 - val_loss: 0.0093\n",
      "Epoch 36/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0230 - val_loss: 0.0105\n",
      "Epoch 37/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0262 - val_loss: 0.0093\n",
      "Epoch 38/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0243 - val_loss: 0.0113\n",
      "Epoch 39/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0271 - val_loss: 0.0095\n",
      "Epoch 40/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0293 - val_loss: 0.0106\n",
      "Epoch 41/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0292 - val_loss: 0.0092\n",
      "Epoch 42/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0275 - val_loss: 0.0097\n",
      "Epoch 43/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0230 - val_loss: 0.0096\n",
      "Epoch 44/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0256 - val_loss: 0.0102\n",
      "Epoch 45/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0223 - val_loss: 0.0103\n",
      "Epoch 46/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0247 - val_loss: 0.0097\n",
      "Epoch 47/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0245 - val_loss: 0.0102\n",
      "Epoch 48/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0237 - val_loss: 0.0102\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 345ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "RMSE: (133.3403115535824, 10.964642715454099)\n",
      "RMSE Random Walk: (42.88681002223107, 5.194734883656744)\n",
      "[6196.03996277 6121.47996521 5981.44012451 5883.37989807 5931.84004211\n",
      " 5879.75993347 5793.61987305 5764.04000854 5678.55999756 5618.13998413\n",
      " 6143.2217865  6067.14937592 5921.30840302 6065.91338348 5933.71463013\n",
      " 6012.36042786 5927.46755981 5933.73571777 5889.31751251 5927.52605438\n",
      " 5879.4068985  5817.81554413 5839.58019257 5854.41527557 5867.30775452\n",
      " 5834.46945953 5799.52754974 5778.1569519  5759.87117004 5744.00845337]\n",
      "[122.51000214 120.31999969 120.86000061 118.12000275 115.06999969\n",
      " 114.76999664 114.69999695 117.69999695 111.30000305 110.33999634\n",
      " 116.68526459 117.85140228 117.86769867 117.35243988 117.40313721\n",
      " 116.6608963  117.64987946 117.037117   117.26306152 116.8113327\n",
      " 117.12606049 116.95807648 117.35597992 115.84149933 117.02819061\n",
      " 116.09475708 115.4495697  114.58901978 114.35068512 113.84679413]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 107ms/step - loss: 0.4056 - val_loss: 0.0234\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1912 - val_loss: 0.0328\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1252 - val_loss: 0.0293\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0676 - val_loss: 0.0136\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0511 - val_loss: 0.0158\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0402 - val_loss: 0.0148\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0341 - val_loss: 0.0088\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0348 - val_loss: 0.0090\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0295 - val_loss: 0.0095\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0294 - val_loss: 0.0111\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0294 - val_loss: 0.0104\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0305 - val_loss: 0.0114\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0280 - val_loss: 0.0117\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0280 - val_loss: 0.0116\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0290 - val_loss: 0.0105\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0253 - val_loss: 0.0107\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0285 - val_loss: 0.0097\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0249 - val_loss: 0.0108\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0309 - val_loss: 0.0098\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0285 - val_loss: 0.0095\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0280 - val_loss: 0.0103\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0269 - val_loss: 0.0098\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0281 - val_loss: 0.0111\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0279 - val_loss: 0.0101\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0281 - val_loss: 0.0112\n",
      "Epoch 26/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0276 - val_loss: 0.0106\n",
      "Epoch 27/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0272 - val_loss: 0.0104\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 353ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "RMSE: (174.85112757799212, 12.745480346679685)\n",
      "RMSE Random Walk: (41.548471377646436, 4.987204102348556)\n",
      "[6318.5499649  6241.7999649  6102.30012512 6001.49990082 6046.91004181\n",
      " 5994.52993011 5908.31987    5881.74000549 5789.86000061 5728.47998047\n",
      " 6259.90705109 6185.0007782  6039.17610168 6183.26582336 6051.11776733\n",
      " 6129.02132416 6045.11743927 6050.77283478 6006.58057404 6044.33738708\n",
      " 5996.53295898 5934.77362061 5956.93617249 5970.2567749  5984.33594513\n",
      " 5950.56421661 5914.97711945 5892.74597168 5874.22185516 5857.8552475 ]\n",
      "[122.51000214 120.31999969 120.86000061 118.12000275 115.06999969\n",
      " 114.76999664 114.69999695 117.69999695 111.30000305 110.33999634\n",
      " 117.88542938 119.12058258 120.29930115 119.99958038 121.24681091\n",
      " 119.27135468 118.91732788 117.93977356 118.83857727 118.92032623\n",
      " 119.35980225 119.01786041 120.25483704 119.74401093 118.38507843\n",
      " 117.35545349 116.42612457 115.37428284 114.52881622 113.95428467]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 133ms/step - loss: 0.3098 - val_loss: 0.0038\n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2458 - val_loss: 0.0122\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1221 - val_loss: 0.0124\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0688 - val_loss: 0.0031\n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0445 - val_loss: 0.0023\n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0258 - val_loss: 0.0062\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0173 - val_loss: 0.0107\n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0146 - val_loss: 0.0116\n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0152 - val_loss: 0.0102\n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0128 - val_loss: 0.0076\n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0116 - val_loss: 0.0059\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0121 - val_loss: 0.0049\n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0114 - val_loss: 0.0050\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0113 - val_loss: 0.0051\n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0113 - val_loss: 0.0055\n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0105 - val_loss: 0.0055\n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0100 - val_loss: 0.0055\n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0109 - val_loss: 0.0055\n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0103 - val_loss: 0.0055\n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0111 - val_loss: 0.0054\n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0109 - val_loss: 0.0056\n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0095 - val_loss: 0.0060\n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0090 - val_loss: 0.0061\n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0090 - val_loss: 0.0061\n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0099 - val_loss: 0.0062\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "RMSE: (376.1023997526032, 15.782421112060543)\n",
      "RMSE Random Walk: (105.99444965207161, 8.020558643691611)\n",
      "[6441.05996704 6362.1199646  6223.16012573 6119.61990356 6161.9800415\n",
      " 6109.29992676 6023.01986694 5999.44000244 5901.16000366 5838.81997681\n",
      " 6377.79248047 6304.12136078 6159.47540283 6303.26540375 6172.36457825\n",
      " 6248.29267883 6164.03476715 6168.71260834 6125.41915131 6163.25771332\n",
      " 6115.89276123 6053.79148102 6077.19100952 6090.00078583 6102.72102356\n",
      " 6067.9196701  6031.40324402 6008.12025452 5988.75067139 5971.80953217]\n",
      "[244.11000061 242.69999695 249.30000305 249.11000061 245.69000244\n",
      " 241.1499939  245.16999817 241.16000366 226.53999329 224.55000305\n",
      " 262.39022827 238.55561829 232.39360046 225.12158203 233.63583374\n",
      " 218.77186584 227.4503479  243.29585266 218.26130676 222.34477234\n",
      " 228.48219299 245.17201233 276.11541748 233.24333191 213.2890625\n",
      " 209.92086792 210.10887146 209.29537964 207.86030579 204.62411499]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 225ms/step - loss: 0.2690 - val_loss: 0.0028\n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.1476 - val_loss: 0.0087\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.1069 - val_loss: 0.0066\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0538 - val_loss: 0.0034\n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0323 - val_loss: 0.0031\n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0256 - val_loss: 0.0020\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0221 - val_loss: 0.0021\n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0153 - val_loss: 0.0028\n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0138 - val_loss: 0.0043\n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0131 - val_loss: 0.0052\n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0130 - val_loss: 0.0053\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0134 - val_loss: 0.0050\n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0121 - val_loss: 0.0044\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0121 - val_loss: 0.0038\n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0121 - val_loss: 0.0034\n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0115 - val_loss: 0.0030\n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0121 - val_loss: 0.0029\n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0131 - val_loss: 0.0027\n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0115 - val_loss: 0.0027\n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0122 - val_loss: 0.0027\n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0124 - val_loss: 0.0028\n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0107 - val_loss: 0.0028\n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0110 - val_loss: 0.0028\n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0113 - val_loss: 0.0027\n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0113 - val_loss: 0.0026\n",
      "Epoch 26/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0123 - val_loss: 0.0026\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 404ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "RMSE: (592.8992114786294, 19.176620483398438)\n",
      "RMSE Random Walk: (125.02883316638568, 9.066584826659575)\n",
      "[6685.16996765 6604.81996155 6472.46012878 6368.72990417 6407.67004395\n",
      " 6350.44992065 6268.18986511 6240.6000061  6127.69999695 6063.36997986\n",
      " 6640.18270874 6542.67697906 6391.8690033  6528.38698578 6406.00041199\n",
      " 6467.06454468 6391.48511505 6412.008461   6343.68045807 6385.60248566\n",
      " 6344.37495422 6298.96349335 6353.306427   6323.24411774 6316.01008606\n",
      " 6277.84053802 6241.51211548 6217.41563416 6196.61097717 6176.43364716]\n",
      "[244.11000061 242.69999695 249.30000305 249.11000061 245.69000244\n",
      " 241.1499939  245.16999817 241.16000366 226.53999329 224.55000305\n",
      " 244.66702271 280.52209473 237.7428894  257.19277954 265.74725342\n",
      " 270.97592163 241.09692383 259.51730347 276.12081909 255.37335205\n",
      " 237.73193359 251.00013733 255.69923401 242.13900757 242.52839661\n",
      " 242.06274414 241.00494385 239.33432007 237.34803772 235.43859863]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 147ms/step - loss: 0.3434 - val_loss: 0.0132\n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.1992 - val_loss: 0.0544\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1568 - val_loss: 0.0311\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0865 - val_loss: 0.0176\n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0579 - val_loss: 0.0145\n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0402 - val_loss: 0.0057\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0325 - val_loss: 0.0051\n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0327 - val_loss: 0.0046\n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0355 - val_loss: 0.0046\n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0273 - val_loss: 0.0054\n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0278 - val_loss: 0.0060\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0239 - val_loss: 0.0083\n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0246 - val_loss: 0.0082\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0225 - val_loss: 0.0080\n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0249 - val_loss: 0.0077\n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0209 - val_loss: 0.0072\n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0210 - val_loss: 0.0078\n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0230 - val_loss: 0.0089\n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0210 - val_loss: 0.0081\n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0257 - val_loss: 0.0117\n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0220 - val_loss: 0.0092\n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0211 - val_loss: 0.0120\n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0185 - val_loss: 0.0098\n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0209 - val_loss: 0.0112\n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0252 - val_loss: 0.0100\n",
      "Epoch 26/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0188 - val_loss: 0.0102\n",
      "Epoch 27/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0177 - val_loss: 0.0115\n",
      "Epoch 28/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0206 - val_loss: 0.0124\n",
      "Epoch 29/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0200 - val_loss: 0.0116\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 397ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "RMSE: (161.57631140416487, 10.576805114746094)\n",
      "RMSE Random Walk: (221.93448082785352, 12.082668387748834)\n",
      "[6929.27996826 6847.5199585  6721.76013184 6617.83990479 6653.36004639\n",
      " 6591.59991455 6513.35986328 6481.76000977 6354.23999023 6287.91998291\n",
      " 6884.84973145 6823.19907379 6629.6118927  6785.57976532 6671.74766541\n",
      " 6738.04046631 6632.58203888 6671.52576447 6619.80127716 6640.97583771\n",
      " 6582.10688782 6549.96363068 6609.00566101 6565.38312531 6558.53848267\n",
      " 6519.90328217 6482.51705933 6456.74995422 6433.95901489 6411.87224579]\n",
      "[183.08999634 183.16999817 169.27000427 159.1499939  169.58000183\n",
      " 160.72000122 159.92999268 160.19000244 168.80000305 167.11000061\n",
      " 176.86373901 163.71908569 172.9269104  168.40031433 162.9183197\n",
      " 161.79704285 159.79162598 157.35090637 143.99568176 162.27662659\n",
      " 158.60455322 162.20916748 169.36273193 159.85012817 162.46228027\n",
      " 161.27856445 160.14927673 160.62278748 161.14053345 162.0406189 ]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 153ms/step - loss: 0.3994 - val_loss: 0.0087\n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.2914 - val_loss: 0.0119\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.1397 - val_loss: 0.0300\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0757 - val_loss: 0.0218\n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0505 - val_loss: 0.0189\n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0387 - val_loss: 0.0132\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0328 - val_loss: 0.0064\n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0277 - val_loss: 0.0054\n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0265 - val_loss: 0.0047\n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0265 - val_loss: 0.0045\n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0240 - val_loss: 0.0045\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0243 - val_loss: 0.0048\n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0253 - val_loss: 0.0054\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0266 - val_loss: 0.0062\n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0219 - val_loss: 0.0064\n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0212 - val_loss: 0.0072\n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0229 - val_loss: 0.0070\n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0262 - val_loss: 0.0085\n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0240 - val_loss: 0.0081\n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0215 - val_loss: 0.0085\n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0223 - val_loss: 0.0085\n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0207 - val_loss: 0.0082\n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0209 - val_loss: 0.0084\n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0221 - val_loss: 0.0084\n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0200 - val_loss: 0.0087\n",
      "Epoch 26/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0185 - val_loss: 0.0088\n",
      "Epoch 27/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0199 - val_loss: 0.0101\n",
      "Epoch 28/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0218 - val_loss: 0.0106\n",
      "Epoch 29/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0204 - val_loss: 0.0102\n",
      "Epoch 30/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0201 - val_loss: 0.0112\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "RMSE: (185.92621802570065, 11.546764373779297)\n",
      "RMSE Random Walk: (220.35042890788222, 12.209683078713107)\n",
      "[7112.3699646  7030.68995667 6891.03013611 6776.98989868 6822.94004822\n",
      " 6752.31991577 6673.28985596 6641.95001221 6523.03999329 6455.02998352\n",
      " 7061.71347046 6986.91815948 6802.5388031  6953.98007965 6834.66598511\n",
      " 6899.83750916 6792.37366486 6828.87667084 6763.79695892 6803.25246429\n",
      " 6740.71144104 6712.17279816 6778.36839294 6725.23325348 6721.00076294\n",
      " 6681.18184662 6642.66633606 6617.3727417  6595.09954834 6573.91286469]\n",
      "[183.08999634 183.16999817 169.27000427 159.1499939  169.58000183\n",
      " 160.72000122 159.92999268 160.19000244 168.80000305 167.11000061\n",
      " 179.7518158  183.71537781 168.55438232 170.46870422 162.42469788\n",
      " 166.03858948 165.05921936 165.77008057 167.97589111 169.83952332\n",
      " 156.34350586 171.5557251  164.27723694 165.32717896 165.57215881\n",
      " 164.50836182 163.70144653 163.72961426 164.34567261 164.96884155]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 167ms/step - loss: 0.3480 - val_loss: 0.0530\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1493 - val_loss: 0.0312\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0737 - val_loss: 0.0188\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0496 - val_loss: 0.0191\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0402 - val_loss: 0.0197\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0381 - val_loss: 0.0280\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0293 - val_loss: 0.0362\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0312 - val_loss: 0.0411\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0291 - val_loss: 0.0426\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0251 - val_loss: 0.0415\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0285 - val_loss: 0.0386\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0258 - val_loss: 0.0370\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0284 - val_loss: 0.0354\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0269 - val_loss: 0.0356\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0285 - val_loss: 0.0335\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0282 - val_loss: 0.0354\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0253 - val_loss: 0.0339\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0282 - val_loss: 0.0362\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0248 - val_loss: 0.0365\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0260 - val_loss: 0.0371\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0247 - val_loss: 0.0365\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0236 - val_loss: 0.0381\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0256 - val_loss: 0.0362\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 801ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "RMSE: (256.0233116594347, 14.52458152770996)\n",
      "RMSE Random Walk: (56.212420903012344, 6.032846708113075)\n",
      "[7295.45996094 7213.85995483 7060.30014038 6936.13989258 6992.52005005\n",
      " 6913.03991699 6833.21984863 6802.14001465 6691.83999634 6622.13998413\n",
      " 7241.46528625 7170.63353729 6971.09318542 7124.44878387 6997.09068298\n",
      " 7065.87609863 6957.43288422 6994.6467514  6931.77285004 6973.09198761\n",
      " 6897.0549469  6883.72852325 6942.64562988 6890.56043243 6886.57292175\n",
      " 6845.69020844 6806.36778259 6781.10235596 6759.44522095 6738.88170624]\n",
      "[142.30000305 138.22999573 133.22000122 133.61999512 133.80000305\n",
      " 137.27999878 130.75       129.78999329 128.72999573 126.76999664\n",
      " 135.43083191 144.95822144 149.23008728 144.33102417 142.42514038\n",
      " 148.27568054 142.11268616 151.92610168 140.53988647 131.98365784\n",
      " 142.60395813 152.42770386 133.90715027 146.75794983 127.28640747\n",
      " 126.95465088 126.36217499 126.08852386 125.83657837 125.54323578]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 132ms/step - loss: 0.3820 - val_loss: 0.0671\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.2211 - val_loss: 0.0270\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0822 - val_loss: 0.0133\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0567 - val_loss: 0.0208\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0387 - val_loss: 0.0227\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0377 - val_loss: 0.0274\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0344 - val_loss: 0.0317\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0294 - val_loss: 0.0367\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0336 - val_loss: 0.0411\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0309 - val_loss: 0.0395\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0263 - val_loss: 0.0436\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0273 - val_loss: 0.0414\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0308 - val_loss: 0.0413\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0269 - val_loss: 0.0424\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0270 - val_loss: 0.0412\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0283 - val_loss: 0.0406\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0278 - val_loss: 0.0420\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0255 - val_loss: 0.0385\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0248 - val_loss: 0.0428\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0295 - val_loss: 0.0369\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0267 - val_loss: 0.0410\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0307 - val_loss: 0.0370\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0279 - val_loss: 0.0413\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 512ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "RMSE: (801.5960324521002, 25.524425506591797)\n",
      "RMSE Random Walk: (52.32971560448883, 5.454762515331824)\n",
      "[7437.75996399 7352.08995056 7193.5201416  7069.7598877  7126.3200531\n",
      " 7050.31991577 6963.96984863 6931.93000793 6820.56999207 6748.90998077\n",
      " 7376.89611816 7315.59175873 7120.32327271 7268.77980804 7139.51582336\n",
      " 7214.15177917 7099.54557037 7146.57285309 7072.31273651 7105.07564545\n",
      " 7039.65890503 7036.15622711 7076.55278015 7037.31838226 7013.85932922\n",
      " 6972.64485931 6932.72995758 6907.19087982 6885.28179932 6864.42494202]\n",
      "[142.30000305 138.22999573 133.22000122 133.61999512 133.80000305\n",
      " 137.27999878 130.75       129.78999329 128.72999573 126.76999664\n",
      " 144.84664917 143.27070618 141.02938843 142.74539185 142.75993347\n",
      " 128.40556335 146.01403809 143.51759338 153.20947266 155.80503845\n",
      " 140.7424469  155.53330994 152.19891357 144.04237366 159.99258423\n",
      " 158.30761719 156.79502869 156.06185913 155.39639282 154.5753479 ]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 122ms/step - loss: 0.3796 - val_loss: 0.2012\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2382 - val_loss: 0.1025\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.1424 - val_loss: 0.0629\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0875 - val_loss: 0.0483\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0524 - val_loss: 0.0283\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0418 - val_loss: 0.0319\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0385 - val_loss: 0.0408\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0349 - val_loss: 0.0434\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0344 - val_loss: 0.0621\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0334 - val_loss: 0.0594\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0340 - val_loss: 0.0491\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0316 - val_loss: 0.0495\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0303 - val_loss: 0.0520\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0315 - val_loss: 0.0489\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0310 - val_loss: 0.0581\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0298 - val_loss: 0.0588\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0286 - val_loss: 0.0634\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0286 - val_loss: 0.0611\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0296 - val_loss: 0.0643\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0281 - val_loss: 0.0849\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0313 - val_loss: 0.0572\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0306 - val_loss: 0.0718\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0274 - val_loss: 0.0714\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0273 - val_loss: 0.0615\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0267 - val_loss: 0.0854\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "RMSE: (117.530560779816, 10.011954498291015)\n",
      "RMSE Random Walk: (28.83900085722446, 4.332675060977944)\n",
      "[7580.05996704 7490.31994629 7326.74014282 7203.37988281 7260.12005615\n",
      " 7187.59991455 7094.71984863 7061.72000122 6949.29998779 6875.67997742\n",
      " 7521.74276733 7458.8624649  7261.35266113 7411.52519989 7282.27575684\n",
      " 7342.55734253 7245.55960846 7290.09044647 7225.52220917 7260.8806839\n",
      " 7180.40135193 7191.68953705 7228.75169373 7181.36075592 7173.85191345\n",
      " 7130.9524765  7089.52498627 7063.25273895 7040.67819214 7019.00028992]\n",
      "[174.1499939  171.52000427 167.57000732 167.22999573 167.52999878\n",
      " 170.02999878 163.61999512 161.38000488 158.91000366 157.22000122\n",
      " 167.60102844 168.2784729  169.54093933 163.54139709 158.47871399\n",
      " 163.55433655 164.94984436 163.61151123 167.542099   167.51893616\n",
      " 166.09877014 164.56855774 173.79223633 158.04081726 160.87117004\n",
      " 160.34242249 159.8221283  159.44895935 159.051651   158.44506836]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 191ms/step - loss: 0.3601 - val_loss: 0.1814\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1904 - val_loss: 0.0613\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0898 - val_loss: 0.0313\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0465 - val_loss: 0.0353\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0474 - val_loss: 0.0265\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0407 - val_loss: 0.0418\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0357 - val_loss: 0.0388\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0386 - val_loss: 0.0445\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0372 - val_loss: 0.0445\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0339 - val_loss: 0.0412\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0354 - val_loss: 0.0481\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0330 - val_loss: 0.0401\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0352 - val_loss: 0.0488\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0366 - val_loss: 0.0354\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0351 - val_loss: 0.0488\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0357 - val_loss: 0.0335\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0335 - val_loss: 0.0420\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0361 - val_loss: 0.0441\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0313 - val_loss: 0.0351\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0362 - val_loss: 0.0437\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0338 - val_loss: 0.0419\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0334 - val_loss: 0.0403\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0335 - val_loss: 0.0424\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0319 - val_loss: 0.0380\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0309 - val_loss: 0.0430\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "RMSE: (244.91956580353434, 14.184783935546875)\n",
      "RMSE Random Walk: (29.920174528564736, 4.135721565122763)\n",
      "[7754.20996094 7661.83995056 7494.31015015 7370.60987854 7427.65005493\n",
      " 7357.62991333 7258.33984375 7223.1000061  7108.20999146 7032.89997864\n",
      " 7689.34379578 7627.14093781 7430.89360046 7575.06659698 7440.75447083\n",
      " 7506.11167908 7410.50945282 7453.7019577  7393.06430817 7428.39962006\n",
      " 7346.50012207 7356.25809479 7402.54393005 7339.40157318 7334.7230835\n",
      " 7291.29489899 7249.34711456 7222.7016983  7199.72984314 7177.44535828]\n",
      "[174.1499939  171.52000427 167.57000732 167.22999573 167.52999878\n",
      " 170.02999878 163.61999512 161.38000488 158.91000366 157.22000122\n",
      " 167.68925476 170.65931702 168.30125427 165.15603638 164.29873657\n",
      " 166.51628113 164.81793213 168.97137451 161.56890869 164.07443237\n",
      " 166.97619629 164.77561951 163.3014679  167.45213318 174.09321594\n",
      " 173.35626221 172.57626343 171.98953247 171.38485718 170.59657288]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 113ms/step - loss: 0.4219 - val_loss: 0.0225\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2273 - val_loss: 0.0383\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1141 - val_loss: 0.0346\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0677 - val_loss: 0.0254\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0455 - val_loss: 0.0120\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0381 - val_loss: 0.0095\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0312 - val_loss: 0.0142\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0331 - val_loss: 0.0137\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0285 - val_loss: 0.0126\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0281 - val_loss: 0.0120\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0261 - val_loss: 0.0115\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0266 - val_loss: 0.0101\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0283 - val_loss: 0.0102\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0299 - val_loss: 0.0097\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0255 - val_loss: 0.0099\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0280 - val_loss: 0.0095\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0271 - val_loss: 0.0093\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0267 - val_loss: 0.0094\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0285 - val_loss: 0.0100\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0268 - val_loss: 0.0099\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0277 - val_loss: 0.0108\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0316 - val_loss: 0.0101\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0293 - val_loss: 0.0094\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0311 - val_loss: 0.0101\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0256 - val_loss: 0.0093\n",
      "Epoch 26/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0256 - val_loss: 0.0106\n",
      "Epoch 27/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0299 - val_loss: 0.0098\n",
      "Epoch 28/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0241 - val_loss: 0.0100\n",
      "Epoch 29/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0253 - val_loss: 0.0101\n",
      "Epoch 30/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0267 - val_loss: 0.0097\n",
      "Epoch 31/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0265 - val_loss: 0.0097\n",
      "Epoch 32/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0235 - val_loss: 0.0094\n",
      "Epoch 33/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0255 - val_loss: 0.0102\n",
      "Epoch 34/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0249 - val_loss: 0.0096\n",
      "Epoch 35/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0248 - val_loss: 0.0094\n",
      "Epoch 36/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0254 - val_loss: 0.0095\n",
      "Epoch 37/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0268 - val_loss: 0.0096\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 365ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "RMSE: (129.8966453428351, 10.900297164916989)\n",
      "RMSE Random Walk: (48.65213605712465, 5.405311818707543)\n",
      "[7928.35995483 7833.35995483 7661.88015747 7537.83987427 7595.18005371\n",
      " 7527.65991211 7421.95983887 7384.48001099 7267.11999512 7190.11997986\n",
      " 7857.03305054 7797.80025482 7599.19485474 7740.22263336 7605.0532074\n",
      " 7672.62796021 7575.32738495 7622.67333221 7554.63321686 7592.47405243\n",
      " 7513.47631836 7521.03371429 7565.84539795 7506.85370636 7508.81629944\n",
      " 7464.65116119 7421.92337799 7394.69123077 7371.11470032 7348.04193115]\n",
      "[122.51000214 120.31999969 120.86000061 118.12000275 115.06999969\n",
      " 114.76999664 114.69999695 117.69999695 111.30000305 110.33999634\n",
      " 118.30951691 118.61768341 118.31802368 117.91439819 116.93045044\n",
      " 116.89215851 117.94145966 116.20116425 117.37612915 116.59143829\n",
      " 116.93064117 116.22479248 116.9378891  116.87507629 116.89228821\n",
      " 115.75735474 114.92089844 113.90237427 113.46154022 112.94067383]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 112ms/step - loss: 0.3569 - val_loss: 0.0224\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2006 - val_loss: 0.0316\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0838 - val_loss: 0.0369\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0574 - val_loss: 0.0275\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0436 - val_loss: 0.0117\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0348 - val_loss: 0.0098\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0342 - val_loss: 0.0102\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0324 - val_loss: 0.0107\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0308 - val_loss: 0.0137\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0341 - val_loss: 0.0109\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0315 - val_loss: 0.0119\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0290 - val_loss: 0.0102\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0299 - val_loss: 0.0107\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0260 - val_loss: 0.0099\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0287 - val_loss: 0.0103\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0293 - val_loss: 0.0104\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0278 - val_loss: 0.0105\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0270 - val_loss: 0.0107\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0281 - val_loss: 0.0104\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0278 - val_loss: 0.0102\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0270 - val_loss: 0.0110\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0293 - val_loss: 0.0100\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0298 - val_loss: 0.0106\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0270 - val_loss: 0.0106\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0288 - val_loss: 0.0105\n",
      "Epoch 26/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0272 - val_loss: 0.0108\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 350ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "RMSE: (166.9688751857058, 12.297245407104489)\n",
      "RMSE Random Walk: (42.60968568181877, 5.0200393865846555)\n",
      "[8050.86995697 7953.67995453 7782.74015808 7655.95987701 7710.25005341\n",
      " 7642.42990875 7536.65983582 7502.18000793 7378.41999817 7300.4599762\n",
      " 7975.34256744 7916.41793823 7717.51287842 7858.13703156 7721.98365784\n",
      " 7789.52011871 7693.2688446  7738.87449646 7672.00934601 7709.06549072\n",
      " 7630.40695953 7637.25850677 7682.78328705 7623.72878265 7625.70858765\n",
      " 7580.40851593 7536.84427643 7508.59360504 7484.57624054 7460.98260498]\n",
      "[122.51000214 120.31999969 120.86000061 118.12000275 115.06999969\n",
      " 114.76999664 114.69999695 117.69999695 111.30000305 110.33999634\n",
      " 119.54763794 119.76283264 117.93448639 118.88485718 119.39894104\n",
      " 117.86517334 115.07722473 119.4630127  118.0307312  123.14865875\n",
      " 116.6446228  120.82250977 117.68707275 117.40935516 118.26192474\n",
      " 117.31070709 116.41671753 115.46539307 114.64191437 114.10114288]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 213ms/step - loss: 0.3038 - val_loss: 0.0038\n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1896 - val_loss: 0.0257\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0868 - val_loss: 0.0234\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0483 - val_loss: 0.0062\n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0341 - val_loss: 0.0028\n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0205 - val_loss: 0.0065\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0136 - val_loss: 0.0100\n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0155 - val_loss: 0.0092\n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0154 - val_loss: 0.0063\n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0134 - val_loss: 0.0040\n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0118 - val_loss: 0.0030\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0115 - val_loss: 0.0026\n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0117 - val_loss: 0.0028\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0121 - val_loss: 0.0033\n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0107 - val_loss: 0.0038\n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0111 - val_loss: 0.0038\n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0123 - val_loss: 0.0041\n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0109 - val_loss: 0.0039\n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0107 - val_loss: 0.0038\n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0101 - val_loss: 0.0041\n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0106 - val_loss: 0.0041\n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0092 - val_loss: 0.0043\n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0112 - val_loss: 0.0040\n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0110 - val_loss: 0.0045\n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0112 - val_loss: 0.0044\n",
      "Epoch 26/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0098 - val_loss: 0.0048\n",
      "Epoch 27/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0092 - val_loss: 0.0049\n",
      "Epoch 28/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0100 - val_loss: 0.0056\n",
      "Epoch 29/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0100 - val_loss: 0.0056\n",
      "Epoch 30/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0093 - val_loss: 0.0059\n",
      "Epoch 31/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0090 - val_loss: 0.0055\n",
      "Epoch 32/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0096 - val_loss: 0.0056\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 360ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "RMSE: (312.077182529529, 14.682845306396482)\n",
      "RMSE Random Walk: (126.93755175627567, 8.729197142809301)\n",
      "[8173.37995911 8073.99995422 7903.60015869 7774.07987976 7825.3200531\n",
      " 7757.1999054  7651.35983276 7619.88000488 7489.72000122 7410.79997253\n",
      " 8094.89020538 8036.18077087 7835.44736481 7977.02188873 7841.38259888\n",
      " 7907.38529205 7808.34606934 7858.33750916 7790.04007721 7832.21414948\n",
      " 7747.05158234 7758.08101654 7800.4703598  7741.13813782 7743.97051239\n",
      " 7697.71922302 7653.26099396 7624.05899811 7599.21815491 7575.08374786]\n",
      "[244.11000061 242.69999695 249.30000305 249.11000061 245.69000244\n",
      " 241.1499939  245.16999817 241.16000366 226.53999329 224.55000305\n",
      " 219.89408875 221.53979492 223.58650208 215.92634583 218.09761047\n",
      " 226.67326355 220.39671326 206.91998291 236.05163574 210.65153503\n",
      " 228.53765869 223.3289032  211.33592224 210.43373108 219.74125671\n",
      " 220.02037048 218.8387146  217.44378662 215.95744324 214.18414307]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 131ms/step - loss: 0.3049 - val_loss: 0.0021\n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.1828 - val_loss: 0.0043\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0768 - val_loss: 0.0116\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0488 - val_loss: 0.0050\n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0337 - val_loss: 0.0032\n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0266 - val_loss: 0.0028\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0222 - val_loss: 0.0025\n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0159 - val_loss: 0.0032\n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0142 - val_loss: 0.0039\n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0142 - val_loss: 0.0051\n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0125 - val_loss: 0.0054\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0131 - val_loss: 0.0052\n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0141 - val_loss: 0.0050\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0127 - val_loss: 0.0043\n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0130 - val_loss: 0.0040\n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0129 - val_loss: 0.0035\n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0120 - val_loss: 0.0033\n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0118 - val_loss: 0.0031\n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0118 - val_loss: 0.0029\n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0110 - val_loss: 0.0029\n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0118 - val_loss: 0.0028\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 362ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "RMSE: (1230.707004253403, 31.03470458984375)\n",
      "RMSE Random Walk: (127.08568097321765, 9.027036355417245)\n",
      "[8417.48995972 8316.69995117 8152.90016174 8023.18988037 8071.01005554\n",
      " 7998.34989929 7896.52983093 7861.04000854 7716.25999451 7635.34997559\n",
      " 8314.78429413 8257.7205658  8059.03386688 8192.94823456 8059.48020935\n",
      " 8134.0585556  8028.74278259 8065.25749207 8026.09171295 8042.86568451\n",
      " 7975.58924103 7981.40991974 8011.80628204 7951.5718689  7963.7117691\n",
      " 7917.73959351 7872.09970856 7841.50278473 7815.17559814 7789.26789093]\n",
      "[244.11000061 242.69999695 249.30000305 249.11000061 245.69000244\n",
      " 241.1499939  245.16999817 241.16000366 226.53999329 224.55000305\n",
      " 167.03172302 190.36181641 199.2558136  196.0458374  196.80537415\n",
      " 212.15827942 201.87281799 175.83343506 200.42648315 199.2068634\n",
      " 189.76409912 195.26585388 208.9021759  180.6915741  221.56584167\n",
      " 221.31584167 220.85246277 220.04067993 219.09837341 218.19059753]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 150ms/step - loss: 0.4251 - val_loss: 0.0087\n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2721 - val_loss: 0.0201\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1459 - val_loss: 0.0615\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0918 - val_loss: 0.0275\n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0577 - val_loss: 0.0156\n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0371 - val_loss: 0.0130\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0357 - val_loss: 0.0069\n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0297 - val_loss: 0.0057\n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0274 - val_loss: 0.0048\n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0256 - val_loss: 0.0048\n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0264 - val_loss: 0.0058\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0251 - val_loss: 0.0067\n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0234 - val_loss: 0.0081\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0223 - val_loss: 0.0073\n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0241 - val_loss: 0.0083\n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0204 - val_loss: 0.0070\n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0213 - val_loss: 0.0082\n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0205 - val_loss: 0.0078\n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0221 - val_loss: 0.0091\n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0220 - val_loss: 0.0086\n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0210 - val_loss: 0.0097\n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0211 - val_loss: 0.0093\n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0203 - val_loss: 0.0106\n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0162 - val_loss: 0.0102\n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0207 - val_loss: 0.0126\n",
      "Epoch 26/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0217 - val_loss: 0.0110\n",
      "Epoch 27/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0211 - val_loss: 0.0127\n",
      "Epoch 28/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0203 - val_loss: 0.0132\n",
      "Epoch 29/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0183 - val_loss: 0.0131\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 339ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "RMSE: (131.02261801330604, 9.86948013305664)\n",
      "RMSE Random Walk: (205.19684758099157, 11.69833504911432)\n",
      "[8661.59996033 8559.39994812 8402.20016479 8272.29988098 8316.70005798\n",
      " 8239.49989319 8141.6998291  8102.20001221 7942.79998779 7859.89997864\n",
      " 8481.81601715 8448.0823822  8258.28968048 8388.99407196 8256.2855835\n",
      " 8346.21683502 8230.61560059 8241.09092712 8226.51819611 8242.07254791\n",
      " 8165.35334015 8176.67577362 8220.70845795 8132.26344299 8185.27761078\n",
      " 8139.05543518 8092.95217133 8061.54346466 8034.27397156 8007.45848846]\n",
      "[183.08999634 183.16999817 169.27000427 159.1499939  169.58000183\n",
      " 160.72000122 159.92999268 160.19000244 168.80000305 167.11000061\n",
      " 188.38156128 164.66305542 169.51974487 158.14840698 162.50952148\n",
      " 157.41467285 156.60476685 165.83847046 163.92481995 160.86598206\n",
      " 173.37524414 163.94299316 168.20053101 162.85997009 155.8971405\n",
      " 155.80415344 155.13352966 154.47761536 155.81478882 156.33990479]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 140ms/step - loss: 0.3830 - val_loss: 0.0079\n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2110 - val_loss: 0.0110\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0885 - val_loss: 0.0330\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0582 - val_loss: 0.0155\n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0461 - val_loss: 0.0114\n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0410 - val_loss: 0.0131\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0359 - val_loss: 0.0083\n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0303 - val_loss: 0.0056\n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0289 - val_loss: 0.0050\n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0279 - val_loss: 0.0045\n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0251 - val_loss: 0.0044\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0251 - val_loss: 0.0045\n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0271 - val_loss: 0.0047\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0225 - val_loss: 0.0049\n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0263 - val_loss: 0.0055\n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0245 - val_loss: 0.0060\n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0222 - val_loss: 0.0060\n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0262 - val_loss: 0.0071\n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0206 - val_loss: 0.0067\n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0265 - val_loss: 0.0076\n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0250 - val_loss: 0.0079\n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0244 - val_loss: 0.0073\n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0227 - val_loss: 0.0095\n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0284 - val_loss: 0.0074\n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0241 - val_loss: 0.0095\n",
      "Epoch 26/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0238 - val_loss: 0.0078\n",
      "Epoch 27/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0190 - val_loss: 0.0087\n",
      "Epoch 28/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0204 - val_loss: 0.0079\n",
      "Epoch 29/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0224 - val_loss: 0.0088\n",
      "Epoch 30/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0192 - val_loss: 0.0079\n",
      "Epoch 31/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0217 - val_loss: 0.0087\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 400ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "RMSE: (158.43327935652343, 10.175259399414063)\n",
      "RMSE Random Walk: (221.37513792779583, 12.283475534965806)\n",
      "[8844.68995667 8742.56994629 8571.47016907 8431.44987488 8486.28005981\n",
      " 8400.21989441 8301.62982178 8262.39001465 8111.59999084 8027.00997925\n",
      " 8670.19757843 8612.74543762 8427.80942535 8547.14247894 8418.79510498\n",
      " 8503.63150787 8387.22036743 8406.92939758 8390.44301605 8402.93852997\n",
      " 8338.72858429 8340.61876678 8388.90898895 8295.12341309 8341.17475128\n",
      " 8294.85958862 8248.08570099 8216.02108002 8190.08876038 8163.79839325]\n",
      "[183.08999634 183.16999817 169.27000427 159.1499939  169.58000183\n",
      " 160.72000122 159.92999268 160.19000244 168.80000305 167.11000061\n",
      " 176.2557373  178.56944275 166.73957825 165.54664612 165.01255798\n",
      " 162.58518982 166.36225891 168.06848145 166.49383545 163.58905029\n",
      " 167.03291321 167.87988281 163.87051392 165.3483429  164.7409668\n",
      " 163.19012451 161.84425354 161.77867126 162.59939575 163.38186646]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 116ms/step - loss: 0.3244 - val_loss: 0.0473\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1568 - val_loss: 0.0281\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0898 - val_loss: 0.0178\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0558 - val_loss: 0.0152\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0398 - val_loss: 0.0241\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0383 - val_loss: 0.0303\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0337 - val_loss: 0.0383\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0289 - val_loss: 0.0414\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0269 - val_loss: 0.0392\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0284 - val_loss: 0.0392\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0291 - val_loss: 0.0383\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0273 - val_loss: 0.0387\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0268 - val_loss: 0.0394\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0256 - val_loss: 0.0363\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0280 - val_loss: 0.0387\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0288 - val_loss: 0.0376\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0265 - val_loss: 0.0382\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0253 - val_loss: 0.0406\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0247 - val_loss: 0.0382\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0262 - val_loss: 0.0422\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0265 - val_loss: 0.0397\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0269 - val_loss: 0.0443\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0282 - val_loss: 0.0357\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0252 - val_loss: 0.0441\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "RMSE: (298.813410295808, 16.57389602661133)\n",
      "RMSE Random Walk: (52.77670865312359, 5.70514071946055)\n",
      "[9027.779953   8925.73994446 8740.74017334 8590.59986877 8655.86006165\n",
      " 8560.93989563 8461.55981445 8422.58001709 8280.3999939  8194.11997986\n",
      " 8846.45331573 8791.31488037 8594.5490036  8712.68912506 8583.80766296\n",
      " 8666.21669769 8553.58262634 8574.99787903 8556.9368515  8566.52758026\n",
      " 8505.7614975  8508.4986496  8552.77950287 8460.47175598 8505.91571808\n",
      " 8458.04971313 8409.92995453 8377.79975128 8352.68815613 8327.1802597 ]\n",
      "[142.30000305 138.22999573 133.22000122 133.61999512 133.80000305\n",
      " 137.27999878 130.75       129.78999329 128.72999573 126.76999664\n",
      " 143.92559814 146.91065979 140.94515991 139.90090942 143.30490112\n",
      " 143.96459961 141.2303009  138.16172791 150.63760376 137.1315155\n",
      " 144.46813965 148.63580322 145.2618103  136.59362793 136.08149719\n",
      " 134.7351532  134.02922058 133.79995728 133.31269836 132.9370575 ]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 116ms/step - loss: 0.3452 - val_loss: 0.0649\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2290 - val_loss: 0.0423\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1419 - val_loss: 0.0330\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0926 - val_loss: 0.0260\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0551 - val_loss: 0.0294\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0356 - val_loss: 0.0352\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0365 - val_loss: 0.0384\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0350 - val_loss: 0.0448\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0263 - val_loss: 0.0445\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0311 - val_loss: 0.0438\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0266 - val_loss: 0.0442\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0262 - val_loss: 0.0395\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0277 - val_loss: 0.0399\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0272 - val_loss: 0.0388\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0260 - val_loss: 0.0414\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0288 - val_loss: 0.0369\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0266 - val_loss: 0.0420\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0284 - val_loss: 0.0355\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0278 - val_loss: 0.0405\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0260 - val_loss: 0.0376\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0285 - val_loss: 0.0395\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0258 - val_loss: 0.0398\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0277 - val_loss: 0.0380\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0270 - val_loss: 0.0403\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 373ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "RMSE: (349.54161984015076, 16.570995712280272)\n",
      "RMSE Random Walk: (49.56416924049539, 5.492204575778431)\n",
      "[9170.07995605 9063.96994019 8873.96017456 8724.21986389 8789.6600647\n",
      " 8698.21989441 8592.30981445 8552.37001038 8409.12998962 8320.8899765\n",
      " 8990.37891388 8938.22554016 8735.49416351 8852.59003448 8727.11256409\n",
      " 8810.1812973  8694.81292725 8713.15960693 8707.57445526 8703.65909576\n",
      " 8650.22963715 8657.13445282 8698.04131317 8597.06538391 8641.99721527\n",
      " 8592.78486633 8543.95917511 8511.59970856 8486.00085449 8460.1173172 ]\n",
      "[142.30000305 138.22999573 133.22000122 133.61999512 133.80000305\n",
      " 137.27999878 130.75       129.78999329 128.72999573 126.76999664\n",
      " 139.52435303 139.51655579 136.83529663 131.11903381 146.33786011\n",
      " 136.69792175 147.72343445 160.19137573 141.45349121 111.19664001\n",
      " 133.93637085 145.09780884 131.90266418 126.61530304 144.05607605\n",
      " 142.57411194 141.30583191 140.6285553  139.91136169 139.11917114]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 116ms/step - loss: 0.3633 - val_loss: 0.1589\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1725 - val_loss: 0.0437\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0838 - val_loss: 0.0340\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0518 - val_loss: 0.0194\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0446 - val_loss: 0.0288\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0400 - val_loss: 0.0402\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0361 - val_loss: 0.0465\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0354 - val_loss: 0.0545\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0344 - val_loss: 0.0482\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0348 - val_loss: 0.0596\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0338 - val_loss: 0.0420\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0314 - val_loss: 0.0571\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0319 - val_loss: 0.0485\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0310 - val_loss: 0.0518\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0306 - val_loss: 0.0562\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0316 - val_loss: 0.0545\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0301 - val_loss: 0.0553\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0297 - val_loss: 0.0673\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0290 - val_loss: 0.0533\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0323 - val_loss: 0.0562\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0298 - val_loss: 0.0650\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0292 - val_loss: 0.0519\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0293 - val_loss: 0.0687\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0286 - val_loss: 0.0659\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 417ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "RMSE: (205.49979601720116, 12.534117889404296)\n",
      "RMSE Random Walk: (25.32755390446102, 4.03896589668039)\n",
      "[9312.37995911 9202.19993591 9007.18017578 8857.83985901 8923.46006775\n",
      " 8835.49989319 8723.05981445 8682.16000366 8537.85998535 8447.65997314\n",
      " 9129.90326691 9077.74209595 8872.32946014 8983.7090683  8873.45042419\n",
      " 8946.87921906 8842.53636169 8873.35098267 8849.02794647 8814.85573578\n",
      " 8784.166008   8802.23226166 8829.94397736 8723.68068695 8786.05329132\n",
      " 8735.35897827 8685.26500702 8652.22826385 8625.91221619 8599.23648834]\n",
      "[174.1499939  171.52000427 167.57000732 167.22999573 167.52999878\n",
      " 170.02999878 163.61999512 161.38000488 158.91000366 157.22000122\n",
      " 164.44178772 163.92379761 164.93141174 160.82945251 160.80537415\n",
      " 173.14398193 166.36486816 163.20985413 163.55877686 155.99641418\n",
      " 164.30683899 161.50048828 165.66810608 168.72839355 172.64804077\n",
      " 172.09533691 171.49629211 171.08865356 170.71069336 170.09376526]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 151ms/step - loss: 0.3431 - val_loss: 0.1712\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.2043 - val_loss: 0.0792\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.1139 - val_loss: 0.0588\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0660 - val_loss: 0.0309\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0451 - val_loss: 0.0270\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0412 - val_loss: 0.0397\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0413 - val_loss: 0.0373\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0350 - val_loss: 0.0442\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0356 - val_loss: 0.0513\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0389 - val_loss: 0.0418\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0363 - val_loss: 0.0589\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0417 - val_loss: 0.0365\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0353 - val_loss: 0.0521\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0354 - val_loss: 0.0375\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0368 - val_loss: 0.0442\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0364 - val_loss: 0.0406\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0324 - val_loss: 0.0432\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0338 - val_loss: 0.0419\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0363 - val_loss: 0.0415\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0360 - val_loss: 0.0415\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0339 - val_loss: 0.0437\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0358 - val_loss: 0.0414\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0341 - val_loss: 0.0388\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0378 - val_loss: 0.0417\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0331 - val_loss: 0.0369\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 357ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "RMSE: (161.5182339091203, 12.174605560302734)\n",
      "RMSE Random Walk: (31.33506393745798, 4.52417635185748)\n",
      "[9486.529953   9373.71994019 9174.75018311 9025.06985474 9090.99006653\n",
      " 9005.52989197 8886.67980957 8843.54000854 8696.76998901 8604.87997437\n",
      " 9294.34505463 9241.66589355 9037.26087189 9144.53852081 9034.25579834\n",
      " 9120.02320099 9008.90122986 9036.56083679 9012.58672333 8970.85214996\n",
      " 8948.47284698 8963.73274994 8995.61208344 8892.40908051 8958.70133209\n",
      " 8907.45431519 8856.76129913 8823.31691742 8796.62290955 8769.3302536 ]\n",
      "[174.1499939  171.52000427 167.57000732 167.22999573 167.52999878\n",
      " 170.02999878 163.61999512 161.38000488 158.91000366 157.22000122\n",
      " 165.62120056 165.52279663 167.56958008 163.34111023 165.08752441\n",
      " 171.32226562 169.24026489 164.68136597 168.37690735 162.81692505\n",
      " 161.30586243 165.2255249  167.95558167 163.92985535 167.25445557\n",
      " 166.74533081 166.17860413 165.77758789 165.42855835 164.97077942]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 108ms/step - loss: 0.3754 - val_loss: 0.0201\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2010 - val_loss: 0.0517\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0964 - val_loss: 0.0258\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0639 - val_loss: 0.0285\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0465 - val_loss: 0.0170\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0379 - val_loss: 0.0108\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0323 - val_loss: 0.0092\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0276 - val_loss: 0.0100\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0305 - val_loss: 0.0147\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0381 - val_loss: 0.0109\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0297 - val_loss: 0.0097\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0285 - val_loss: 0.0101\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0274 - val_loss: 0.0099\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0292 - val_loss: 0.0108\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0268 - val_loss: 0.0102\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0276 - val_loss: 0.0118\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0268 - val_loss: 0.0102\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0276 - val_loss: 0.0114\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0250 - val_loss: 0.0104\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0275 - val_loss: 0.0113\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0266 - val_loss: 0.0099\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0264 - val_loss: 0.0102\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0249 - val_loss: 0.0099\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0272 - val_loss: 0.0097\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0257 - val_loss: 0.0098\n",
      "Epoch 26/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0265 - val_loss: 0.0100\n",
      "Epoch 27/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0272 - val_loss: 0.0100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 411ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "RMSE: (161.0857157880935, 11.856212234497068)\n",
      "RMSE Random Walk: (40.00184363799998, 5.081564061869344)\n",
      "[9660.6799469  9545.23994446 9342.32019043 9192.29985046 9258.52006531\n",
      " 9175.55989075 9050.29980469 9004.92001343 8855.67999268 8762.09997559\n",
      " 9459.96625519 9407.18869019 9204.83045197 9307.87963104 9199.34332275\n",
      " 9291.34546661 9178.14149475 9201.24220276 9180.96363068 9133.66907501\n",
      " 9109.77870941 9128.95827484 9163.5676651  9056.33893585 9125.95578766\n",
      " 9074.199646   9022.93990326 8989.09450531 8962.0514679  8934.30103302]\n",
      "[122.51000214 120.31999969 120.86000061 118.12000275 115.06999969\n",
      " 114.76999664 114.69999695 117.69999695 111.30000305 110.33999634\n",
      " 115.58805084 116.43664551 117.57062531 119.09333801 116.39118195\n",
      " 117.56621552 117.74412537 116.48596191 118.13147736 119.51644135\n",
      " 117.30532837 118.70301819 121.3915329  116.94695282 119.33049774\n",
      " 118.19744873 117.10877991 115.93260956 115.11460876 114.49941254]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 112ms/step - loss: 0.3963 - val_loss: 0.0250\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2304 - val_loss: 0.0260\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1118 - val_loss: 0.0259\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0654 - val_loss: 0.0156\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0451 - val_loss: 0.0142\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0382 - val_loss: 0.0098\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0331 - val_loss: 0.0093\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0319 - val_loss: 0.0098\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0299 - val_loss: 0.0111\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0304 - val_loss: 0.0116\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0294 - val_loss: 0.0112\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0267 - val_loss: 0.0104\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0270 - val_loss: 0.0103\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0287 - val_loss: 0.0094\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0269 - val_loss: 0.0098\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0269 - val_loss: 0.0096\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0279 - val_loss: 0.0096\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0267 - val_loss: 0.0099\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0276 - val_loss: 0.0092\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0287 - val_loss: 0.0108\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0324 - val_loss: 0.0090\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0312 - val_loss: 0.0092\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0271 - val_loss: 0.0090\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0278 - val_loss: 0.0101\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0274 - val_loss: 0.0101\n",
      "Epoch 26/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0254 - val_loss: 0.0098\n",
      "Epoch 27/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0287 - val_loss: 0.0100\n",
      "Epoch 28/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0265 - val_loss: 0.0101\n",
      "Epoch 29/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0264 - val_loss: 0.0111\n",
      "Epoch 30/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0288 - val_loss: 0.0098\n",
      "Epoch 31/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0275 - val_loss: 0.0098\n",
      "Epoch 32/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0252 - val_loss: 0.0094\n",
      "Epoch 33/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0268 - val_loss: 0.0094\n",
      "Epoch 34/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0255 - val_loss: 0.0095\n",
      "Epoch 35/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0236 - val_loss: 0.0106\n",
      "Epoch 36/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0280 - val_loss: 0.0100\n",
      "Epoch 37/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0274 - val_loss: 0.0105\n",
      "Epoch 38/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0244 - val_loss: 0.0115\n",
      "Epoch 39/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0308 - val_loss: 0.0096\n",
      "Epoch 40/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0278 - val_loss: 0.0110\n",
      "Epoch 41/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0261 - val_loss: 0.0092\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 357ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "RMSE: (129.94315585080474, 10.93867607116699)\n",
      "RMSE Random Walk: (44.34036026352983, 5.286999727549413)\n",
      "[9783.18994904 9665.55994415 9463.18019104 9310.41985321 9373.590065\n",
      " 9290.32988739 9164.99980164 9122.62001038 8966.97999573 8872.43997192\n",
      " 9575.55430603 9523.62533569 9322.40107727 9426.97296906 9315.7345047\n",
      " 9408.91168213 9295.88562012 9317.72816467 9299.09510803 9253.18551636\n",
      " 9227.08403778 9247.66129303 9284.959198   9173.28588867 9245.2862854\n",
      " 9192.39709473 9140.04868317 9105.02711487 9077.16607666 9048.80044556]\n",
      "[122.51000214 120.31999969 120.86000061 118.12000275 115.06999969\n",
      " 114.76999664 114.69999695 117.69999695 111.30000305 110.33999634\n",
      " 118.17644501 117.65922546 118.35636139 117.94935608 117.20797729\n",
      " 118.18406677 117.35806274 116.68580627 117.32899475 116.97624969\n",
      " 117.61485291 116.81021118 117.64762115 117.14457703 116.50748444\n",
      " 115.52511597 114.62210846 113.69507599 112.91454315 112.33939362]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 148ms/step - loss: 0.3059 - val_loss: 0.0042\n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2388 - val_loss: 0.0169\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1234 - val_loss: 0.0261\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0718 - val_loss: 0.0089\n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0403 - val_loss: 0.0038\n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0248 - val_loss: 0.0040\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0180 - val_loss: 0.0091\n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0160 - val_loss: 0.0107\n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0157 - val_loss: 0.0082\n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0124 - val_loss: 0.0053\n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0122 - val_loss: 0.0039\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0117 - val_loss: 0.0033\n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0120 - val_loss: 0.0032\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0114 - val_loss: 0.0034\n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0105 - val_loss: 0.0038\n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0105 - val_loss: 0.0040\n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0113 - val_loss: 0.0042\n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0128 - val_loss: 0.0038\n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0103 - val_loss: 0.0040\n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0105 - val_loss: 0.0040\n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0105 - val_loss: 0.0043\n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0103 - val_loss: 0.0041\n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0107 - val_loss: 0.0046\n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0101 - val_loss: 0.0045\n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0112 - val_loss: 0.0049\n",
      "Epoch 26/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0101 - val_loss: 0.0047\n",
      "Epoch 27/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0093 - val_loss: 0.0049\n",
      "Epoch 28/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0094 - val_loss: 0.0049\n",
      "Epoch 29/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0092 - val_loss: 0.0051\n",
      "Epoch 30/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0093 - val_loss: 0.0051\n",
      "Epoch 31/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0091 - val_loss: 0.0051\n",
      "Epoch 32/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0100 - val_loss: 0.0056\n",
      "Epoch 33/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0105 - val_loss: 0.0053\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 379ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "RMSE: (813.479997564398, 23.990398406982422)\n",
      "RMSE Random Walk: (138.96261872338644, 9.52941450696154)\n",
      "[9905.69995117 9785.87994385 9584.04019165 9428.53985596 9488.6600647\n",
      " 9405.09988403 9279.69979858 9240.32000732 9078.27999878 8982.77996826\n",
      " 9693.73075104 9641.28456116 9440.75743866 9544.92232513 9432.94248199\n",
      " 9527.0957489  9413.24368286 9434.41397095 9416.42410278 9370.16176605\n",
      " 9344.69889069 9364.47150421 9402.60681915 9290.4304657  9361.79376984\n",
      " 9307.92221069 9254.67079163 9218.72219086 9190.08061981 9161.13983917]\n",
      "[244.11000061 242.69999695 249.30000305 249.11000061 245.69000244\n",
      " 241.1499939  245.16999817 241.16000366 226.53999329 224.55000305\n",
      " 219.42906189 235.13909912 217.39730835 212.33648682 221.09382629\n",
      " 209.67564392 209.1375885  224.36627197 213.10922241 201.99693298\n",
      " 199.04327393 209.02380371 196.1089325  186.59936523 206.12905884\n",
      " 204.55404663 204.49763489 204.04139709 203.4881134  200.72320557]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 133ms/step - loss: 0.3124 - val_loss: 0.0029\n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.2331 - val_loss: 0.0049\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1264 - val_loss: 0.0064\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0915 - val_loss: 0.0030\n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0517 - val_loss: 0.0028\n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0350 - val_loss: 0.0022\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0208 - val_loss: 0.0024\n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0152 - val_loss: 0.0030\n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0143 - val_loss: 0.0039\n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0136 - val_loss: 0.0042\n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0132 - val_loss: 0.0040\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0121 - val_loss: 0.0036\n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0112 - val_loss: 0.0032\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0119 - val_loss: 0.0028\n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0111 - val_loss: 0.0026\n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0124 - val_loss: 0.0026\n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0115 - val_loss: 0.0025\n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0109 - val_loss: 0.0026\n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0121 - val_loss: 0.0026\n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0106 - val_loss: 0.0027\n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0110 - val_loss: 0.0027\n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0113 - val_loss: 0.0028\n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0121 - val_loss: 0.0027\n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0114 - val_loss: 0.0027\n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0114 - val_loss: 0.0027\n",
      "Epoch 26/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0112 - val_loss: 0.0027\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "RMSE: (314.20104816212546, 14.377484893798828)\n",
      "RMSE Random Walk: (118.90940724531875, 8.528608291995933)\n",
      "[10149.80995178 10028.5799408   9833.3401947   9677.64985657\n",
      "  9734.35006714  9646.24987793  9524.86979675  9481.48001099\n",
      "  9304.81999207  9207.32997131  9913.15981293  9876.42366028\n",
      "  9658.15474701  9757.25881195  9654.03630829  9736.77139282\n",
      "  9622.38127136  9658.78024292  9629.5333252   9572.15869904\n",
      "  9543.74216461  9573.49530792  9598.71575165  9477.02983093\n",
      "  9567.92282867  9512.47625732  9459.16842651  9422.76358795\n",
      "  9393.56873322  9361.86304474]\n",
      "[244.11000061 242.69999695 249.30000305 249.11000061 245.69000244\n",
      " 241.1499939  245.16999817 241.16000366 226.53999329 224.55000305\n",
      " 251.78573608 254.06724548 254.10443115 249.18939209 241.97521973\n",
      " 249.920578   270.16778564 232.17596436 239.25405884 236.6862793\n",
      " 244.06872559 253.6441803  254.290802   229.45541382 239.48040771\n",
      " 238.76054382 237.44876099 235.81678772 233.85856628 232.11320496]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 156ms/step - loss: 0.3878 - val_loss: 0.0066\n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2458 - val_loss: 0.0426\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1228 - val_loss: 0.0701\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0777 - val_loss: 0.0356\n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0603 - val_loss: 0.0272\n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0410 - val_loss: 0.0150\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0334 - val_loss: 0.0061\n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0271 - val_loss: 0.0057\n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0274 - val_loss: 0.0053\n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0256 - val_loss: 0.0054\n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0280 - val_loss: 0.0068\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0267 - val_loss: 0.0095\n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0226 - val_loss: 0.0082\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0228 - val_loss: 0.0099\n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0214 - val_loss: 0.0084\n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0209 - val_loss: 0.0079\n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0220 - val_loss: 0.0073\n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0199 - val_loss: 0.0073\n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0250 - val_loss: 0.0084\n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0234 - val_loss: 0.0088\n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0246 - val_loss: 0.0101\n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0216 - val_loss: 0.0117\n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0219 - val_loss: 0.0099\n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0217 - val_loss: 0.0108\n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0205 - val_loss: 0.0093\n",
      "Epoch 26/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0197 - val_loss: 0.0096\n",
      "Epoch 27/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0214 - val_loss: 0.0102\n",
      "Epoch 28/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0187 - val_loss: 0.0103\n",
      "Epoch 29/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0189 - val_loss: 0.0106\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 341ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "RMSE: (238.37818333550823, 12.861787414550781)\n",
      "RMSE Random Walk: (208.81767146926057, 11.808688969288209)\n",
      "[10393.91995239 10271.27993774 10082.64019775  9926.75985718\n",
      "  9980.04006958  9887.39987183  9770.03979492  9722.64001465\n",
      "  9531.35998535  9431.87997437 10164.94554901 10130.49090576\n",
      "  9912.25917816 10006.44820404  9896.01152802  9986.69197083\n",
      "  9892.54905701  9890.95620728  9868.78738403  9808.84497833\n",
      "  9787.8108902   9827.13948822  9853.00655365  9706.48524475\n",
      "  9807.40323639  9751.23680115  9696.6171875   9658.58037567\n",
      "  9627.4272995   9593.97624969]\n",
      "[183.08999634 183.16999817 169.27000427 159.1499939  169.58000183\n",
      " 160.72000122 159.92999268 160.19000244 168.80000305 167.11000061\n",
      " 176.71481323 179.42553711 164.73464966 163.27323914 160.11103821\n",
      " 157.95988464 163.8971405  151.92195129 150.04405212 155.64202881\n",
      " 157.9072113  159.57919312 164.8768158  177.35220337 167.93139648\n",
      " 166.22167969 164.52859497 164.47105408 164.13989258 164.17985535]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - loss: 0.3770 - val_loss: 0.0099\n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.2761 - val_loss: 0.0137\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1630 - val_loss: 0.0335\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0978 - val_loss: 0.0191\n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0640 - val_loss: 0.0134\n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0517 - val_loss: 0.0130\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0381 - val_loss: 0.0076\n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0298 - val_loss: 0.0052\n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0297 - val_loss: 0.0046\n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0285 - val_loss: 0.0045\n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0276 - val_loss: 0.0046\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0285 - val_loss: 0.0045\n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0252 - val_loss: 0.0050\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0264 - val_loss: 0.0052\n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0248 - val_loss: 0.0061\n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0231 - val_loss: 0.0060\n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0219 - val_loss: 0.0066\n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0215 - val_loss: 0.0067\n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0239 - val_loss: 0.0072\n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0200 - val_loss: 0.0072\n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0221 - val_loss: 0.0076\n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0233 - val_loss: 0.0081\n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0208 - val_loss: 0.0075\n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0209 - val_loss: 0.0094\n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0218 - val_loss: 0.0085\n",
      "Epoch 26/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0202 - val_loss: 0.0089\n",
      "Epoch 27/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0188 - val_loss: 0.0099\n",
      "Epoch 28/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0191 - val_loss: 0.0101\n",
      "Epoch 29/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0200 - val_loss: 0.0094\n",
      "Epoch 30/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0192 - val_loss: 0.0114\n",
      "Epoch 31/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0232 - val_loss: 0.0093\n",
      "Epoch 32/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0200 - val_loss: 0.0108\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 397ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "RMSE: (173.70112898022634, 10.60550765991211)\n",
      "RMSE Random Walk: (228.74954359093826, 12.148674449898973)\n",
      "[10577.00994873 10454.44993591 10251.91020203 10085.90985107\n",
      " 10149.62007141 10048.11987305  9929.9697876   9882.83001709\n",
      "  9700.1599884   9598.98997498 10341.66036224 10309.91644287\n",
      " 10076.99382782 10169.72144318 10056.12256622 10144.65185547\n",
      " 10056.44619751 10042.87815857 10018.83143616  9964.48700714\n",
      "  9945.7181015   9986.71868134 10017.88336945  9883.83744812\n",
      "  9975.33463287  9917.45848083  9861.14578247  9823.05142975\n",
      "  9791.56719208  9758.15610504]\n",
      "[183.08999634 183.16999817 169.27000427 159.1499939  169.58000183\n",
      " 160.72000122 159.92999268 160.19000244 168.80000305 167.11000061\n",
      " 180.07321167 176.62574768 172.70892334 169.09991455 166.85501099\n",
      " 165.19772339 173.31069946 168.75976562 166.10852051 164.90258789\n",
      " 165.93785095 165.34384155 168.24591064 169.02641296 165.27052307\n",
      " 163.71734619 162.29994202 162.16264343 162.82096863 163.41131592]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 127ms/step - loss: 0.3756 - val_loss: 0.0654\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2536 - val_loss: 0.0381\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1550 - val_loss: 0.0246\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0979 - val_loss: 0.0206\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0590 - val_loss: 0.0201\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0403 - val_loss: 0.0292\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0312 - val_loss: 0.0428\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0348 - val_loss: 0.0503\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0298 - val_loss: 0.0450\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0331 - val_loss: 0.0396\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0316 - val_loss: 0.0347\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0280 - val_loss: 0.0342\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0286 - val_loss: 0.0331\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0262 - val_loss: 0.0371\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0281 - val_loss: 0.0333\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0316 - val_loss: 0.0386\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0283 - val_loss: 0.0354\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0281 - val_loss: 0.0379\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0291 - val_loss: 0.0356\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0268 - val_loss: 0.0405\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0282 - val_loss: 0.0350\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0254 - val_loss: 0.0404\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0292 - val_loss: 0.0362\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0256 - val_loss: 0.0378\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0255 - val_loss: 0.0353\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 368ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "RMSE: (408.1556690445344, 18.951139068603517)\n",
      "RMSE Random Walk: (48.07163273102486, 5.406416741397563)\n",
      "[10760.09994507 10637.61993408 10421.1802063  10245.05984497\n",
      " 10319.20007324 10208.83987427 10089.89978027 10043.02001953\n",
      "  9868.95999146  9766.09997559 10521.73357391 10486.54219055\n",
      " 10249.70275116 10338.82135773 10222.97757721 10309.84957886\n",
      " 10229.75689697 10211.63792419 10184.93995667 10129.38959503\n",
      " 10111.65595245 10152.06252289 10186.12928009 10052.86386108\n",
      " 10140.60515594 10081.17582703 10023.44572449  9985.21407318\n",
      "  9954.38816071  9921.56742096]\n",
      "[142.30000305 138.22999573 133.22000122 133.61999512 133.80000305\n",
      " 137.27999878 130.75       129.78999329 128.72999573 126.76999664\n",
      " 142.43103027 147.74432373 149.2694397  142.20625305 143.14463806\n",
      " 143.68667603 151.37008667 139.79858398 146.68247986 138.24415588\n",
      " 154.64904785 132.5401001  133.30482483 136.10864258 144.61723328\n",
      " 143.03514099 142.04017639 141.56144714 140.83859253 140.2399292 ]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 121ms/step - loss: 0.3798 - val_loss: 0.0678\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2312 - val_loss: 0.0315\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0856 - val_loss: 0.0225\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0559 - val_loss: 0.0245\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0493 - val_loss: 0.0187\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0377 - val_loss: 0.0192\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0373 - val_loss: 0.0270\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0298 - val_loss: 0.0299\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0304 - val_loss: 0.0341\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0287 - val_loss: 0.0396\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0297 - val_loss: 0.0408\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0277 - val_loss: 0.0442\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0270 - val_loss: 0.0423\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0276 - val_loss: 0.0433\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0268 - val_loss: 0.0422\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0275 - val_loss: 0.0405\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0270 - val_loss: 0.0430\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0260 - val_loss: 0.0398\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0267 - val_loss: 0.0426\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0298 - val_loss: 0.0423\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0256 - val_loss: 0.0386\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0295 - val_loss: 0.0420\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0272 - val_loss: 0.0383\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0265 - val_loss: 0.0410\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0267 - val_loss: 0.0393\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "RMSE: (256.1269107906672, 14.650473022460938)\n",
      "RMSE Random Walk: (48.49548395656891, 5.524564010426112)\n",
      "[10902.39994812 10775.84992981 10554.40020752 10378.67984009\n",
      " 10453.00007629 10346.11987305 10220.64978027 10172.81001282\n",
      "  9997.68998718  9892.86997223 10664.16460419 10634.28651428\n",
      " 10398.97219086 10481.02761078 10366.12221527 10453.53625488\n",
      " 10381.12698364 10351.43650818 10331.62243652 10267.63375092\n",
      " 10266.30500031 10284.60262299 10319.43410492 10188.97250366\n",
      " 10285.22238922 10224.21096802 10165.48590088 10126.77552032\n",
      " 10095.22675323 10061.80735016]\n",
      "[142.30000305 138.22999573 133.22000122 133.61999512 133.80000305\n",
      " 137.27999878 130.75       129.78999329 128.72999573 126.76999664\n",
      " 137.46330261 143.04273987 139.97245789 141.3691864  135.13273621\n",
      " 134.63618469 141.05006409 138.96569824 139.20736694 137.2000885\n",
      " 138.51976013 140.97315979 143.96578979 131.16043091 139.68821716\n",
      " 138.52490234 137.50384521 136.92527771 136.39195251 135.80632019]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 126ms/step - loss: 0.3251 - val_loss: 0.1352\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1767 - val_loss: 0.0424\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0946 - val_loss: 0.0258\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0608 - val_loss: 0.0273\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0468 - val_loss: 0.0241\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0417 - val_loss: 0.0441\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0367 - val_loss: 0.0460\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0346 - val_loss: 0.0674\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0332 - val_loss: 0.0518\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0350 - val_loss: 0.0628\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0356 - val_loss: 0.0542\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0323 - val_loss: 0.0381\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0334 - val_loss: 0.0624\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0306 - val_loss: 0.0462\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0296 - val_loss: 0.0727\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0290 - val_loss: 0.0567\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0295 - val_loss: 0.0691\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0293 - val_loss: 0.0664\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0268 - val_loss: 0.0698\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0290 - val_loss: 0.0615\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0285 - val_loss: 0.0904\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0294 - val_loss: 0.0618\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0298 - val_loss: 0.0794\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0267 - val_loss: 0.0675\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0265 - val_loss: 0.0771\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 355ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "RMSE: (126.92216030465207, 10.620293426513673)\n",
      "RMSE Random Walk: (26.752672338452335, 4.0957377999165825)\n",
      "[11044.69995117 10914.07992554 10687.62020874 10512.29983521\n",
      " 10586.80007935 10483.39987183 10351.39978027 10302.6000061\n",
      " 10126.41998291 10019.63996887 10801.6279068  10777.32925415\n",
      " 10538.94464874 10622.39679718 10501.25495148 10588.17243958\n",
      " 10522.17704773 10490.40220642 10470.82980347 10404.83383942\n",
      " 10404.82476044 10425.57578278 10463.39989471 10320.13293457\n",
      " 10424.91060638 10362.73587036 10302.98974609 10263.70079803\n",
      " 10231.61870575 10197.61367035]\n",
      "[174.1499939  171.52000427 167.57000732 167.22999573 167.52999878\n",
      " 170.02999878 163.61999512 161.38000488 158.91000366 157.22000122\n",
      " 166.74255371 165.37927246 163.43025208 167.96185303 166.63795471\n",
      " 160.27832031 166.82844543 165.42153931 167.7129364  167.88244629\n",
      " 167.14860535 165.00250244 162.81228638 169.97889709 162.15653992\n",
      " 161.54077148 160.92526245 160.38787842 159.85371399 159.18380737]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 125ms/step - loss: 0.3617 - val_loss: 0.2152\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2473 - val_loss: 0.1110\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1260 - val_loss: 0.0491\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0538 - val_loss: 0.0374\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0461 - val_loss: 0.0182\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0421 - val_loss: 0.0361\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0372 - val_loss: 0.0358\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0386 - val_loss: 0.0393\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0357 - val_loss: 0.0435\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0357 - val_loss: 0.0371\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0363 - val_loss: 0.0450\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0351 - val_loss: 0.0403\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0360 - val_loss: 0.0411\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0314 - val_loss: 0.0439\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0323 - val_loss: 0.0407\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0336 - val_loss: 0.0436\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0323 - val_loss: 0.0369\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0347 - val_loss: 0.0454\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0343 - val_loss: 0.0418\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0330 - val_loss: 0.0428\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0337 - val_loss: 0.0417\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0337 - val_loss: 0.0417\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0316 - val_loss: 0.0418\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0315 - val_loss: 0.0382\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0334 - val_loss: 0.0396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 359ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "RMSE: (260.6861261533224, 14.915814971923828)\n",
      "RMSE Random Walk: (30.974602481740476, 4.563804759166506)\n",
      "[11218.84994507 11085.59992981 10855.19021606 10679.52983093\n",
      " 10754.33007812 10653.42987061 10515.01977539 10463.98001099\n",
      " 10285.32998657 10176.85997009 10968.37046051 10942.70852661\n",
      " 10702.37490082 10790.35865021 10667.89290619 10748.45075989\n",
      " 10689.00549316 10655.82374573 10638.54273987 10572.71628571\n",
      " 10571.97336578 10590.57828522 10626.21218109 10490.11183167\n",
      " 10587.0671463  10524.27664185 10463.91500854 10424.08867645\n",
      " 10391.47241974 10356.79747772]\n",
      "[174.1499939  171.52000427 167.57000732 167.22999573 167.52999878\n",
      " 170.02999878 163.61999512 161.38000488 158.91000366 157.22000122\n",
      " 161.31929016 170.49443054 158.70326233 172.16149902 169.70088196\n",
      " 168.71426392 169.65087891 172.06692505 164.19812012 167.76246643\n",
      " 174.28387451 169.83396912 165.23426819 166.43650818 171.52934265\n",
      " 171.0606842  170.55223083 170.20729065 169.8712616  169.39482117]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 114ms/step - loss: 0.4130 - val_loss: 0.0253\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2613 - val_loss: 0.0371\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1161 - val_loss: 0.0547\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0615 - val_loss: 0.0299\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0505 - val_loss: 0.0146\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0370 - val_loss: 0.0111\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0341 - val_loss: 0.0144\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0300 - val_loss: 0.0136\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0272 - val_loss: 0.0130\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0289 - val_loss: 0.0102\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0278 - val_loss: 0.0107\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0302 - val_loss: 0.0092\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0257 - val_loss: 0.0097\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0284 - val_loss: 0.0091\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0273 - val_loss: 0.0106\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0314 - val_loss: 0.0097\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0287 - val_loss: 0.0105\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0286 - val_loss: 0.0093\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0273 - val_loss: 0.0093\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0261 - val_loss: 0.0093\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0259 - val_loss: 0.0092\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0265 - val_loss: 0.0097\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0272 - val_loss: 0.0097\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0261 - val_loss: 0.0098\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0267 - val_loss: 0.0098\n",
      "Epoch 26/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0232 - val_loss: 0.0098\n",
      "Epoch 27/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0258 - val_loss: 0.0093\n",
      "Epoch 28/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0254 - val_loss: 0.0098\n",
      "Epoch 29/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0289 - val_loss: 0.0091\n",
      "Epoch 30/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0298 - val_loss: 0.0097\n",
      "Epoch 31/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0273 - val_loss: 0.0093\n",
      "Epoch 32/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0247 - val_loss: 0.0095\n",
      "Epoch 33/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0237 - val_loss: 0.0093\n",
      "Epoch 34/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0239 - val_loss: 0.0100\n",
      "Epoch 35/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0244 - val_loss: 0.0101\n",
      "Epoch 36/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0256 - val_loss: 0.0100\n",
      "Epoch 37/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0248 - val_loss: 0.0093\n",
      "Epoch 38/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0252 - val_loss: 0.0095\n",
      "Epoch 39/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0238 - val_loss: 0.0091\n",
      "Epoch 40/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0234 - val_loss: 0.0093\n",
      "Epoch 41/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0253 - val_loss: 0.0097\n",
      "Epoch 42/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0254 - val_loss: 0.0093\n",
      "Epoch 43/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0219 - val_loss: 0.0093\n",
      "Epoch 44/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0229 - val_loss: 0.0095\n",
      "Epoch 45/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0250 - val_loss: 0.0095\n",
      "Epoch 46/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0233 - val_loss: 0.0098\n",
      "Epoch 47/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0226 - val_loss: 0.0093\n",
      "Epoch 48/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0235 - val_loss: 0.0091\n",
      "Epoch 49/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0226 - val_loss: 0.0095\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 349ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "RMSE: (118.93925704605643, 10.39280090332031)\n",
      "RMSE Random Walk: (38.77972060486749, 4.9985756976108044)\n",
      "[11392.99993896 11257.11993408 11022.76022339 10846.75982666\n",
      " 10921.8600769  10823.45986938 10678.63977051 10625.36001587\n",
      " 10444.23999023 10334.07997131 11129.68975067 11113.20295715\n",
      " 10861.07816315 10962.52014923 10837.59378815 10917.1650238\n",
      " 10858.65637207 10827.89067078 10802.74085999 10740.47875214\n",
      " 10746.2572403  10760.41225433 10791.44644928 10656.54833984\n",
      " 10758.59648895 10695.33732605 10634.46723938 10594.2959671\n",
      " 10561.34368134 10526.19229889]\n",
      "[122.51000214 120.31999969 120.86000061 118.12000275 115.06999969\n",
      " 114.76999664 114.69999695 117.69999695 111.30000305 110.33999634\n",
      " 116.97138214 117.96463776 117.3634491  117.70389557 116.27056885\n",
      " 116.8505249  116.18800354 117.70297241 116.90021515 115.89688873\n",
      " 115.65257263 115.68742371 115.77213287 116.64648438 116.19387817\n",
      " 115.21043396 114.54138184 113.68082428 113.51504517 113.07331085]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 152ms/step - loss: 0.3730 - val_loss: 0.0178\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.1662 - val_loss: 0.0343\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0868 - val_loss: 0.0213\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0510 - val_loss: 0.0127\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0449 - val_loss: 0.0163\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0373 - val_loss: 0.0098\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0327 - val_loss: 0.0092\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0311 - val_loss: 0.0091\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0276 - val_loss: 0.0096\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0284 - val_loss: 0.0096\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0266 - val_loss: 0.0111\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0299 - val_loss: 0.0107\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0286 - val_loss: 0.0107\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0260 - val_loss: 0.0104\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0289 - val_loss: 0.0105\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0278 - val_loss: 0.0106\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0263 - val_loss: 0.0106\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0284 - val_loss: 0.0106\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0257 - val_loss: 0.0103\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0285 - val_loss: 0.0113\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0295 - val_loss: 0.0098\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0345 - val_loss: 0.0105\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0270 - val_loss: 0.0098\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0269 - val_loss: 0.0090\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0271 - val_loss: 0.0100\n",
      "Epoch 26/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0299 - val_loss: 0.0096\n",
      "Epoch 27/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0277 - val_loss: 0.0102\n",
      "Epoch 28/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0255 - val_loss: 0.0116\n",
      "Epoch 29/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0295 - val_loss: 0.0108\n",
      "Epoch 30/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0283 - val_loss: 0.0100\n",
      "Epoch 31/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0244 - val_loss: 0.0115\n",
      "Epoch 32/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0286 - val_loss: 0.0101\n",
      "Epoch 33/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0272 - val_loss: 0.0108\n",
      "Epoch 34/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0245 - val_loss: 0.0104\n",
      "Epoch 35/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0264 - val_loss: 0.0105\n",
      "Epoch 36/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0258 - val_loss: 0.0099\n",
      "Epoch 37/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0292 - val_loss: 0.0097\n",
      "Epoch 38/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0250 - val_loss: 0.0097\n",
      "Epoch 39/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0261 - val_loss: 0.0095\n",
      "Epoch 40/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0255 - val_loss: 0.0096\n",
      "Epoch 41/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0248 - val_loss: 0.0090\n",
      "Epoch 42/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0278 - val_loss: 0.0095\n",
      "Epoch 43/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0263 - val_loss: 0.0096\n",
      "Epoch 44/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0260 - val_loss: 0.0095\n",
      "Epoch 45/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0273 - val_loss: 0.0096\n",
      "Epoch 46/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0274 - val_loss: 0.0110\n",
      "Epoch 47/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0275 - val_loss: 0.0095\n",
      "Epoch 48/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0254 - val_loss: 0.0106\n",
      "Epoch 49/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0272 - val_loss: 0.0091\n",
      "Epoch 50/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0298 - val_loss: 0.0099\n",
      "Epoch 51/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0241 - val_loss: 0.0093\n",
      "Epoch 52/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0243 - val_loss: 0.0097\n",
      "Epoch 53/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0236 - val_loss: 0.0094\n",
      "Epoch 54/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0239 - val_loss: 0.0097\n",
      "Epoch 55/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0236 - val_loss: 0.0108\n",
      "Epoch 56/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0259 - val_loss: 0.0099\n",
      "Epoch 57/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0258 - val_loss: 0.0099\n",
      "Epoch 58/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0252 - val_loss: 0.0093\n",
      "Epoch 59/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0233 - val_loss: 0.0095\n",
      "Epoch 60/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0251 - val_loss: 0.0087\n",
      "Epoch 61/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0277 - val_loss: 0.0097\n",
      "Epoch 62/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0274 - val_loss: 0.0089\n",
      "Epoch 63/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0253 - val_loss: 0.0088\n",
      "Epoch 64/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0269 - val_loss: 0.0099\n",
      "Epoch 65/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0268 - val_loss: 0.0088\n",
      "Epoch 66/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0270 - val_loss: 0.0096\n",
      "Epoch 67/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0265 - val_loss: 0.0089\n",
      "Epoch 68/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0235 - val_loss: 0.0094\n",
      "Epoch 69/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0254 - val_loss: 0.0097\n",
      "Epoch 70/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0245 - val_loss: 0.0089\n",
      "Epoch 71/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0278 - val_loss: 0.0100\n",
      "Epoch 72/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0250 - val_loss: 0.0089\n",
      "Epoch 73/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0238 - val_loss: 0.0097\n",
      "Epoch 74/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0239 - val_loss: 0.0091\n",
      "Epoch 75/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0242 - val_loss: 0.0102\n",
      "Epoch 76/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0251 - val_loss: 0.0096\n",
      "Epoch 77/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0234 - val_loss: 0.0103\n",
      "Epoch 78/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0237 - val_loss: 0.0095\n",
      "Epoch 79/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0228 - val_loss: 0.0110\n",
      "Epoch 80/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0257 - val_loss: 0.0093\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 414ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "RMSE: (113.24787590465273, 10.032508850097653)\n",
      "RMSE Random Walk: (45.3538462306596, 5.2350492720789426)\n",
      "[11515.5099411  11377.43993378 11143.620224   10964.87982941\n",
      " 11036.9300766  10938.22986603 10793.33976746 10743.06001282\n",
      " 10555.53999329 10444.41996765 11246.66113281 11231.16759491\n",
      " 10978.44161224 11080.2240448  10953.86435699 11034.01554871\n",
      " 10974.84437561 10945.59364319 10919.64107513 10856.37564087\n",
      " 10861.90981293 10876.09967804 10907.21858215 10773.19482422\n",
      " 10874.79036713 10810.54776001 10749.00862122 10707.97679138\n",
      " 10674.8587265  10639.26560974]\n",
      "[122.51000214 120.31999969 120.86000061 118.12000275 115.06999969\n",
      " 114.76999664 114.69999695 117.69999695 111.30000305 110.33999634\n",
      " 116.3562851  117.04187012 116.96186066 115.74552155 116.81052399\n",
      " 116.01036835 115.97228241 116.10611725 116.31397247 116.3711319\n",
      " 116.39829254 116.29296112 117.01934814 116.07341766 115.97442627\n",
      " 115.03765869 114.16260529 113.32221985 112.62059784 111.98872375]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 133ms/step - loss: 0.2798 - val_loss: 0.0027\n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1218 - val_loss: 0.0267\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0689 - val_loss: 0.0150\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0394 - val_loss: 0.0036\n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0297 - val_loss: 0.0022\n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0216 - val_loss: 0.0045\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0175 - val_loss: 0.0095\n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0152 - val_loss: 0.0111\n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0143 - val_loss: 0.0108\n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0147 - val_loss: 0.0083\n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0126 - val_loss: 0.0059\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0109 - val_loss: 0.0049\n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0113 - val_loss: 0.0040\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0106 - val_loss: 0.0041\n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0119 - val_loss: 0.0041\n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0122 - val_loss: 0.0044\n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0103 - val_loss: 0.0051\n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0097 - val_loss: 0.0052\n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0102 - val_loss: 0.0054\n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0118 - val_loss: 0.0051\n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0101 - val_loss: 0.0051\n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0100 - val_loss: 0.0047\n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0097 - val_loss: 0.0051\n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0097 - val_loss: 0.0055\n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0100 - val_loss: 0.0056\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "RMSE: (437.62120153425485, 16.601497650146484)\n",
      "RMSE Random Walk: (107.8397635754416, 8.285786161058475)\n",
      "[11638.01994324 11497.75993347 11264.48022461 11082.99983215\n",
      " 11152.00007629 11052.99986267 10908.0397644  10860.76000977\n",
      " 10666.83999634 10554.75996399 11363.01741791 11348.20946503\n",
      " 11095.4034729  11195.96956635 11070.67488098 11150.02591705\n",
      " 11090.81665802 11061.69976044 11035.95504761 10972.74677277\n",
      " 10978.30810547 10992.39263916 11024.2379303  10889.26824188\n",
      " 10990.7647934  10925.5854187  10863.1712265  10821.29901123\n",
      " 10787.47932434 10751.2543335 ]\n",
      "[244.11000061 242.69999695 249.30000305 249.11000061 245.69000244\n",
      " 241.1499939  245.16999817 241.16000366 226.53999329 224.55000305\n",
      " 272.90182495 237.34591675 239.64675903 265.57345581 254.31867981\n",
      " 245.13569641 245.2585907  261.52246094 246.98606873 251.25952148\n",
      " 234.38873291 257.87957764 244.59403992 239.91610718 227.23646545\n",
      " 224.92463684 224.27101135 222.10137939 219.32115173 217.9331665 ]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 226ms/step - loss: 0.3276 - val_loss: 0.0031\n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.2302 - val_loss: 0.0046\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.1548 - val_loss: 0.0091\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0846 - val_loss: 0.0060\n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0563 - val_loss: 0.0044\n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0369 - val_loss: 0.0050\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0200 - val_loss: 0.0048\n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0173 - val_loss: 0.0050\n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0154 - val_loss: 0.0049\n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0143 - val_loss: 0.0040\n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0146 - val_loss: 0.0032\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0127 - val_loss: 0.0026\n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.0136 - val_loss: 0.0022\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0125 - val_loss: 0.0021\n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.0128 - val_loss: 0.0021\n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0127 - val_loss: 0.0021\n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0119 - val_loss: 0.0022\n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0120 - val_loss: 0.0023\n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0121 - val_loss: 0.0024\n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0119 - val_loss: 0.0024\n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0123 - val_loss: 0.0024\n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0119 - val_loss: 0.0023\n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0111 - val_loss: 0.0024\n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0119 - val_loss: 0.0022\n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0117 - val_loss: 0.0023\n",
      "Epoch 26/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0120 - val_loss: 0.0023\n",
      "Epoch 27/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0118 - val_loss: 0.0022\n",
      "Epoch 28/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0128 - val_loss: 0.0022\n",
      "Epoch 29/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0113 - val_loss: 0.0022\n",
      "Epoch 30/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0112 - val_loss: 0.0022\n",
      "Epoch 31/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0122 - val_loss: 0.0022\n",
      "Epoch 32/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0115 - val_loss: 0.0024\n",
      "Epoch 33/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0124 - val_loss: 0.0023\n",
      "Epoch 34/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0111 - val_loss: 0.0023\n",
      "Epoch 35/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0111 - val_loss: 0.0025\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 724ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "RMSE: (435.0603487904764, 16.846132659912108)\n",
      "RMSE Random Walk: (126.79857918282605, 8.761656579331461)\n",
      "[11882.12994385 11740.45993042 11513.78022766 11332.10983276\n",
      " 11397.69007874 11294.14985657 11153.20976257 11101.92001343\n",
      " 10893.37998962 10779.30996704 11635.91924286 11585.55538177\n",
      " 11335.05023193 11461.54302216 11324.99356079 11395.16161346\n",
      " 11336.07524872 11323.22222137 11282.94111633 11224.00629425\n",
      " 11212.69683838 11250.2722168  11268.83197021 11129.18434906\n",
      " 11218.00125885 11150.51005554 11087.44223785 11043.40039062\n",
      " 11006.80047607 10969.1875    ]\n",
      "[244.11000061 242.69999695 249.30000305 249.11000061 245.69000244\n",
      " 241.1499939  245.16999817 241.16000366 226.53999329 224.55000305\n",
      " 225.77606201 222.75169373 215.81114197 221.08474731 229.43096924\n",
      " 230.99325562 221.81881714 217.46635437 225.26353455 224.05067444\n",
      " 217.53460693 220.50915527 206.39093018 211.44003296 208.86022949\n",
      " 208.4447937  207.81274414 206.73518372 205.55697632 204.37394714]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 180ms/step - loss: 0.4042 - val_loss: 0.0076\n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.2494 - val_loss: 0.0249\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.1118 - val_loss: 0.0790\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.0822 - val_loss: 0.0249\n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0487 - val_loss: 0.0124\n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.0398 - val_loss: 0.0111\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0343 - val_loss: 0.0054\n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0330 - val_loss: 0.0037\n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0283 - val_loss: 0.0035\n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0286 - val_loss: 0.0037\n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0221 - val_loss: 0.0044\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0265 - val_loss: 0.0047\n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0225 - val_loss: 0.0068\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0224 - val_loss: 0.0063\n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0241 - val_loss: 0.0079\n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0244 - val_loss: 0.0069\n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0217 - val_loss: 0.0074\n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0227 - val_loss: 0.0070\n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0220 - val_loss: 0.0074\n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0231 - val_loss: 0.0073\n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0214 - val_loss: 0.0077\n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0230 - val_loss: 0.0085\n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0236 - val_loss: 0.0087\n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0221 - val_loss: 0.0089\n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0252 - val_loss: 0.0099\n",
      "Epoch 26/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0217 - val_loss: 0.0089\n",
      "Epoch 27/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0239 - val_loss: 0.0110\n",
      "Epoch 28/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0234 - val_loss: 0.0102\n",
      "Epoch 29/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0219 - val_loss: 0.0101\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 414ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "RMSE: (97.13131444465834, 8.345234680175782)\n",
      "RMSE Random Walk: (233.20846148381924, 12.332053292997617)\n",
      "[12126.23994446 11983.15992737 11763.08023071 11581.21983337\n",
      " 11643.38008118 11535.29985046 11398.37976074 11343.08001709\n",
      " 11119.91998291 11003.85997009 11861.69530487 11808.3070755\n",
      " 11550.8613739  11682.62776947 11554.42453003 11626.15486908\n",
      " 11557.89406586 11540.68857574 11508.20465088 11448.05696869\n",
      " 11430.23144531 11470.78137207 11475.22290039 11340.62438202\n",
      " 11426.86148834 11358.95484924 11295.25498199 11250.13557434\n",
      " 11212.35745239 11173.56144714]\n",
      "[183.08999634 183.16999817 169.27000427 159.1499939  169.58000183\n",
      " 160.72000122 159.92999268 160.19000244 168.80000305 167.11000061\n",
      " 171.48620605 170.48120117 163.01165771 161.98597717 167.8369751\n",
      " 157.38964844 169.64857483 170.0052948  162.88526917 155.50639343\n",
      " 154.42901611 154.35946655 157.77146912 161.11181641 158.18334961\n",
      " 157.61618042 156.50537109 155.59184265 156.30000305 156.2696228 ]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 303ms/step - loss: 0.3914 - val_loss: 0.0082\n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2592 - val_loss: 0.0080\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0905 - val_loss: 0.0392\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0672 - val_loss: 0.0255\n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0518 - val_loss: 0.0152\n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0384 - val_loss: 0.0110\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0327 - val_loss: 0.0072\n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0319 - val_loss: 0.0053\n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0283 - val_loss: 0.0047\n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0273 - val_loss: 0.0043\n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0234 - val_loss: 0.0044\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0242 - val_loss: 0.0046\n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0248 - val_loss: 0.0052\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0236 - val_loss: 0.0056\n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0212 - val_loss: 0.0066\n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0234 - val_loss: 0.0068\n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0235 - val_loss: 0.0076\n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0220 - val_loss: 0.0074\n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0227 - val_loss: 0.0084\n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0222 - val_loss: 0.0077\n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0200 - val_loss: 0.0086\n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0194 - val_loss: 0.0081\n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0204 - val_loss: 0.0089\n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0188 - val_loss: 0.0082\n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0232 - val_loss: 0.0103\n",
      "Epoch 26/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0245 - val_loss: 0.0094\n",
      "Epoch 27/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0224 - val_loss: 0.0098\n",
      "Epoch 28/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0188 - val_loss: 0.0103\n",
      "Epoch 29/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0195 - val_loss: 0.0102\n",
      "Epoch 30/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0192 - val_loss: 0.0104\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 469ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "RMSE: (231.46834338073387, 11.93234634399414)\n",
      "RMSE Random Walk: (218.31551343769723, 11.795218798664159)\n",
      "[12309.3299408  12166.32992554 11932.35023499 11740.36982727\n",
      " 11812.96008301 11696.01985168 11558.30975342 11503.27001953\n",
      " 11288.71998596 11170.9699707  12033.18151093 11978.78827667\n",
      " 11713.87303162 11844.61374664 11722.26150513 11783.54451752\n",
      " 11727.54264069 11710.69387054 11671.08992004 11603.56336212\n",
      " 11584.66046143 11625.14083862 11632.99436951 11501.73619843\n",
      " 11585.04483795 11516.57102966 11451.76035309 11405.72741699\n",
      " 11368.65745544 11329.83106995]\n",
      "[183.08999634 183.16999817 169.27000427 159.1499939  169.58000183\n",
      " 160.72000122 159.92999268 160.19000244 168.80000305 167.11000061\n",
      " 179.27557373 179.83590698 164.80897522 171.86967468 166.70069885\n",
      " 167.96134949 161.45166016 160.32704163 165.34269714 166.17857361\n",
      " 168.39355469 160.75418091 160.68081665 161.04267883 170.88723755\n",
      " 169.25325012 167.71672058 167.48674011 168.01046753 168.50360107]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 406ms/step - loss: 0.3639 - val_loss: 0.0544\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.2235 - val_loss: 0.0338\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.1393 - val_loss: 0.0203\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0708 - val_loss: 0.0181\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0464 - val_loss: 0.0284\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0376 - val_loss: 0.0485\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0331 - val_loss: 0.0524\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0314 - val_loss: 0.0576\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0295 - val_loss: 0.0417\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0295 - val_loss: 0.0419\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0278 - val_loss: 0.0396\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0279 - val_loss: 0.0408\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0265 - val_loss: 0.0384\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0251 - val_loss: 0.0421\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0280 - val_loss: 0.0361\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0279 - val_loss: 0.0433\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0299 - val_loss: 0.0401\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0271 - val_loss: 0.0353\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0294 - val_loss: 0.0417\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0304 - val_loss: 0.0356\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0261 - val_loss: 0.0395\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0261 - val_loss: 0.0367\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0267 - val_loss: 0.0425\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0268 - val_loss: 0.0387\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 449ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "RMSE: (374.9503919217794, 18.905968475341798)\n",
      "RMSE Random Walk: (52.41621793015504, 5.804140707105018)\n",
      "[12492.41993713 12349.49992371 12101.62023926 11899.51982117\n",
      " 11982.54008484 11856.73985291 11718.23974609 11663.46002197\n",
      " 11457.51998901 11338.07997131 12212.45708466 12158.62418365\n",
      " 11878.68200684 12016.48342133 11888.96220398 11951.505867\n",
      " 11888.99430084 11871.02091217 11836.43261719 11769.74193573\n",
      " 11753.05401611 11785.89501953 11793.67518616 11662.77887726\n",
      " 11755.9320755  11685.82427979 11619.47707367 11573.2141571\n",
      " 11536.66792297 11498.33467102]\n",
      "[142.30000305 138.22999573 133.22000122 133.61999512 133.80000305\n",
      " 137.27999878 130.75       129.78999329 128.72999573 126.76999664\n",
      " 149.67704773 142.62770081 139.55456543 150.7696228  146.19055176\n",
      " 145.12745667 145.9831543  149.01782227 148.60919189 142.72819519\n",
      " 142.46435547 147.07557678 136.47357178 137.81829834 140.49356079\n",
      " 138.5528717  137.64698792 137.62872314 137.07325745 137.09687805]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 197ms/step - loss: 0.3732 - val_loss: 0.0660\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.2094 - val_loss: 0.0363\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.1123 - val_loss: 0.0196\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.0616 - val_loss: 0.0189\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0410 - val_loss: 0.0233\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0379 - val_loss: 0.0294\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0339 - val_loss: 0.0343\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.0306 - val_loss: 0.0385\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0318 - val_loss: 0.0404\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0330 - val_loss: 0.0410\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.0265 - val_loss: 0.0414\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.0276 - val_loss: 0.0387\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0303 - val_loss: 0.0372\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 0.0253 - val_loss: 0.0387\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.0281 - val_loss: 0.0362\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0266 - val_loss: 0.0388\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.0264 - val_loss: 0.0386\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0271 - val_loss: 0.0376\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0276 - val_loss: 0.0403\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0287 - val_loss: 0.0359\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0261 - val_loss: 0.0411\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0297 - val_loss: 0.0376\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0244 - val_loss: 0.0395\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0238 - val_loss: 0.0381\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 653ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "RMSE: (341.3437921907578, 16.731134796142577)\n",
      "RMSE Random Walk: (52.720427464931674, 5.737648469113495)\n",
      "[12634.71994019 12487.72991943 12234.84024048 12033.13981628\n",
      " 12116.34008789 11994.01985168 11848.98974609 11793.25001526\n",
      " 11586.24998474 11464.84996796 12362.13413239 12301.25188446\n",
      " 12018.23657227 12167.25304413 12035.15275574 12096.63332367\n",
      " 12034.97745514 12020.03873444 11985.04180908 11912.47013092\n",
      " 11895.51837158 11932.97059631 11930.14875793 11800.5971756\n",
      " 11896.42563629 11824.37715149 11757.12406158 11710.84288025\n",
      " 11673.74118042 11635.43154907]\n",
      "[142.30000305 138.22999573 133.22000122 133.61999512 133.80000305\n",
      " 137.27999878 130.75       129.78999329 128.72999573 126.76999664\n",
      " 143.96525574 131.36610413 140.05154419 143.4034729  141.52354431\n",
      " 149.24310303 137.2396698  136.55580139 142.93022156 134.01675415\n",
      " 135.78652954 144.73751831 136.33256531 145.47697449 143.8677063\n",
      " 142.36721802 141.10762024 140.47436523 139.73735046 138.92939758]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 176ms/step - loss: 0.3630 - val_loss: 0.1883\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.2246 - val_loss: 0.0739\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1209 - val_loss: 0.0274\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0611 - val_loss: 0.0287\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0490 - val_loss: 0.0228\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0401 - val_loss: 0.0390\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0357 - val_loss: 0.0505\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0343 - val_loss: 0.0642\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0345 - val_loss: 0.0629\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0351 - val_loss: 0.0550\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0322 - val_loss: 0.0646\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0317 - val_loss: 0.0501\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0324 - val_loss: 0.0592\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0306 - val_loss: 0.0540\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0324 - val_loss: 0.0667\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0291 - val_loss: 0.0587\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0305 - val_loss: 0.0732\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0307 - val_loss: 0.0626\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0325 - val_loss: 0.0810\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0299 - val_loss: 0.0631\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0285 - val_loss: 0.0798\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0296 - val_loss: 0.0701\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0270 - val_loss: 0.0750\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0282 - val_loss: 0.0724\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0272 - val_loss: 0.0827\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 461ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "RMSE: (156.90688094050856, 11.537755584716797)\n",
      "RMSE Random Walk: (35.45896507204927, 4.778670168029277)\n",
      "[12777.01994324 12625.95991516 12368.0602417  12166.7598114\n",
      " 12250.14009094 12131.29985046 11979.73974609 11923.04000854\n",
      " 11714.97998047 11591.6199646  12506.09938812 12432.61798859\n",
      " 12158.28811646 12310.65651703 12176.67630005 12245.8764267\n",
      " 12172.21712494 12156.59453583 12127.97203064 12046.48688507\n",
      " 12031.30490112 12077.70811462 12066.48132324 11946.07415009\n",
      " 12040.29334259 11966.74436951 11898.23168182 11851.31724548\n",
      " 11813.47853088 11774.36094666]\n",
      "[174.1499939  171.52000427 167.57000732 167.22999573 167.52999878\n",
      " 170.02999878 163.61999512 161.38000488 158.91000366 157.22000122\n",
      " 162.21006775 173.28346252 170.11024475 165.81646729 168.62120056\n",
      " 163.62237549 162.56985474 171.1217041  163.85720825 166.11924744\n",
      " 164.95344543 162.0942688  160.66194153 165.71983337 164.91000366\n",
      " 164.4793396  164.026474   163.69929504 163.3066864  162.71168518]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 164ms/step - loss: 0.3743 - val_loss: 0.2089\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.2549 - val_loss: 0.1094\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.1289 - val_loss: 0.0601\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0764 - val_loss: 0.0460\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0488 - val_loss: 0.0299\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0420 - val_loss: 0.0365\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0400 - val_loss: 0.0365\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0359 - val_loss: 0.0500\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0374 - val_loss: 0.0457\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0362 - val_loss: 0.0445\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0360 - val_loss: 0.0444\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0347 - val_loss: 0.0422\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0364 - val_loss: 0.0446\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0336 - val_loss: 0.0369\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0318 - val_loss: 0.0381\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0330 - val_loss: 0.0397\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0367 - val_loss: 0.0390\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0347 - val_loss: 0.0408\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0319 - val_loss: 0.0426\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0319 - val_loss: 0.0430\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0342 - val_loss: 0.0367\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0322 - val_loss: 0.0389\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0323 - val_loss: 0.0374\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0336 - val_loss: 0.0367\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0311 - val_loss: 0.0430\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 482ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "RMSE: (127.86308174381266, 10.48342514038086)\n",
      "RMSE Random Walk: (31.005778370212376, 4.24293753907174)\n",
      "[12951.16993713 12797.47991943 12535.63024902 12333.98980713\n",
      " 12417.67008972 12301.32984924 12143.35974121 12084.42001343\n",
      " 11873.88998413 11748.83996582 12668.30945587 12605.90145111\n",
      " 12328.39836121 12476.47298431 12345.29750061 12409.49880219\n",
      " 12334.78697968 12327.71623993 12291.82923889 12212.60613251\n",
      " 12196.25834656 12239.80238342 12227.14326477 12111.79398346\n",
      " 12205.20334625 12131.22370911 12062.25815582 12015.01654053\n",
      " 11976.78521729 11937.07263184]\n",
      "[174.1499939  171.52000427 167.57000732 167.22999573 167.52999878\n",
      " 170.02999878 163.61999512 161.38000488 158.91000366 157.22000122\n",
      " 162.11221313 173.23213196 170.90104675 168.51112366 167.87034607\n",
      " 165.70451355 165.2844696  160.6321106  170.09916687 163.40992737\n",
      " 165.28289795 159.10084534 164.84762573 165.49815369 161.17799377\n",
      " 160.82203674 160.4486084  160.1632843  159.88700867 159.54296875]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 229ms/step - loss: 0.4132 - val_loss: 0.0220\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.2282 - val_loss: 0.0316\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1119 - val_loss: 0.0343\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0498 - val_loss: 0.0208\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0459 - val_loss: 0.0115\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0355 - val_loss: 0.0101\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0293 - val_loss: 0.0148\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0310 - val_loss: 0.0152\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0289 - val_loss: 0.0181\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0289 - val_loss: 0.0146\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0270 - val_loss: 0.0120\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0279 - val_loss: 0.0115\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0286 - val_loss: 0.0101\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0281 - val_loss: 0.0095\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0300 - val_loss: 0.0098\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0291 - val_loss: 0.0095\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0261 - val_loss: 0.0091\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0270 - val_loss: 0.0099\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0281 - val_loss: 0.0096\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0278 - val_loss: 0.0094\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0283 - val_loss: 0.0097\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0255 - val_loss: 0.0101\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0265 - val_loss: 0.0098\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0241 - val_loss: 0.0104\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0271 - val_loss: 0.0097\n",
      "Epoch 26/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0264 - val_loss: 0.0094\n",
      "Epoch 27/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0258 - val_loss: 0.0095\n",
      "Epoch 28/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0245 - val_loss: 0.0092\n",
      "Epoch 29/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0242 - val_loss: 0.0094\n",
      "Epoch 30/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0245 - val_loss: 0.0096\n",
      "Epoch 31/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0253 - val_loss: 0.0099\n",
      "Epoch 32/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0257 - val_loss: 0.0106\n",
      "Epoch 33/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0225 - val_loss: 0.0104\n",
      "Epoch 34/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0224 - val_loss: 0.0101\n",
      "Epoch 35/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0287 - val_loss: 0.0090\n",
      "Epoch 36/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0253 - val_loss: 0.0089\n",
      "Epoch 37/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0245 - val_loss: 0.0090\n",
      "Epoch 38/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0229 - val_loss: 0.0096\n",
      "Epoch 39/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0245 - val_loss: 0.0094\n",
      "Epoch 40/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0246 - val_loss: 0.0095\n",
      "Epoch 41/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0237 - val_loss: 0.0098\n",
      "Epoch 42/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0241 - val_loss: 0.0095\n",
      "Epoch 43/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0246 - val_loss: 0.0098\n",
      "Epoch 44/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0247 - val_loss: 0.0105\n",
      "Epoch 45/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0267 - val_loss: 0.0095\n",
      "Epoch 46/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0268 - val_loss: 0.0110\n",
      "Epoch 47/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0257 - val_loss: 0.0096\n",
      "Epoch 48/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0261 - val_loss: 0.0114\n",
      "Epoch 49/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0278 - val_loss: 0.0092\n",
      "Epoch 50/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0228 - val_loss: 0.0095\n",
      "Epoch 51/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0240 - val_loss: 0.0094\n",
      "Epoch 52/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0229 - val_loss: 0.0112\n",
      "Epoch 53/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0257 - val_loss: 0.0100\n",
      "Epoch 54/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0246 - val_loss: 0.0104\n",
      "Epoch 55/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0231 - val_loss: 0.0097\n",
      "Epoch 56/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0223 - val_loss: 0.0101\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 391ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "RMSE: (116.19012565946029, 10.156037521362302)\n",
      "RMSE Random Walk: (39.2058027528618, 4.953706245050964)\n",
      "[13125.31993103 12968.99992371 12703.20025635 12501.21980286\n",
      " 12585.2000885  12471.35984802 12306.97973633 12245.80001831\n",
      " 12032.79998779 11906.05996704 12830.42166901 12779.13358307\n",
      " 12499.29940796 12644.98410797 12513.16784668 12575.20331573\n",
      " 12500.07144928 12488.34835052 12461.92840576 12376.01605988\n",
      " 12361.54124451 12398.90322876 12391.9908905  12277.29213715\n",
      " 12366.38134003 12292.04574585 12222.70676422 12175.17982483\n",
      " 12136.67222595 12096.61560059]\n",
      "[122.51000214 120.31999969 120.86000061 118.12000275 115.06999969\n",
      " 114.76999664 114.69999695 117.69999695 111.30000305 110.33999634\n",
      " 116.94499969 117.07965851 115.99136353 116.31283569 117.00224304\n",
      " 116.46438599 116.11564636 116.04800415 116.14444733 116.5136795\n",
      " 116.47320557 116.18190002 116.75997162 116.31584167 116.24750519\n",
      " 115.25405884 114.30316162 113.29050446 113.09757996 112.50976562]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 128ms/step - loss: 0.3913 - val_loss: 0.0234\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.2454 - val_loss: 0.0311\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1285 - val_loss: 0.0213\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0667 - val_loss: 0.0174\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0432 - val_loss: 0.0179\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0382 - val_loss: 0.0104\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0294 - val_loss: 0.0096\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0305 - val_loss: 0.0112\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0294 - val_loss: 0.0109\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0272 - val_loss: 0.0137\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0281 - val_loss: 0.0107\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0305 - val_loss: 0.0125\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0284 - val_loss: 0.0116\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0307 - val_loss: 0.0110\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0273 - val_loss: 0.0105\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0279 - val_loss: 0.0102\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0260 - val_loss: 0.0106\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0284 - val_loss: 0.0102\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0257 - val_loss: 0.0106\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0264 - val_loss: 0.0105\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0267 - val_loss: 0.0102\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0251 - val_loss: 0.0103\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0253 - val_loss: 0.0099\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0269 - val_loss: 0.0105\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0281 - val_loss: 0.0092\n",
      "Epoch 26/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0303 - val_loss: 0.0103\n",
      "Epoch 27/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0259 - val_loss: 0.0092\n",
      "Epoch 28/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0286 - val_loss: 0.0102\n",
      "Epoch 29/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0290 - val_loss: 0.0091\n",
      "Epoch 30/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0267 - val_loss: 0.0097\n",
      "Epoch 31/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0273 - val_loss: 0.0093\n",
      "Epoch 32/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0271 - val_loss: 0.0094\n",
      "Epoch 33/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0271 - val_loss: 0.0090\n",
      "Epoch 34/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0284 - val_loss: 0.0101\n",
      "Epoch 35/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0297 - val_loss: 0.0092\n",
      "Epoch 36/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0277 - val_loss: 0.0099\n",
      "Epoch 37/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0263 - val_loss: 0.0096\n",
      "Epoch 38/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0247 - val_loss: 0.0092\n",
      "Epoch 39/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0284 - val_loss: 0.0096\n",
      "Epoch 40/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0263 - val_loss: 0.0096\n",
      "Epoch 41/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0259 - val_loss: 0.0099\n",
      "Epoch 42/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0241 - val_loss: 0.0108\n",
      "Epoch 43/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0271 - val_loss: 0.0099\n",
      "Epoch 44/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0256 - val_loss: 0.0107\n",
      "Epoch 45/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0259 - val_loss: 0.0097\n",
      "Epoch 46/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0270 - val_loss: 0.0100\n",
      "Epoch 47/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0256 - val_loss: 0.0098\n",
      "Epoch 48/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0270 - val_loss: 0.0091\n",
      "Epoch 49/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0275 - val_loss: 0.0100\n",
      "Epoch 50/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0264 - val_loss: 0.0095\n",
      "Epoch 51/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0249 - val_loss: 0.0101\n",
      "Epoch 52/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0254 - val_loss: 0.0094\n",
      "Epoch 53/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0254 - val_loss: 0.0101\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 399ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "RMSE: (124.23100914481908, 10.555729675292966)\n",
      "RMSE Random Walk: (47.8657559311696, 5.2622854697871855)\n",
      "[13247.82993317 13089.3199234  12824.06025696 12619.3398056\n",
      " 12700.2700882  12586.12984467 12421.67973328 12363.50001526\n",
      " 12144.09999084 12016.39996338 12947.3666687  12896.21324158\n",
      " 12615.29077148 12761.29694366 12630.17008972 12691.66770172\n",
      " 12616.18709564 12604.39635468 12578.07285309 12492.52973938\n",
      " 12478.01445007 12515.08512878 12508.75086212 12393.60797882\n",
      " 12482.62884521 12407.29980469 12337.00992584 12288.47032928\n",
      " 12249.76980591 12209.12536621]\n",
      "[122.51000214 120.31999969 120.86000061 118.12000275 115.06999969\n",
      " 114.76999664 114.69999695 117.69999695 111.30000305 110.33999634\n",
      " 117.35459137 117.04971313 117.63111115 116.90390778 117.16951752\n",
      " 116.91156006 116.63162231 116.61610413 116.20809937 117.05847168\n",
      " 116.09306335 116.71563721 116.04901123 116.76964569 116.75067139\n",
      " 115.8182373  114.96082306 114.11307526 113.40300751 112.83673096]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 151ms/step - loss: 0.2797 - val_loss: 0.0040\n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.2007 - val_loss: 0.0164\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.1097 - val_loss: 0.0274\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0673 - val_loss: 0.0124\n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0354 - val_loss: 0.0037\n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0235 - val_loss: 0.0050\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0169 - val_loss: 0.0072\n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0165 - val_loss: 0.0061\n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0149 - val_loss: 0.0037\n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0132 - val_loss: 0.0026\n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0133 - val_loss: 0.0024\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0125 - val_loss: 0.0025\n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0121 - val_loss: 0.0029\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0114 - val_loss: 0.0033\n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0128 - val_loss: 0.0032\n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0148 - val_loss: 0.0036\n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0114 - val_loss: 0.0035\n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0102 - val_loss: 0.0037\n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0107 - val_loss: 0.0037\n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0103 - val_loss: 0.0041\n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0114 - val_loss: 0.0040\n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0102 - val_loss: 0.0042\n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0107 - val_loss: 0.0044\n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0101 - val_loss: 0.0045\n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0110 - val_loss: 0.0046\n",
      "Epoch 26/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0104 - val_loss: 0.0049\n",
      "Epoch 27/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0096 - val_loss: 0.0046\n",
      "Epoch 28/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0109 - val_loss: 0.0049\n",
      "Epoch 29/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0089 - val_loss: 0.0053\n",
      "Epoch 30/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0090 - val_loss: 0.0054\n",
      "Epoch 31/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0092 - val_loss: 0.0062\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 418ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "RMSE: (320.70515549557734, 15.07470092773437)\n",
      "RMSE Random Walk: (129.3074506145376, 9.130355024336446)\n",
      "[13370.3399353  13209.6399231  12944.92025757 12737.45980835\n",
      " 12815.34008789 12700.89984131 12536.37973022 12481.20001221\n",
      " 12255.3999939  12126.73995972 13064.72126007 13013.26295471\n",
      " 12732.92188263 12878.20085144 12747.33960724 12808.57926178\n",
      " 12732.81871796 12721.0124588  12694.28095245 12609.58821106\n",
      " 12594.10751343 12631.80076599 12624.79987335 12510.37762451\n",
      " 12599.3795166  12523.11804199 12451.9707489  12402.58340454\n",
      " 12363.17281342 12321.96209717]\n",
      "[244.11000061 242.69999695 249.30000305 249.11000061 245.69000244\n",
      " 241.1499939  245.16999817 241.16000366 226.53999329 224.55000305\n",
      " 236.45217896 228.16752625 236.32382202 227.99014282 240.44589233\n",
      " 225.31503296 231.48551941 216.52856445 226.86430359 221.39465332\n",
      " 209.920578   222.58361816 225.06288147 205.28433228 217.01716614\n",
      " 216.77085876 216.03117371 215.16200256 214.31852722 211.90234375]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 150ms/step - loss: 0.2954 - val_loss: 0.0024\n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.1798 - val_loss: 0.0059\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0738 - val_loss: 0.0156\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0561 - val_loss: 0.0055\n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0349 - val_loss: 0.0030\n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0259 - val_loss: 0.0026\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0234 - val_loss: 0.0020\n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0168 - val_loss: 0.0025\n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0135 - val_loss: 0.0032\n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0139 - val_loss: 0.0045\n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0141 - val_loss: 0.0052\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0128 - val_loss: 0.0051\n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0140 - val_loss: 0.0050\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0120 - val_loss: 0.0042\n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0122 - val_loss: 0.0039\n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0119 - val_loss: 0.0034\n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0126 - val_loss: 0.0034\n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0134 - val_loss: 0.0031\n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0118 - val_loss: 0.0031\n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0131 - val_loss: 0.0032\n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0119 - val_loss: 0.0030\n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0142 - val_loss: 0.0032\n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0112 - val_loss: 0.0028\n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0142 - val_loss: 0.0031\n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0121 - val_loss: 0.0029\n",
      "Epoch 26/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0109 - val_loss: 0.0031\n",
      "Epoch 27/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0134 - val_loss: 0.0030\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 381ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "RMSE: (428.79806221339396, 16.35010528564453)\n",
      "RMSE Random Walk: (116.57147674373134, 8.500046385010712)\n",
      "[13614.44993591 13452.33992004 13194.22026062 12986.56980896\n",
      " 13061.03009033 12942.04983521 12781.54972839 12722.36001587\n",
      " 12481.93998718 12351.28996277 13301.17343903 13241.43048096\n",
      " 12969.24570465 13106.19099426 12987.78549957 13033.89429474\n",
      " 12964.30423737 12937.54102325 12921.14525604 12830.98286438\n",
      " 12804.02809143 12854.38438416 12849.86275482 12715.66195679\n",
      " 12816.39668274 12739.88890076 12668.00192261 12617.7454071\n",
      " 12577.49134064 12533.86444092]\n",
      "[244.11000061 242.69999695 249.30000305 249.11000061 245.69000244\n",
      " 241.1499939  245.16999817 241.16000366 226.53999329 224.55000305\n",
      " 235.58963013 257.51107788 266.15530396 241.88952637 252.0599823\n",
      " 273.07922363 241.6806488  240.51849365 258.23074341 236.72042847\n",
      " 235.30586243 243.3420105  236.53009033 254.47032166 223.33547974\n",
      " 223.00532532 221.89338684 220.37466431 218.44219971 216.61828613]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 195ms/step - loss: 0.3888 - val_loss: 0.0103\n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.2746 - val_loss: 0.0206\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.1579 - val_loss: 0.0435\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.1145 - val_loss: 0.0144\n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0771 - val_loss: 0.0086\n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0487 - val_loss: 0.0078\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0361 - val_loss: 0.0050\n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0311 - val_loss: 0.0048\n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0275 - val_loss: 0.0049\n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0270 - val_loss: 0.0050\n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0252 - val_loss: 0.0061\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0234 - val_loss: 0.0079\n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0238 - val_loss: 0.0091\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0236 - val_loss: 0.0096\n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0258 - val_loss: 0.0086\n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0196 - val_loss: 0.0092\n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0236 - val_loss: 0.0086\n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.0226 - val_loss: 0.0102\n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0213 - val_loss: 0.0098\n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0203 - val_loss: 0.0102\n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0197 - val_loss: 0.0102\n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0198 - val_loss: 0.0111\n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0196 - val_loss: 0.0110\n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0195 - val_loss: 0.0121\n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0176 - val_loss: 0.0117\n",
      "Epoch 26/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0216 - val_loss: 0.0150\n",
      "Epoch 27/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0249 - val_loss: 0.0122\n",
      "Epoch 28/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0213 - val_loss: 0.0143\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 395ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "RMSE: (164.00759421214462, 10.744082641601562)\n",
      "RMSE Random Walk: (219.2602686106192, 12.033938084378772)\n",
      "[13858.55993652 13695.03991699 13443.52026367 13235.67980957\n",
      " 13306.72009277 13183.1998291  13026.71972656 12963.52001953\n",
      " 12708.47998047 12575.83996582 13536.76306915 13498.94155884\n",
      " 13235.40100861 13348.08052063 13239.84548187 13306.97351837\n",
      " 13205.98488617 13178.05951691 13179.37599945 13067.70329285\n",
      " 13039.33395386 13097.72639465 13086.39284515 12970.13227844\n",
      " 13039.73216248 12962.89422607 12889.89530945 12838.12007141\n",
      " 12795.93354034 12750.48272705]\n",
      "[183.08999634 183.16999817 169.27000427 159.1499939  169.58000183\n",
      " 160.72000122 159.92999268 160.19000244 168.80000305 167.11000061\n",
      " 176.8571167  171.9981842  165.20068359 159.12599182 161.2232666\n",
      " 159.26477051 161.55984497 179.66152954 157.65383911 155.53990173\n",
      " 165.52745056 161.97451782 162.77590942 161.8349762  163.38618469\n",
      " 161.98851013 161.03297424 161.08378601 162.56834412 163.87585449]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 150ms/step - loss: 0.4155 - val_loss: 0.0093\n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.2523 - val_loss: 0.0138\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.1428 - val_loss: 0.0250\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0705 - val_loss: 0.0164\n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0485 - val_loss: 0.0154\n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0391 - val_loss: 0.0111\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0308 - val_loss: 0.0062\n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0286 - val_loss: 0.0047\n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0263 - val_loss: 0.0041\n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0261 - val_loss: 0.0044\n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0273 - val_loss: 0.0044\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0251 - val_loss: 0.0051\n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0256 - val_loss: 0.0053\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0243 - val_loss: 0.0062\n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0258 - val_loss: 0.0064\n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0247 - val_loss: 0.0072\n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0219 - val_loss: 0.0073\n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0252 - val_loss: 0.0084\n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0231 - val_loss: 0.0075\n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0232 - val_loss: 0.0099\n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0227 - val_loss: 0.0080\n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0258 - val_loss: 0.0100\n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0222 - val_loss: 0.0091\n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0212 - val_loss: 0.0103\n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0196 - val_loss: 0.0092\n",
      "Epoch 26/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0200 - val_loss: 0.0105\n",
      "Epoch 27/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0206 - val_loss: 0.0102\n",
      "Epoch 28/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0194 - val_loss: 0.0111\n",
      "Epoch 29/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0216 - val_loss: 0.0110\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 374ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "RMSE: (118.1604451626772, 8.420770263671875)\n",
      "RMSE Random Walk: (213.45096338838502, 11.968567320578968)\n",
      "[14041.64993286 13878.20991516 13612.79026794 13394.82980347\n",
      " 13476.3000946  13343.91983032 13186.64971924 13123.71002197\n",
      " 12877.27998352 12742.94996643 13713.62018585 13670.93974304\n",
      " 13400.6016922  13507.20651245 13401.06874847 13466.23828888\n",
      " 13367.54473114 13357.72104645 13337.02983856 13223.24319458\n",
      " 13204.86140442 13259.70091248 13249.16875458 13131.96725464\n",
      " 13203.11834717 13124.88273621 13050.92828369 12999.20385742\n",
      " 12958.50188446 12914.35858154]\n",
      "[183.08999634 183.16999817 169.27000427 159.1499939  169.58000183\n",
      " 160.72000122 159.92999268 160.19000244 168.80000305 167.11000061\n",
      " 179.76837158 177.67141724 171.9536438  169.10220337 165.73974609\n",
      " 166.80604553 165.2208252  162.61491394 161.99679565 167.33758545\n",
      " 160.77850342 165.2481842  155.24752808 173.55561829 159.82705688\n",
      " 158.52191162 157.49429321 157.59828186 158.28862    159.11486816]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 125ms/step - loss: 0.3638 - val_loss: 0.0643\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2196 - val_loss: 0.0473\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1476 - val_loss: 0.0369\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0973 - val_loss: 0.0247\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0633 - val_loss: 0.0312\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0418 - val_loss: 0.0410\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0337 - val_loss: 0.0575\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0336 - val_loss: 0.0583\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0331 - val_loss: 0.0554\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0282 - val_loss: 0.0483\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0260 - val_loss: 0.0457\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0285 - val_loss: 0.0417\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0259 - val_loss: 0.0410\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0261 - val_loss: 0.0418\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0263 - val_loss: 0.0417\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0236 - val_loss: 0.0436\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0280 - val_loss: 0.0414\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0279 - val_loss: 0.0397\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0273 - val_loss: 0.0430\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0276 - val_loss: 0.0439\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0260 - val_loss: 0.0420\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0233 - val_loss: 0.0420\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0266 - val_loss: 0.0394\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0237 - val_loss: 0.0410\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "RMSE: (225.85314874281465, 12.528532028198242)\n",
      "RMSE Random Walk: (53.71745350406202, 5.857535215649024)\n",
      "[14224.7399292  14061.37991333 13782.06027222 13553.97979736\n",
      " 13645.88009644 13504.63983154 13346.57971191 13283.90002441\n",
      " 13046.07998657 12910.05996704 13893.38855743 13848.61116028\n",
      " 13572.555336   13676.30871582 13566.80849457 13633.04433441\n",
      " 13532.76555634 13520.33596039 13499.02663422 13390.58078003\n",
      " 13365.63990784 13424.94909668 13404.41628265 13305.52287292\n",
      " 13362.94540405 13283.40464783 13208.4225769  13156.80213928\n",
      " 13116.79050446 13073.47344971]\n",
      "[142.30000305 138.22999573 133.22000122 133.61999512 133.80000305\n",
      " 137.27999878 130.75       129.78999329 128.72999573 126.76999664\n",
      " 122.60449982 146.168396   143.49119568 142.65653992 148.92572021\n",
      " 119.4145813  153.34536743 151.74833679 148.16247559 123.87282562\n",
      " 143.94361877 131.7063446  151.02618408 138.7300415  120.71147919\n",
      " 120.3396759  119.63434601 118.98634338 118.66310883 117.97338867]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 134ms/step - loss: 0.3523 - val_loss: 0.0690\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2239 - val_loss: 0.0427\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1195 - val_loss: 0.0308\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0705 - val_loss: 0.0257\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0444 - val_loss: 0.0191\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0376 - val_loss: 0.0242\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0353 - val_loss: 0.0309\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0325 - val_loss: 0.0363\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0269 - val_loss: 0.0411\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0312 - val_loss: 0.0413\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0272 - val_loss: 0.0427\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0261 - val_loss: 0.0414\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0277 - val_loss: 0.0386\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0285 - val_loss: 0.0375\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0287 - val_loss: 0.0375\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0253 - val_loss: 0.0380\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0291 - val_loss: 0.0372\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0260 - val_loss: 0.0373\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0268 - val_loss: 0.0395\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0261 - val_loss: 0.0345\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0260 - val_loss: 0.0394\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0258 - val_loss: 0.0374\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0244 - val_loss: 0.0390\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0282 - val_loss: 0.0395\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0226 - val_loss: 0.0380\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 394ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "RMSE: (339.4085988865176, 17.735003662109374)\n",
      "RMSE Random Walk: (52.157031795207615, 5.648704440453718)\n",
      "[14367.03993225 14199.60990906 13915.28027344 13687.59979248\n",
      " 13779.68009949 13641.91983032 13477.32971191 13413.6900177\n",
      " 13174.8099823  13036.82996368 14015.99305725 13994.77955627\n",
      " 13716.04653168 13818.96525574 13715.73421478 13752.45891571\n",
      " 13686.11092377 13672.08429718 13647.1891098  13514.45360565\n",
      " 13509.58352661 13556.65544128 13555.44246674 13444.25291443\n",
      " 13483.65688324 13403.74432373 13328.05692291 13275.78848267\n",
      " 13235.45361328 13191.44683838]\n",
      "[142.30000305 138.22999573 133.22000122 133.61999512 133.80000305\n",
      " 137.27999878 130.75       129.78999329 128.72999573 126.76999664\n",
      " 149.1633606  144.94795227 142.3478241  150.06340027 139.04998779\n",
      " 140.34832764 142.45359802 145.00990295 146.32156372 144.62539673\n",
      " 137.6394043  146.94909668 141.47415161 143.51405334 138.00909424\n",
      " 136.88755798 135.93797302 135.4209137  134.82885742 134.19767761]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 123ms/step - loss: 0.3651 - val_loss: 0.1883\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.2134 - val_loss: 0.0752\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1160 - val_loss: 0.0466\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0689 - val_loss: 0.0409\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0471 - val_loss: 0.0247\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0413 - val_loss: 0.0414\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0341 - val_loss: 0.0418\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0332 - val_loss: 0.0601\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0317 - val_loss: 0.0538\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0335 - val_loss: 0.0585\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0322 - val_loss: 0.0536\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0357 - val_loss: 0.0587\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0304 - val_loss: 0.0496\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0337 - val_loss: 0.0587\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0302 - val_loss: 0.0557\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0307 - val_loss: 0.0643\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0271 - val_loss: 0.0589\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0303 - val_loss: 0.0689\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0287 - val_loss: 0.0607\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0286 - val_loss: 0.0655\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0286 - val_loss: 0.0663\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0264 - val_loss: 0.0754\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0308 - val_loss: 0.0638\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0289 - val_loss: 0.0730\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0266 - val_loss: 0.0738\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 367ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "RMSE: (171.72345189113403, 12.256826019287109)\n",
      "RMSE Random Walk: (33.53964188721587, 4.626026709292631)\n",
      "[14509.3399353  14337.83990479 14048.50027466 13821.2197876\n",
      " 13913.48010254 13779.1998291  13608.07971191 13543.48001099\n",
      " 13303.53997803 13163.59996033 14165.15641785 14139.72750854\n",
      " 13858.39435577 13969.02865601 13854.78420258 13892.80724335\n",
      " 13828.56452179 13817.09420013 13793.51067352 13659.07900238\n",
      " 13647.22293091 13703.60453796 13696.91661835 13587.76696777\n",
      " 13621.66597748 13540.63188171 13463.99489594 13411.20939636\n",
      " 13370.2824707  13325.64451599]\n",
      "[174.1499939  171.52000427 167.57000732 167.22999573 167.52999878\n",
      " 170.02999878 163.61999512 161.38000488 158.91000366 157.22000122\n",
      " 170.1085968  166.85385132 164.51472473 164.77967834 171.08303833\n",
      " 165.62937927 165.74853516 173.42588806 170.50065613 163.46936035\n",
      " 166.31794739 168.75016785 158.94889832 162.61582947 166.05418396\n",
      " 165.42593384 164.68103027 164.23320007 163.83163452 163.0239563 ]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 120ms/step - loss: 0.3538 - val_loss: 0.1781\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.2103 - val_loss: 0.0754\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1048 - val_loss: 0.0546\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0624 - val_loss: 0.0397\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0430 - val_loss: 0.0332\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0387 - val_loss: 0.0380\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0347 - val_loss: 0.0392\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0370 - val_loss: 0.0419\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0341 - val_loss: 0.0426\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0353 - val_loss: 0.0440\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0330 - val_loss: 0.0477\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0349 - val_loss: 0.0428\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0352 - val_loss: 0.0461\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0291 - val_loss: 0.0383\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0334 - val_loss: 0.0424\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0328 - val_loss: 0.0424\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0328 - val_loss: 0.0351\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0339 - val_loss: 0.0432\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0328 - val_loss: 0.0405\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0332 - val_loss: 0.0408\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0330 - val_loss: 0.0420\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0287 - val_loss: 0.0417\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0344 - val_loss: 0.0430\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0297 - val_loss: 0.0405\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0294 - val_loss: 0.0416\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 367ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "RMSE: (121.20749328298261, 10.250951385498047)\n",
      "RMSE Random Walk: (32.642598418077384, 4.534603476618145)\n",
      "[14683.4899292  14509.35990906 14216.07028198 13988.44978333\n",
      " 14081.01010132 13949.22982788 13771.69970703 13704.86001587\n",
      " 13462.44998169 13320.81996155 14335.26501465 14306.58135986\n",
      " 14022.90908051 14133.80833435 14025.86724091 14058.43662262\n",
      " 13994.31305695 13990.5200882  13964.01132965 13822.54836273\n",
      " 13813.5408783  13872.35470581 13855.86551666 13750.38279724\n",
      " 13787.72016144 13706.05781555 13628.67592621 13575.44259644\n",
      " 13534.11410522 13488.66847229]\n",
      "[174.1499939  171.52000427 167.57000732 167.22999573 167.52999878\n",
      " 170.02999878 163.61999512 161.38000488 158.91000366 157.22000122\n",
      " 165.89941406 171.12927246 159.1539917  161.35678101 168.89920044\n",
      " 162.60414124 166.51893616 170.81556702 169.04396057 160.35540771\n",
      " 166.27423096 163.49325562 167.3396759  167.07595825 161.69421387\n",
      " 160.97575378 160.23757935 159.65318298 159.04359436 158.31488037]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 128ms/step - loss: 0.3868 - val_loss: 0.0194\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.2034 - val_loss: 0.0500\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.1131 - val_loss: 0.0319\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0611 - val_loss: 0.0182\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0476 - val_loss: 0.0144\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0321 - val_loss: 0.0108\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0314 - val_loss: 0.0114\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0285 - val_loss: 0.0138\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0287 - val_loss: 0.0141\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0276 - val_loss: 0.0126\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0281 - val_loss: 0.0108\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0260 - val_loss: 0.0101\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0265 - val_loss: 0.0099\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0252 - val_loss: 0.0100\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0282 - val_loss: 0.0095\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0273 - val_loss: 0.0099\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0296 - val_loss: 0.0105\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0267 - val_loss: 0.0111\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0264 - val_loss: 0.0109\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0293 - val_loss: 0.0107\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0267 - val_loss: 0.0100\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0244 - val_loss: 0.0100\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0247 - val_loss: 0.0101\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0265 - val_loss: 0.0099\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0254 - val_loss: 0.0103\n",
      "Epoch 26/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0253 - val_loss: 0.0097\n",
      "Epoch 27/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0273 - val_loss: 0.0092\n",
      "Epoch 28/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0307 - val_loss: 0.0092\n",
      "Epoch 29/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0263 - val_loss: 0.0090\n",
      "Epoch 30/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0257 - val_loss: 0.0091\n",
      "Epoch 31/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0267 - val_loss: 0.0093\n",
      "Epoch 32/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0241 - val_loss: 0.0095\n",
      "Epoch 33/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0238 - val_loss: 0.0105\n",
      "Epoch 34/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0253 - val_loss: 0.0097\n",
      "Epoch 35/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0268 - val_loss: 0.0115\n",
      "Epoch 36/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0342 - val_loss: 0.0095\n",
      "Epoch 37/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.0257 - val_loss: 0.0095\n",
      "Epoch 38/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0276 - val_loss: 0.0094\n",
      "Epoch 39/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0262 - val_loss: 0.0098\n",
      "Epoch 40/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0228 - val_loss: 0.0098\n",
      "Epoch 41/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0239 - val_loss: 0.0099\n",
      "Epoch 42/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0232 - val_loss: 0.0098\n",
      "Epoch 43/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0241 - val_loss: 0.0098\n",
      "Epoch 44/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0226 - val_loss: 0.0098\n",
      "Epoch 45/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0244 - val_loss: 0.0098\n",
      "Epoch 46/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0225 - val_loss: 0.0105\n",
      "Epoch 47/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0279 - val_loss: 0.0095\n",
      "Epoch 48/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0258 - val_loss: 0.0104\n",
      "Epoch 49/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0263 - val_loss: 0.0095\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 395ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "RMSE: (134.1422293750016, 11.16698188781738)\n",
      "RMSE Random Walk: (40.77829153448717, 4.960556081684529)\n",
      "[14857.6399231  14680.87991333 14383.64028931 14155.67977905\n",
      " 14248.5401001  14119.25982666 13935.31970215 13866.24002075\n",
      " 13621.35998535 13478.03996277 14501.16442871 14477.71063232\n",
      " 14182.0630722  14295.16511536 14194.76644135 14221.04076385\n",
      " 14160.8319931  14161.33565521 14133.05529022 13982.90377045\n",
      " 13979.81510925 14035.84796143 14023.20519257 13917.45875549\n",
      " 13949.41437531 13867.03356934 13788.91350555 13735.09577942\n",
      " 13693.15769958 13646.98335266]\n",
      "[122.51000214 120.31999969 120.86000061 118.12000275 115.06999969\n",
      " 114.76999664 114.69999695 117.69999695 111.30000305 110.33999634\n",
      " 118.04013062 118.47803497 118.54360962 118.1198349  117.37493896\n",
      " 117.82673645 118.03094482 117.62999725 117.59069061 117.84375763\n",
      " 118.01605225 117.43811035 117.39078522 117.37825775 116.2754364\n",
      " 115.30492401 114.45483398 113.50592804 113.27163696 112.75500488]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 121ms/step - loss: 0.3861 - val_loss: 0.0247\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2110 - val_loss: 0.0355\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1402 - val_loss: 0.0164\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0807 - val_loss: 0.0166\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0479 - val_loss: 0.0123\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0376 - val_loss: 0.0097\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0322 - val_loss: 0.0092\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0300 - val_loss: 0.0097\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0289 - val_loss: 0.0111\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0299 - val_loss: 0.0099\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0300 - val_loss: 0.0112\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0299 - val_loss: 0.0095\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0295 - val_loss: 0.0103\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0275 - val_loss: 0.0091\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0293 - val_loss: 0.0103\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0280 - val_loss: 0.0095\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0255 - val_loss: 0.0099\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0263 - val_loss: 0.0094\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0273 - val_loss: 0.0101\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0285 - val_loss: 0.0096\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0250 - val_loss: 0.0097\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0285 - val_loss: 0.0092\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0260 - val_loss: 0.0108\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0313 - val_loss: 0.0093\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0277 - val_loss: 0.0102\n",
      "Epoch 26/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0288 - val_loss: 0.0088\n",
      "Epoch 27/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0312 - val_loss: 0.0101\n",
      "Epoch 28/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0274 - val_loss: 0.0093\n",
      "Epoch 29/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0265 - val_loss: 0.0097\n",
      "Epoch 30/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0275 - val_loss: 0.0093\n",
      "Epoch 31/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0277 - val_loss: 0.0095\n",
      "Epoch 32/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0256 - val_loss: 0.0100\n",
      "Epoch 33/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0279 - val_loss: 0.0102\n",
      "Epoch 34/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0273 - val_loss: 0.0108\n",
      "Epoch 35/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0251 - val_loss: 0.0108\n",
      "Epoch 36/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0261 - val_loss: 0.0111\n",
      "Epoch 37/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0240 - val_loss: 0.0108\n",
      "Epoch 38/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0273 - val_loss: 0.0107\n",
      "Epoch 39/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0262 - val_loss: 0.0103\n",
      "Epoch 40/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0263 - val_loss: 0.0097\n",
      "Epoch 41/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0280 - val_loss: 0.0104\n",
      "Epoch 42/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0273 - val_loss: 0.0094\n",
      "Epoch 43/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0233 - val_loss: 0.0099\n",
      "Epoch 44/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0263 - val_loss: 0.0093\n",
      "Epoch 45/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0240 - val_loss: 0.0098\n",
      "Epoch 46/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0257 - val_loss: 0.0092\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 361ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "RMSE: (125.84672019850984, 10.69609336853027)\n",
      "RMSE Random Walk: (43.65233275684382, 5.20995895927842)\n",
      "[14980.14992523 14801.19991302 14504.50028992 14273.7997818\n",
      " 14363.61009979 14234.0298233  14050.0196991  13983.9400177\n",
      " 13732.6599884  13588.37995911 14619.20455933 14596.1886673\n",
      " 14300.60668182 14413.28495026 14312.14138031 14338.86750031\n",
      " 14278.86293793 14278.96565247 14250.64598083 14100.74752808\n",
      " 14097.8311615  14153.28607178 14140.59597778 14034.83701324\n",
      " 14065.68981171 13982.33849335 13903.36833954 13848.60170746\n",
      " 13806.42933655 13759.73835754]\n",
      "[122.51000214 120.31999969 120.86000061 118.12000275 115.06999969\n",
      " 114.76999664 114.69999695 117.69999695 111.30000305 110.33999634\n",
      " 118.06213379 117.20433044 116.80631256 117.23959351 117.70158386\n",
      " 117.11643219 116.71398163 116.95552826 116.96082306 117.51012421\n",
      " 117.44651031 117.20889282 117.11061096 117.96955872 116.41589355\n",
      " 115.32304382 114.34667969 113.32659912 112.50098419 111.93225861]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 0.3182 - val_loss: 0.0031\n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.1901 - val_loss: 0.0090\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0701 - val_loss: 0.0156\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0570 - val_loss: 0.0039\n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0324 - val_loss: 0.0029\n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0207 - val_loss: 0.0076\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0169 - val_loss: 0.0149\n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0142 - val_loss: 0.0160\n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0139 - val_loss: 0.0126\n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0121 - val_loss: 0.0082\n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0108 - val_loss: 0.0056\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0119 - val_loss: 0.0045\n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0106 - val_loss: 0.0042\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0118 - val_loss: 0.0044\n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0103 - val_loss: 0.0048\n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0109 - val_loss: 0.0052\n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0103 - val_loss: 0.0056\n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0110 - val_loss: 0.0052\n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0110 - val_loss: 0.0052\n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0099 - val_loss: 0.0049\n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0097 - val_loss: 0.0049\n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0101 - val_loss: 0.0050\n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0101 - val_loss: 0.0055\n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0098 - val_loss: 0.0060\n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0091 - val_loss: 0.0057\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 438ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "RMSE: (276.31446749047615, 14.442261505126956)\n",
      "RMSE Random Walk: (108.95235830674444, 8.488819893632842)\n",
      "[15102.65992737 14921.51991272 14625.36029053 14391.91978455\n",
      " 14478.68009949 14348.79981995 14164.71969604 14101.64001465\n",
      " 13843.95999146 13698.71995544 14737.26669312 14713.39299774\n",
      " 14417.41299438 14530.52454376 14429.84296417 14455.9839325\n",
      " 14395.57691956 14395.92118073 14367.60680389 14218.25765228\n",
      " 14215.27767181 14270.4949646  14257.70658875 14152.80657196\n",
      " 14182.10570526 14097.66153717 14017.71501923 13961.92830658\n",
      " 13918.93032074 13871.67061615]\n",
      "[244.11000061 242.69999695 249.30000305 249.11000061 245.69000244\n",
      " 241.1499939  245.16999817 241.16000366 226.53999329 224.55000305\n",
      " 248.21923828 249.76702881 248.14245605 234.70770264 231.22779846\n",
      " 221.59988403 207.99597168 239.28411865 247.59384155 230.54336548\n",
      " 221.42170715 202.84880066 232.35206604 221.71511841 248.3678894\n",
      " 247.30958557 246.16267395 244.20011902 241.78875732 239.76846313]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 209ms/step - loss: 0.2587 - val_loss: 0.0031\n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.2053 - val_loss: 0.0061\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.1080 - val_loss: 0.0112\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0681 - val_loss: 0.0054\n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0402 - val_loss: 0.0034\n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0285 - val_loss: 0.0027\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0216 - val_loss: 0.0028\n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0161 - val_loss: 0.0037\n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0140 - val_loss: 0.0048\n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0133 - val_loss: 0.0056\n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0126 - val_loss: 0.0052\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0124 - val_loss: 0.0045\n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0116 - val_loss: 0.0038\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0125 - val_loss: 0.0032\n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0111 - val_loss: 0.0029\n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0126 - val_loss: 0.0026\n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0127 - val_loss: 0.0025\n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0126 - val_loss: 0.0025\n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0110 - val_loss: 0.0024\n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0119 - val_loss: 0.0025\n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0111 - val_loss: 0.0026\n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0123 - val_loss: 0.0026\n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0113 - val_loss: 0.0026\n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0119 - val_loss: 0.0025\n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0122 - val_loss: 0.0026\n",
      "Epoch 26/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0124 - val_loss: 0.0025\n",
      "Epoch 27/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0142 - val_loss: 0.0026\n",
      "Epoch 28/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0125 - val_loss: 0.0026\n",
      "Epoch 29/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0118 - val_loss: 0.0027\n",
      "Epoch 30/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0103 - val_loss: 0.0026\n",
      "Epoch 31/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0112 - val_loss: 0.0027\n",
      "Epoch 32/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0111 - val_loss: 0.0027\n",
      "Epoch 33/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0112 - val_loss: 0.0027\n",
      "Epoch 34/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0117 - val_loss: 0.0027\n",
      "Epoch 35/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0127 - val_loss: 0.0027\n",
      "Epoch 36/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0125 - val_loss: 0.0028\n",
      "Epoch 37/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0140 - val_loss: 0.0024\n",
      "Epoch 38/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0148 - val_loss: 0.0027\n",
      "Epoch 39/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0138 - val_loss: 0.0025\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 421ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "RMSE: (488.80151825487826, 17.83447341918945)\n",
      "RMSE Random Walk: (110.10228373555105, 8.33974483624006)\n",
      "[15346.76992798 15164.21990967 14874.66029358 14641.02978516\n",
      " 14724.37010193 14589.94981384 14409.88969421 14342.80001831\n",
      " 14070.49998474 13923.2699585  14985.4859314  14963.16002655\n",
      " 14665.55545044 14765.2322464  14661.07076263 14677.58381653\n",
      " 14603.57289124 14635.20529938 14615.20064545 14448.80101776\n",
      " 14436.69937897 14473.34376526 14490.05865479 14374.52169037\n",
      " 14430.47359467 14344.97112274 14263.87769318 14206.1284256\n",
      " 14160.71907806 14111.43907928]\n",
      "[244.11000061 242.69999695 249.30000305 249.11000061 245.69000244\n",
      " 241.1499939  245.16999817 241.16000366 226.53999329 224.55000305\n",
      " 227.93806458 224.06703186 227.08293152 225.68540955 210.33181763\n",
      " 216.4783783  224.98162842 223.17591858 225.7539978  220.48968506\n",
      " 209.71913147 219.88465881 203.56816101 219.98858643 207.64100647\n",
      " 206.85002136 205.85990906 204.47692871 202.96865845 201.58531189]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 140ms/step - loss: 0.3857 - val_loss: 0.0100\n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.2813 - val_loss: 0.0113\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1588 - val_loss: 0.0465\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0765 - val_loss: 0.0446\n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0551 - val_loss: 0.0177\n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0367 - val_loss: 0.0061\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0314 - val_loss: 0.0054\n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0330 - val_loss: 0.0050\n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0258 - val_loss: 0.0050\n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0286 - val_loss: 0.0062\n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0262 - val_loss: 0.0082\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0220 - val_loss: 0.0076\n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0266 - val_loss: 0.0099\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0215 - val_loss: 0.0078\n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0223 - val_loss: 0.0089\n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0235 - val_loss: 0.0081\n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0226 - val_loss: 0.0086\n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0206 - val_loss: 0.0087\n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0217 - val_loss: 0.0090\n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0202 - val_loss: 0.0092\n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0198 - val_loss: 0.0100\n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0227 - val_loss: 0.0104\n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0202 - val_loss: 0.0107\n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0198 - val_loss: 0.0104\n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0196 - val_loss: 0.0112\n",
      "Epoch 26/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0186 - val_loss: 0.0117\n",
      "Epoch 27/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0201 - val_loss: 0.0119\n",
      "Epoch 28/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0224 - val_loss: 0.0151\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 369ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "RMSE: (214.1085487785167, 12.511707305908203)\n",
      "RMSE Random Walk: (203.7515977942379, 11.35639525760637)\n",
      "[15590.87992859 15406.91990662 15123.96029663 14890.13978577\n",
      " 14970.06010437 14831.09980774 14655.05969238 14583.96002197\n",
      " 14297.03997803 14147.81996155 15213.42399597 15187.22705841\n",
      " 14892.63838196 14990.91765594 14871.40258026 14894.06219482\n",
      " 14828.55451965 14858.38121796 14840.95464325 14669.29070282\n",
      " 14646.41851044 14693.22842407 14693.6268158  14594.51027679\n",
      " 14638.11460114 14551.8211441  14469.73760223 14410.60535431\n",
      " 14363.68773651 14313.02439117]\n",
      "[183.08999634 183.16999817 169.27000427 159.1499939  169.58000183\n",
      " 160.72000122 159.92999268 160.19000244 168.80000305 167.11000061\n",
      " 174.80358887 165.77026367 170.39749146 162.26094055 156.95254517\n",
      " 158.04705811 157.67150879 160.6322937  157.96199036 157.06187439\n",
      " 166.19992065 166.94677734 165.52381897 165.35345459 169.39987183\n",
      " 167.53494263 165.07548523 165.02062988 164.61088562 165.25071716]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 147ms/step - loss: 0.3849 - val_loss: 0.0102\n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2952 - val_loss: 0.0139\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1323 - val_loss: 0.0270\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0836 - val_loss: 0.0200\n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0509 - val_loss: 0.0132\n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0437 - val_loss: 0.0079\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0312 - val_loss: 0.0055\n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0270 - val_loss: 0.0044\n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0306 - val_loss: 0.0043\n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0256 - val_loss: 0.0044\n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0227 - val_loss: 0.0048\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0261 - val_loss: 0.0056\n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0282 - val_loss: 0.0066\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0231 - val_loss: 0.0068\n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0240 - val_loss: 0.0076\n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0210 - val_loss: 0.0074\n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0205 - val_loss: 0.0078\n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0214 - val_loss: 0.0076\n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0230 - val_loss: 0.0090\n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0213 - val_loss: 0.0073\n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0233 - val_loss: 0.0106\n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0219 - val_loss: 0.0091\n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0226 - val_loss: 0.0115\n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0211 - val_loss: 0.0090\n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0225 - val_loss: 0.0118\n",
      "Epoch 26/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0225 - val_loss: 0.0093\n",
      "Epoch 27/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0206 - val_loss: 0.0112\n",
      "Epoch 28/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0195 - val_loss: 0.0105\n",
      "Epoch 29/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0217 - val_loss: 0.0116\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 384ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "RMSE: (78.70480529016349, 7.0404296875)\n",
      "RMSE Random Walk: (209.8935539635242, 11.131913565452674)\n",
      "[15773.96992493 15590.08990479 15293.2303009  15049.28977966\n",
      " 15139.6401062  14991.81980896 14814.98968506 14744.15002441\n",
      " 14465.83998108 14314.92996216 15388.22758484 15352.99732208\n",
      " 15063.03587341 15153.1785965  15028.35512543 15052.10925293\n",
      " 14986.22602844 15019.01351166 14998.91663361 14826.35257721\n",
      " 14812.61843109 14860.17520142 14859.15063477 14759.86373138\n",
      " 14807.51447296 14719.35608673 14634.81308746 14575.62598419\n",
      " 14528.29862213 14478.27510834]\n",
      "[183.08999634 183.16999817 169.27000427 159.1499939  169.58000183\n",
      " 160.72000122 159.92999268 160.19000244 168.80000305 167.11000061\n",
      " 177.86129761 177.20941162 174.2219696  165.84791565 171.51156616\n",
      " 163.35652161 171.47695923 162.79685974 160.44839478 160.31860352\n",
      " 158.06781006 166.39897156 168.60902405 169.16708374 153.90975952\n",
      " 152.34559631 151.10954285 151.27461243 152.18518066 153.12342834]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 116ms/step - loss: 0.3416 - val_loss: 0.0575\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1877 - val_loss: 0.0298\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0905 - val_loss: 0.0205\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0591 - val_loss: 0.0205\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0359 - val_loss: 0.0250\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0325 - val_loss: 0.0401\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0334 - val_loss: 0.0391\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0274 - val_loss: 0.0472\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0290 - val_loss: 0.0453\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0257 - val_loss: 0.0423\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0247 - val_loss: 0.0388\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0248 - val_loss: 0.0382\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0278 - val_loss: 0.0363\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0279 - val_loss: 0.0368\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0269 - val_loss: 0.0367\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0266 - val_loss: 0.0373\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0289 - val_loss: 0.0365\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0265 - val_loss: 0.0367\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0249 - val_loss: 0.0338\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0268 - val_loss: 0.0360\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0246 - val_loss: 0.0340\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0222 - val_loss: 0.0381\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0268 - val_loss: 0.0368\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0256 - val_loss: 0.0366\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 372ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "RMSE: (278.9322037516016, 15.245139694213867)\n",
      "RMSE Random Walk: (54.98440013047242, 5.840844439618009)\n",
      "[15957.05992126 15773.25990295 15462.50030518 15208.43977356\n",
      " 15309.22010803 15152.53981018 14974.91967773 14904.34002686\n",
      " 14634.63998413 14482.03996277 15566.08888245 15530.2067337\n",
      " 15237.25784302 15319.02651215 15199.86669159 15215.46577454\n",
      " 15157.70298767 15181.8103714  15159.36502838 14986.67118073\n",
      " 14970.68624115 15026.57417297 15027.75965881 14929.03081512\n",
      " 14961.42423248 14871.70168304 14785.92263031 14726.90059662\n",
      " 14680.4838028  14631.39853668]\n",
      "[142.30000305 138.22999573 133.22000122 133.61999512 133.80000305\n",
      " 137.27999878 130.75       129.78999329 128.72999573 126.76999664\n",
      " 137.71138    141.05644226 145.22781372 141.23603821 142.7477417\n",
      " 133.14797974 125.44025421 140.81283569 134.34350586 132.83355713\n",
      " 136.71466064 143.25073242 140.00593567 129.06384277 141.02012634\n",
      " 139.01698303 138.20870972 138.66923523 138.13931274 138.48219299]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 116ms/step - loss: 0.3640 - val_loss: 0.0691\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.2227 - val_loss: 0.0411\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1376 - val_loss: 0.0333\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0889 - val_loss: 0.0271\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0591 - val_loss: 0.0220\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0429 - val_loss: 0.0252\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0348 - val_loss: 0.0273\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0321 - val_loss: 0.0348\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0302 - val_loss: 0.0374\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0307 - val_loss: 0.0401\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0307 - val_loss: 0.0406\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0255 - val_loss: 0.0399\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0283 - val_loss: 0.0390\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0247 - val_loss: 0.0397\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0287 - val_loss: 0.0381\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0282 - val_loss: 0.0381\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0225 - val_loss: 0.0382\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0242 - val_loss: 0.0374\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0263 - val_loss: 0.0357\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0302 - val_loss: 0.0382\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0261 - val_loss: 0.0360\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0258 - val_loss: 0.0354\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0268 - val_loss: 0.0386\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0303 - val_loss: 0.0328\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0258 - val_loss: 0.0378\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 363ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "RMSE: (362.1046780951408, 16.873269271850585)\n",
      "RMSE Random Walk: (52.1387986245994, 5.708982956717499)\n",
      "[16099.35992432 15911.48989868 15595.7203064  15342.05976868\n",
      " 15443.02011108 15289.81980896 15105.66967773 15034.13002014\n",
      " 14763.36997986 14608.80995941 15703.80026245 15671.26317596\n",
      " 15382.48565674 15460.26255035 15342.61443329 15348.61375427\n",
      " 15283.14324188 15322.62320709 15293.70853424 15119.50473785\n",
      " 15107.40090179 15169.8249054  15167.76559448 15058.0946579\n",
      " 15102.44435883 15010.71866608 14924.13134003 14865.56983185\n",
      " 14818.62311554 14769.88072968]\n",
      "[142.30000305 138.22999573 133.22000122 133.61999512 133.80000305\n",
      " 137.27999878 130.75       129.78999329 128.72999573 126.76999664\n",
      " 145.30249023 145.30430603 148.77513123 138.43788147 132.3863678\n",
      " 134.88972473 139.90374756 149.2315979  125.9230423  135.50906372\n",
      " 140.51901245 144.26347351 139.81420898 137.00030518 144.65589905\n",
      " 143.02125549 141.63395691 140.89634705 140.05987549 139.17379761]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 505ms/step - loss: 0.3668 - val_loss: 0.1756\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.1938 - val_loss: 0.0887\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1120 - val_loss: 0.0471\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0670 - val_loss: 0.0230\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0440 - val_loss: 0.0193\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0403 - val_loss: 0.0313\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0347 - val_loss: 0.0444\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0349 - val_loss: 0.0459\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0350 - val_loss: 0.0587\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0335 - val_loss: 0.0530\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0315 - val_loss: 0.0512\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0324 - val_loss: 0.0480\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0330 - val_loss: 0.0493\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0306 - val_loss: 0.0503\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0319 - val_loss: 0.0560\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0290 - val_loss: 0.0567\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0306 - val_loss: 0.0576\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0315 - val_loss: 0.0583\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0303 - val_loss: 0.0717\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0291 - val_loss: 0.0512\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0293 - val_loss: 0.0755\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0273 - val_loss: 0.0542\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0287 - val_loss: 0.0690\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0275 - val_loss: 0.0646\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0285 - val_loss: 0.0735\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 386ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "RMSE: (172.01569709349423, 12.369194030761719)\n",
      "RMSE Random Walk: (28.628590453781488, 4.150187152400146)\n",
      "[16241.65992737 16049.71989441 15728.94030762 15475.67976379\n",
      " 15576.82011414 15427.09980774 15236.41967773 15163.92001343\n",
      " 14892.09997559 14735.57995605 15849.10275269 15816.56748199\n",
      " 15531.26078796 15598.70043182 15475.00080109 15483.503479\n",
      " 15423.04698944 15471.85480499 15419.63157654 15255.01380157\n",
      " 15247.91991425 15314.08837891 15307.57980347 15195.09496307\n",
      " 15247.10025787 15153.73992157 15065.76529694 15006.46617889\n",
      " 14958.68299103 14909.05452728]\n",
      "[174.1499939  171.52000427 167.57000732 167.22999573 167.52999878\n",
      " 170.02999878 163.61999512 161.38000488 158.91000366 157.22000122\n",
      " 166.85395813 172.12896729 166.92373657 163.86886597 166.0017395\n",
      " 165.76741028 167.55541992 163.81130981 163.05259705 166.43528748\n",
      " 167.17428589 164.28503418 162.8473053  169.51960754 167.10377502\n",
      " 166.63613892 166.19294739 165.80047607 165.38560486 164.89938354]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 124ms/step - loss: 0.3639 - val_loss: 0.1786\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.2263 - val_loss: 0.0992\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.1558 - val_loss: 0.0871\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.1029 - val_loss: 0.0508\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0649 - val_loss: 0.0342\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0475 - val_loss: 0.0330\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0373 - val_loss: 0.0374\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0388 - val_loss: 0.0479\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0370 - val_loss: 0.0421\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0374 - val_loss: 0.0485\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0340 - val_loss: 0.0460\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0392 - val_loss: 0.0461\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0339 - val_loss: 0.0435\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0328 - val_loss: 0.0438\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0355 - val_loss: 0.0394\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0319 - val_loss: 0.0458\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0357 - val_loss: 0.0412\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0310 - val_loss: 0.0402\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0327 - val_loss: 0.0407\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0337 - val_loss: 0.0422\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0320 - val_loss: 0.0416\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0347 - val_loss: 0.0361\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0311 - val_loss: 0.0424\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0322 - val_loss: 0.0375\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0318 - val_loss: 0.0372\n",
      "Epoch 26/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0311 - val_loss: 0.0462\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 378ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "RMSE: (133.82903711543184, 10.943817901611329)\n",
      "RMSE Random Walk: (24.786933323292807, 4.137147214460843)\n",
      "[16415.80992126 16221.23989868 15896.51031494 15642.90975952\n",
      " 15744.35011292 15597.12980652 15400.03967285 15325.30001831\n",
      " 15051.00997925 14892.79995728 16015.95671082 15988.69644928\n",
      " 15698.18452454 15762.56929779 15641.00254059 15649.27088928\n",
      " 15590.60240936 15635.66611481 15582.68417358 15421.44908905\n",
      " 15415.09420013 15478.37341309 15470.42710876 15364.61457062\n",
      " 15414.2040329  15320.37606049 15231.95824432 15172.26665497\n",
      " 15124.06859589 15073.95391083]\n",
      "[174.1499939  171.52000427 167.57000732 167.22999573 167.52999878\n",
      " 170.02999878 163.61999512 161.38000488 158.91000366 157.22000122\n",
      " 167.83717346 165.53903198 163.95098877 162.68130493 169.36340332\n",
      " 164.72747803 160.79048157 160.82107544 164.82670593 166.90620422\n",
      " 163.11306763 168.10169983 162.39390564 167.0189209  165.17614746\n",
      " 164.44190979 163.68089294 163.04324341 162.40393066 161.63973999]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 118ms/step - loss: 0.3864 - val_loss: 0.0249\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2483 - val_loss: 0.0479\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1646 - val_loss: 0.0269\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1102 - val_loss: 0.0202\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0768 - val_loss: 0.0162\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0503 - val_loss: 0.0110\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0346 - val_loss: 0.0122\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0290 - val_loss: 0.0184\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0318 - val_loss: 0.0152\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0297 - val_loss: 0.0156\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0306 - val_loss: 0.0122\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0298 - val_loss: 0.0112\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0296 - val_loss: 0.0109\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0256 - val_loss: 0.0108\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0286 - val_loss: 0.0101\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0274 - val_loss: 0.0102\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0277 - val_loss: 0.0103\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0279 - val_loss: 0.0109\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0263 - val_loss: 0.0104\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0265 - val_loss: 0.0105\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0282 - val_loss: 0.0098\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0287 - val_loss: 0.0102\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0258 - val_loss: 0.0094\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0260 - val_loss: 0.0104\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0274 - val_loss: 0.0097\n",
      "Epoch 26/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0255 - val_loss: 0.0100\n",
      "Epoch 27/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0248 - val_loss: 0.0095\n",
      "Epoch 28/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0251 - val_loss: 0.0096\n",
      "Epoch 29/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0238 - val_loss: 0.0095\n",
      "Epoch 30/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0255 - val_loss: 0.0098\n",
      "Epoch 31/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0252 - val_loss: 0.0097\n",
      "Epoch 32/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0235 - val_loss: 0.0108\n",
      "Epoch 33/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0248 - val_loss: 0.0098\n",
      "Epoch 34/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0234 - val_loss: 0.0104\n",
      "Epoch 35/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0244 - val_loss: 0.0095\n",
      "Epoch 36/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0279 - val_loss: 0.0094\n",
      "Epoch 37/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0251 - val_loss: 0.0096\n",
      "Epoch 38/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0267 - val_loss: 0.0092\n",
      "Epoch 39/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0251 - val_loss: 0.0091\n",
      "Epoch 40/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0267 - val_loss: 0.0098\n",
      "Epoch 41/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0233 - val_loss: 0.0094\n",
      "Epoch 42/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0249 - val_loss: 0.0095\n",
      "Epoch 43/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0270 - val_loss: 0.0092\n",
      "Epoch 44/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0260 - val_loss: 0.0093\n",
      "Epoch 45/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0250 - val_loss: 0.0104\n",
      "Epoch 46/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0269 - val_loss: 0.0096\n",
      "Epoch 47/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0258 - val_loss: 0.0105\n",
      "Epoch 48/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0225 - val_loss: 0.0093\n",
      "Epoch 49/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0218 - val_loss: 0.0101\n",
      "Epoch 50/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0232 - val_loss: 0.0097\n",
      "Epoch 51/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0217 - val_loss: 0.0098\n",
      "Epoch 52/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0231 - val_loss: 0.0095\n",
      "Epoch 53/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0231 - val_loss: 0.0098\n",
      "Epoch 54/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0217 - val_loss: 0.0099\n",
      "Epoch 55/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0250 - val_loss: 0.0098\n",
      "Epoch 56/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0221 - val_loss: 0.0097\n",
      "Epoch 57/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0226 - val_loss: 0.0098\n",
      "Epoch 58/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0249 - val_loss: 0.0097\n",
      "Epoch 59/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0250 - val_loss: 0.0099\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 364ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "RMSE: (131.3907926187734, 10.995697021484371)\n",
      "RMSE Random Walk: (45.271849201264494, 5.2544335783521126)\n",
      "[16589.95991516 16392.75990295 16064.08032227 15810.13975525\n",
      " 15911.88011169 15767.1598053  15563.65966797 15486.68002319\n",
      " 15209.91998291 15050.0199585  16183.79388428 16154.23548126\n",
      " 15862.13551331 15925.25060272 15810.36594391 15813.99836731\n",
      " 15751.39289093 15796.48719025 15747.51087952 15588.35529327\n",
      " 15578.20726776 15646.47511292 15632.8210144  15531.63349152\n",
      " 15579.38018036 15484.81797028 15395.63913727 15335.30989838\n",
      " 15286.47252655 15235.59365082]\n",
      "[122.51000214 120.31999969 120.86000061 118.12000275 115.06999969\n",
      " 114.76999664 114.69999695 117.69999695 111.30000305 110.33999634\n",
      " 117.12459564 118.22034454 118.29355621 117.27203369 117.21088409\n",
      " 117.66332245 117.46315765 117.88584137 117.85254669 117.40005493\n",
      " 116.6248703  117.41304779 116.88256836 116.70968628 116.90127563\n",
      " 116.00173187 114.88265991 113.78637695 113.42469025 112.83070374]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 112ms/step - loss: 0.3835 - val_loss: 0.0237\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2210 - val_loss: 0.0293\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1267 - val_loss: 0.0187\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0654 - val_loss: 0.0167\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0507 - val_loss: 0.0139\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0385 - val_loss: 0.0103\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0328 - val_loss: 0.0101\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0298 - val_loss: 0.0122\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0289 - val_loss: 0.0140\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0277 - val_loss: 0.0130\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0288 - val_loss: 0.0124\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0261 - val_loss: 0.0109\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0289 - val_loss: 0.0110\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0290 - val_loss: 0.0096\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0287 - val_loss: 0.0093\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0281 - val_loss: 0.0097\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0317 - val_loss: 0.0090\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0295 - val_loss: 0.0096\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0267 - val_loss: 0.0098\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0256 - val_loss: 0.0104\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0262 - val_loss: 0.0113\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0267 - val_loss: 0.0102\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0281 - val_loss: 0.0104\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0274 - val_loss: 0.0106\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0274 - val_loss: 0.0101\n",
      "Epoch 26/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0278 - val_loss: 0.0092\n",
      "Epoch 27/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0260 - val_loss: 0.0104\n",
      "Epoch 28/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0310 - val_loss: 0.0088\n",
      "Epoch 29/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0336 - val_loss: 0.0101\n",
      "Epoch 30/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0389 - val_loss: 0.0092\n",
      "Epoch 31/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0274 - val_loss: 0.0088\n",
      "Epoch 32/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0265 - val_loss: 0.0099\n",
      "Epoch 33/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0280 - val_loss: 0.0092\n",
      "Epoch 34/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0289 - val_loss: 0.0106\n",
      "Epoch 35/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0268 - val_loss: 0.0109\n",
      "Epoch 36/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0282 - val_loss: 0.0102\n",
      "Epoch 37/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0260 - val_loss: 0.0123\n",
      "Epoch 38/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0275 - val_loss: 0.0098\n",
      "Epoch 39/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0282 - val_loss: 0.0107\n",
      "Epoch 40/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0254 - val_loss: 0.0102\n",
      "Epoch 41/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0262 - val_loss: 0.0111\n",
      "Epoch 42/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0276 - val_loss: 0.0105\n",
      "Epoch 43/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0274 - val_loss: 0.0113\n",
      "Epoch 44/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0244 - val_loss: 0.0106\n",
      "Epoch 45/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0248 - val_loss: 0.0113\n",
      "Epoch 46/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0259 - val_loss: 0.0104\n",
      "Epoch 47/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0268 - val_loss: 0.0110\n",
      "Epoch 48/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0245 - val_loss: 0.0109\n",
      "Epoch 49/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0271 - val_loss: 0.0114\n",
      "Epoch 50/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0239 - val_loss: 0.0106\n",
      "Epoch 51/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0257 - val_loss: 0.0105\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "RMSE: (145.4715771390823, 11.596517181396482)\n",
      "RMSE Random Walk: (40.52877910655688, 5.071066944984965)\n",
      "[16712.4699173  16513.07990265 16184.94032288 15928.259758\n",
      " 16026.95011139 15881.92980194 15678.35966492 15604.38002014\n",
      " 15321.21998596 15160.35995483 16300.91847992 16272.45582581\n",
      " 15980.42906952 16042.52263641 15927.576828   15931.66168976\n",
      " 15868.85604858 15914.37303162 15865.36342621 15705.75534821\n",
      " 15694.83213806 15763.88816071 15749.70358276 15648.3431778\n",
      " 15696.28145599 15600.81970215 15510.52179718 15449.09627533\n",
      " 15399.8972168  15348.42435455]\n",
      "[122.51000214 120.31999969 120.86000061 118.12000275 115.06999969\n",
      " 114.76999664 114.69999695 117.69999695 111.30000305 110.33999634\n",
      " 117.82449341 117.76062012 118.04621887 117.73822021 117.40068054\n",
      " 119.11639404 118.31085205 117.67127991 119.34601593 117.95264435\n",
      " 117.82505798 118.59526825 117.09910583 117.98137665 117.65553284\n",
      " 116.55438995 115.56700897 114.5476532  113.72702026 113.14051819]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 0.2899 - val_loss: 0.0029\n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1861 - val_loss: 0.0095\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0994 - val_loss: 0.0089\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0614 - val_loss: 0.0035\n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0364 - val_loss: 0.0030\n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0209 - val_loss: 0.0063\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0164 - val_loss: 0.0097\n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0139 - val_loss: 0.0101\n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0138 - val_loss: 0.0082\n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0125 - val_loss: 0.0060\n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0121 - val_loss: 0.0047\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0108 - val_loss: 0.0040\n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0111 - val_loss: 0.0038\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0098 - val_loss: 0.0039\n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0095 - val_loss: 0.0043\n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0106 - val_loss: 0.0044\n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0110 - val_loss: 0.0044\n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0105 - val_loss: 0.0045\n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0110 - val_loss: 0.0046\n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0094 - val_loss: 0.0046\n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0098 - val_loss: 0.0047\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 362ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "RMSE: (1369.71534180491, 31.793729400634767)\n",
      "RMSE Random Walk: (128.4408506344177, 9.0338001393504)\n",
      "[16834.97991943 16633.39990234 16305.80032349 16046.37976074\n",
      " 16142.02011108 15996.69979858 15793.05966187 15722.08001709\n",
      " 15432.51998901 15270.69995117 16418.74297333 16390.21644592\n",
      " 16098.47528839 16160.26085663 16044.97750854 16050.7780838\n",
      " 15987.16690063 16032.04431152 15984.70944214 15823.70799255\n",
      " 15812.65719604 15882.48342896 15866.8026886  15766.32455444\n",
      " 15813.93698883 15717.3740921  15626.08880615 15563.64392853\n",
      " 15513.62423706 15461.56487274]\n",
      "[244.11000061 242.69999695 249.30000305 249.11000061 245.69000244\n",
      " 241.1499939  245.16999817 241.16000366 226.53999329 224.55000305\n",
      " 211.4669342  220.62611389 207.08596802 264.47131348 155.38102722\n",
      " 193.20220947 165.7936554  183.95223999 193.75804138 160.81742859\n",
      " 189.16191101 206.89016724 225.07991028 213.3795166  218.83836365\n",
      " 218.36889648 218.19113159 218.06404114 218.07093811 216.70825195]\n",
      "Training with window_size=10, prediction_length=15, lstm_units=50, batch_size=32, dropout_rate=0.0, predict_day=20\n",
      "30\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ondre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 162ms/step - loss: 0.3110 - val_loss: 0.0038\n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.2221 - val_loss: 0.0057\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.1581 - val_loss: 0.0060\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0886 - val_loss: 0.0053\n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0437 - val_loss: 0.0034\n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0288 - val_loss: 0.0025\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0202 - val_loss: 0.0032\n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.0151 - val_loss: 0.0043\n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.0149 - val_loss: 0.0043\n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0150 - val_loss: 0.0040\n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0138 - val_loss: 0.0029\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0143 - val_loss: 0.0027\n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0134 - val_loss: 0.0022\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0129 - val_loss: 0.0021\n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.0118 - val_loss: 0.0021\n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0133 - val_loss: 0.0021\n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0118 - val_loss: 0.0022\n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0108 - val_loss: 0.0024\n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0130 - val_loss: 0.0024\n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0118 - val_loss: 0.0026\n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0122 - val_loss: 0.0025\n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0120 - val_loss: 0.0026\n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.0122 - val_loss: 0.0025\n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0125 - val_loss: 0.0025\n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0110 - val_loss: 0.0025\n",
      "Epoch 26/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0104 - val_loss: 0.0026\n",
      "Epoch 27/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0113 - val_loss: 0.0026\n",
      "Epoch 28/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0121 - val_loss: 0.0027\n",
      "Epoch 29/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0127 - val_loss: 0.0027\n",
      "Epoch 30/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0119 - val_loss: 0.0026\n",
      "Epoch 31/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0112 - val_loss: 0.0026\n",
      "Epoch 32/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0109 - val_loss: 0.0024\n",
      "Epoch 33/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0114 - val_loss: 0.0025\n",
      "Epoch 34/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0113 - val_loss: 0.0026\n",
      "Epoch 35/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0104 - val_loss: 0.0026\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 462ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "RMSE: (317.8266963245579, 14.347299194335935)\n",
      "RMSE Random Walk: (118.88636583521433, 8.731228243439904)\n",
      "[17079.08992004 16876.09989929 16555.10032654 16295.48976135\n",
      " 16387.71011353 16237.84979248 16038.22966003 15963.24002075\n",
      " 15659.0599823  15495.24995422 16630.20990753 16610.84255981\n",
      " 16305.56125641 16424.7321701  16200.35853577 16243.98029327\n",
      " 16152.96055603 16215.99655151 16178.46748352 15984.52542114\n",
      " 16001.81910706 16089.37359619 16091.88259888 15979.70407104\n",
      " 16032.77535248 15935.74298859 15844.27993774 15781.70796967\n",
      " 15731.69517517 15678.27312469]\n",
      "[244.11000061 242.69999695 249.30000305 249.11000061 245.69000244\n",
      " 241.1499939  245.16999817 241.16000366 226.53999329 224.55000305\n",
      " 225.47842407 226.36128235 224.30856323 224.97958374 209.64778137\n",
      " 229.86849976 221.83166504 224.67446899 218.93110657 207.69041443\n",
      " 211.92111206 209.59542847 224.60879517 214.29472351 218.44215393\n",
      " 217.79704285 216.83908081 215.79879761 214.63088989 213.61051941]\n",
      "RMSE: (30871.730515565876, 175.33874696731567)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyperparameter grid\n",
    "window_sizes = [ 10] #5,\n",
    "prediction_lengths = [15] #5 10,\n",
    "lstm_units_options = [50]#, 100\n",
    "batch_sizes = [32]#, 64\n",
    "dropout_rates = [0.0]#, 0.2\n",
    "predict_days = [20]\n",
    "sents = [True,False]#\n",
    "\n",
    "pred = np.zeros(30)\n",
    "\n",
    "# Example usage\n",
    "for i in range(10):\n",
    "    for ticker in ['META', 'AMZN', 'AAPL','GOOG', 'NFLX']: #, 'META', 'AMZN', 'AAPL','GOOG', 'NFLX' 'TSLA'\n",
    "        for window_size, prediction_length, lstm_units, batch_size, dropout_rate, predict_day, sent in product(\n",
    "                window_sizes, prediction_lengths, lstm_units_options, batch_sizes, dropout_rates, predict_days, sents):\n",
    "\n",
    "            print(f\"Training with window_size={window_size}, prediction_length={prediction_length}, lstm_units={lstm_units}, batch_size={batch_size}, dropout_rate={dropout_rate}, predict_day={predict_day}\")\n",
    "            \n",
    "            test_size = window_size + prediction_length\n",
    "            if prediction_length - predict_day < 0:\n",
    "                test_size += abs(prediction_length - predict_day)\n",
    "            print(test_size)\n",
    "            \n",
    "            predictions, test_y, test_X, importances = train(data_sent, prices_hist, ticker, window_size=window_size, prediction_length=prediction_length, lstm_units=lstm_units, batch_size=batch_size, dropout_rate=dropout_rate, predict_days=test_size, sent=sent)\n",
    "            predictions = np.array(list(test_X['Close'])[:window_size] + list(predictions[0]) + [i[-1] for i in predictions[1:]])[:window_size+predict_day]\n",
    "            rmse = rmse_out(predictions[window_size:], list(test_y.reset_index(drop=True)['Close'])[window_size:window_size+predict_day])\n",
    "            print(f\"RMSE: {rmse}\")\n",
    "\n",
    "            n_steps = predict_day\n",
    "            history = list(test_X['Close'])[:window_size]\n",
    "\n",
    "            # Make the forecast with some variability (adjust variability_factor as needed)\n",
    "            variability_factor = 0.2  # Adjust this parameter to control the amount of variability\n",
    "            # forecast = list(test_X['Close'])[:window_size] + list(random_walk_forecast(history, n_steps, variability_factor))\n",
    "\n",
    "            rmse_random_walk = rmse_out(list(test_y.reset_index(drop=True)['Close'])[window_size:window_size+predict_day], list(random_walk_forecast(history, n_steps, variability_factor)))\n",
    "            print(f\"RMSE Random Walk: {rmse_random_walk}\")\n",
    "\n",
    "            print(pred)\n",
    "            print(predictions)\n",
    "            pred = np.sum([pred, predictions], axis=0)\n",
    "\n",
    "            \n",
    "            #plot_predictions(list(test_y.reset_index(drop=True)['Close'])[:window_size+predict_day], predictions, list(test_X['Close'])[:window_size] + list(random_walk_forecast(history, n_steps, variability_factor)), ticker)\n",
    "\n",
    "                    # Save result to JSON file\n",
    "            result = {\n",
    "                'ticker': ticker,\n",
    "                'window_size': window_size,\n",
    "                'prediction_length': prediction_length,\n",
    "                'lstm_units': lstm_units,\n",
    "                'batch_size': batch_size,\n",
    "                'dropout_rate': dropout_rate,\n",
    "                'predict_day': predict_day,\n",
    "                'rmse': rmse,\n",
    "                'rmse_random_walk': rmse_random_walk,\n",
    "                'without_sentiment' : sent,\n",
    "                'importances' : ' '.join([str(i) for i in importances]),\n",
    "                'predictions' : ' '.join([str(i) for i in predictions])\n",
    "            }\n",
    "            append_to_json('training_results1.json', result)\n",
    "\n",
    "pred = pred/40\n",
    "rmse = rmse_out(pred[window_size:], list(test_y.reset_index(drop=True)['Close'])[window_size:window_size+predict_day])\n",
    "print(f\"RMSE: {rmse}\")\n",
    "plot_predictions(list(test_y.reset_index(drop=True)['Close'])[:window_size+predict_day], pred, list(test_X['Close'])[:window_size] + list(random_walk_forecast(history, n_steps, variability_factor)), ticker)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation of prices and sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ticker': 'META', 'positive': 0.08740544078235996, 'neutral': 0.09643629866796304, 'negative': -0.21712226387373826}\n",
      "{'ticker': 'AMZN', 'positive': 0.15634513880047135, 'neutral': -0.036205405941915576, 'negative': -0.18436085186132425}\n",
      "{'ticker': 'AAPL', 'positive': 0.0994623609543123, 'neutral': -0.01946460457527111, 'negative': -0.10868631513777716}\n",
      "{'ticker': 'GOOG', 'positive': 0.030823106440933758, 'neutral': -0.05485250234639411, 'negative': 0.023279072735494205}\n",
      "{'ticker': 'NFLX', 'positive': 0.23148838284695858, 'neutral': -0.03149062271352068, 'negative': -0.2474097073275972}\n",
      "{'ticker': 'TSLA', 'positive': 0.34345618919252324, 'neutral': -0.02993766523200747, 'negative': -0.4166752705828227}\n"
     ]
    }
   ],
   "source": [
    "for stock in ['META', 'AMZN', 'AAPL','GOOG', 'NFLX', 'TSLA']:\n",
    "    df_corr = df_for_inp(data_sent, prices_hist, stock)\n",
    "    corrs = {'ticker': stock}\n",
    "    for sent in ['positive', 'neutral', 'negative']:\n",
    "        correlation = df_corr[sent].corr(df_corr['Close'])\n",
    "        corrs[sent] = correlation\n",
    "    print(corrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rescale the 'Close' series\n",
    "scaler = MinMaxScaler()\n",
    "df_corr['Close_rescaled'] = scaler.fit_transform(df_corr[['Close']])\n",
    "df_corr['positive_rescaled'] = scaler.fit_transform(df_corr[['positive']])\n",
    "\n",
    "# Plot both series\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_corr.index, df_corr['positive_rescaled'], label='Positive')\n",
    "plt.plot(df_corr.index, df_corr['Close_rescaled'], label='Close (Rescaled)')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Positive and Rescaled Close Series')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
